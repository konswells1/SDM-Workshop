{"title":"Assessment & Projection","markdown":{"yaml":{"title":"Assessment & Projection","format":"html","editor":"visual","draft":false},"headingText":"Load required packages","containsRefs":false,"markdown":"\n\n```{r}\n#| label: setup-assessproject\n#| echo: false\n#| include: false\n#| warning: false\n#| message: false\n\nsource(\"scripts/load_packages.R\")\n\n# Load shared environmental and species data\nsource(\"scripts/shared_sdm_data.R\")\n\n# Load shared SDM models\nsource(\"scripts/shared_sdm_models.R\")\n```\n\n<br>\n\nAfter a species distribution model (SDM) has been fitted, the next critical steps are to **assess its performance** and **project it across space or time**. Model assessment ensures that the relationships identified between species occurrences and environmental predictors are robust, interpretable, and generalisable. Projection then allows us to translate these relationships into maps of habitat suitability, either under current conditions or for future scenarios such as climate change.\n\n**Model assessment** involves several interconnected steps:\\\n**Model fit**: Examining how well the model explains the training data.\\\n**Model validation**: Testing model predictions on independent data (data not used for fitting) to avoid overfitting and assess generalization.\\\n**Model evaluation**: Calculating quantitative performance metrics on the validation data to objectively judge model quality.\\\n\nA common and recommended approach is to **split the available data into training and testing sets** or use **cross-validation** methods. This ensures that evaluation statistics such as AUC or TSS reflect the model’s predictive ability rather than just its capacity to reproduce the training data.\n\nEvaluation metrics fall into two main types:\\\n**Threshold-independent metrics** like AUC and Continuous Boyce Index evaluate the model’s discrimination ability without requiring a binary classification.\\\n**Threshold-dependent metrics**\\*\\* like sensitivity, specificity, and True Skill Statistic (TSS) require converting continuous predictions into binary outcomes and assess classification accuracy at selected thresholds.\\\n\nTogether, these evaluation metrics, when applied on validation data, provide a comprehensive picture of model performance.\n\n**Projection** extends model application by using fitted relationships to predict species distributions across geographic space, or under different environmental conditions. This is particularly powerful for biodiversity monitoring, invasive species risk mapping, and forecasting impacts of global change. However, projections must always be interpreted in light of uncertainties stemming from data quality, model choice, and environmental scenarios.\n\n::: {.callout-note collapse=\"true\" icon=\"info\"}\n### Essential SDM Workflow: Fitting, Validating, Evaluation and Projection\n\nWhen building species distribution models (SDMs), it's essential to distinguish between **model fit**, **model validation**, **model evaluation**, and **final projection**. These are distinct but interconnected steps in a robust modelling workflow.\n\n------------------------------------------------------------------------\n\n#### **Model fit**\n\nRefers to how well a model explains the data it was **trained on**. This can be assessed using residuals, likelihood measures, or internal performance statistics. However, good fit to training data does **not guarantee predictive accuracy** — models can overfit, especially with small sample sizes or many predictors.\n\n------------------------------------------------------------------------\n\n#### **Model validation**\n\nEvaluates how well the model **generalizes to new data**. This requires testing predictions on **data not used for model fitting**, typically via: - **Data splitting** (e.g., 70% training, 30% testing), or\\\n- **Cross-validation** (e.g., k-fold, spatial block).\n\nValidation ensures the model's performance is **not just memorizing patterns in the training data**, but reflects true predictive power.\n\n------------------------------------------------------------------------\n\n#### **Model evaluation**\n\nInvolves calculating quantitative performance metrics (e.g., AUC, TSS, Boyce Index) on the **validation (test) data**. This step quantifies how well the model distinguishes presences from absences, both: - **Threshold-independently** (e.g., AUC, Boyce), and\\\n- **Threshold-dependently** (e.g., sensitivity, specificity, TSS).\n\nEvaluation provides **objective evidence** of model reliability.\n\n------------------------------------------------------------------------\n\n#### **Final projection**\n\nOnce the model has been validated and evaluated: - You can **refit the final model using all available data** (training + test) to maximize information. - Use this full model to **project species distributions** across space (e.g., habitat suitability maps) or time (e.g., future climate scenarios).\n\n**Why refit with all data?**\\\nTo improve accuracy of projections, especially for rare species or sparse datasets. But remember: this projection step should **only happen after validation** is complete.\n\n------------------------------------------------------------------------\n\n**Best practice workflow summary**:\n\n1.  **Split** data → for training and validation.\\\n2.  **Fit** model on training data.\\\n3.  **Validate & evaluate** on test data.\\\n4.  **Refit** model on all data.\\\n5.  **Project** using the refitted model.\n\nThis approach supports both **scientific rigor** and **reliable application** of SDMs in conservation, forecasting, and spatial planning.\n:::\n\n<br>\n\n### 1. Model evaluation\n\nModel evaluation is a key step in SDM analysis. Since SDMs typically predict a continuous probability of occurrence or relative suitability, while field data are often binary (presence vs. absence/pseudo-absence), we need appropriate metrics to judge model quality.\n\nA critical principle in SDM evaluation is to **assess model predictions on independent data** not used during model fitting. This can be achieved by:\\\n**Data splitting**: dividing the dataset into training and testing subsets, fitting the model only on training data, then evaluating on testing data.\\\n**Cross-validation**: repeatedly splitting the data into folds, training on some folds and testing on the others, to obtain robust performance estimates. Evaluating models on independent data helps to avoid overly optimistic assessments caused by overfitting and better reflects how the model will perform in real-world applications.\n\nAfter a model is fitted, and data have been partitioned appropriately, evaluation metrics are calculated on the validation (test) data to assess predictive performance based on either\n\n**Threshold-independent** or **Threshold-dependent** metrics.\n\nFollowing best-practice, we now split our data into training and testing subsets and refit the GLM model using only the training data before performing any validation.\n\n```{r}\n#| label: data_split\n#| echo: true\n#| warning: false\n#| message: false\n\nlibrary(caret)\n\nset.seed(123)\n# Generate an index that assign 70% of data to training data\ntrainIndex <- createDataPartition(sdm_df_clean$pa, p = .7, list = FALSE)\nsdm_df_train <- sdm_df_clean[trainIndex, ]\nsdm_df_test <- sdm_df_clean[-trainIndex, ]\n\n# Fit model on training data\nglm_multi_train <- glm(pa ~ poly(elevation, 2) + poly(bio_1, 2) + poly(bio_12, 2),\n                 data = sdm_df_train,\n                 family = binomial)\n\nglm_step_train <- step(glm_multi_train , direction = \"both\", trace = FALSE)\n\n```\n\n#### 1.1 Threshold-independent evaluation\n\nThe most widely used threshold-independent measure is the **area under the ROC curve (AUC)**. AUC evaluates how well the model distinguishes presences from absences across all possible thresholds. An AUC of 0.5 indicates random performance, while values above 0.7 are typically considered fair, and above 0.9 excellent (Araujo et al. 2005).\n\n```{r}\n#| label: auc-eval\n#| echo: true\n#| warning: false\n#| message: false\n\nlibrary(pROC)\n\n# Predict on test data\nglm_predict_test <- predict(glm_step_train , newdata = sdm_df_test, type = \"response\")\n\n# 'Observed' test data (presence/pseudoabsence)\nobs_test <- sdm_df_test$pa\n\n# Evaluate with ROC and AUC on test data\nroc_glm <- pROC::roc(obs_test, glm_predict_test)\nauc_glm <- pROC::auc(roc_glm)\n\n# Print AUC\nprint(auc_glm)\n\n# Plot ROC curve\nplot(roc_glm, col = \"#e31a1c\", lwd = 2, main = \"ROC Curve\")\nabline(a = 0, b = 1, lty = 2, col = \"grey\")\n```\n\n::: {.callout-note collapse=\"true\" icon=\"info-circle\"}\n### How to interpret the ROC curve\n\n-   The ROC curve plots **sensitivity (true positive rate)** against **1 – specificity (false positive rate)** across all thresholds.\\\n-   A model with **no predictive ability** follows the diagonal line (AUC ≈ 0.5).\\\n-   Curves that **bow towards the upper-left corner** indicate stronger discrimination between presences and absences.\\\n-   The **area under the curve (AUC)** provides a summary metric:\n    -   **AUC = 0.5** → random predictions\\\n    -   **0.7 ≤ AUC \\< 0.8** → fair\\\n    -   **0.8 ≤ AUC \\< 0.9** → good\\\n    -   **AUC ≥ 0.9** → excellent\n\nAlways interpret AUC together with ecological reasoning and other evaluation measures\\\n(e.g., TSS, Boyce index), since AUC alone can be misleading if prevalence is unbalanced\\\nor sampling design introduces bias.\n:::\n\n<br>\n\nThe **Continuous Boyce Index (CBI)** is an evaluation metric used for species distribution models (SDMs) and habitat suitability models that are built using presence-only data. The CBI is particularly useful for SDMs, as it directly evaluates how predictions for observed presences deviate from a random distribution of available habitat suitability values. It ranges from -1 (counter-predictive), to 0 (no better than random), to +1 (perfect agreement between model predictions and presence distribution).\n\n```{r}\n#| label: boyce-eval\n#| echo: true\n#| warning: false\n#| message: false\n\nlibrary(ecospat)\n\n# Compute Continuous Boyce Index (CBI)\nboyce_res <- ecospat.boyce(\n  fit = glm_predict_test,            # model predictions\n  obs = glm_predict_test[obs_test == 1],  # predictions at presence points\n  nclass = 0,                  # automatic binning\n  window.w = \"default\"         # default moving window\n)\n\nboyce_res$Spearman.cor\n```\n\n::: {.callout-note collapse=\"true\" icon=\"info-circle\"}\n### How to interprete the Boyce curve\n\n-   **X-axis (suitability):** predicted suitability scores from the SDM.\\\n-   **Y-axis (predicted/expected ratio):** how much more (or less) frequently presences are found in each suitability class than expected at random.\n\n**Guidelines:**\\\n- A flat line near **1** → model performs no better than random.\\\n- An increasing curve, with ratios **\\>1** in high suitability bins → good model performance.\\\n- Ratios **\\<1** in high suitability bins → the model may be misleading.\n\nWhile P/E ratios in the plot may exceed 1, the Boyce Index correlation ranges from -1 to +1..\n:::\n\n<br>\n\n#### 1.2 Threshold-dependent evaluation\n\nTo compute threshold-dependent metrics, we need to choose a threshold that converts continuous probabilities into binary predictions. Several rules exist (see Liu et al., 2005), such as:\n\nMaximizing sensitivity + specificity\n\nEqual sensitivity and specificity\n\nFixed probability cutoffs (e.g., 0.5)\n\nHere we use the `PresenceAbsence` package to determine an optimal threshold and then compute metrics like sensitivity, specificity, and the True Skill Statistic (TSS).\n\n```{r}\n#| label: threshold-eval\n#| echo: true\n#| warning: false\n#| message: false\n\nlibrary(PresenceAbsence)\n\n# Construct PresenceAbsence dataframe\n# Ensure observed and predicted values are numeric\npa_df_test <- data.frame(\n  ID = seq_along(obs_test),\n  observed = as.numeric(as.character(obs_test)),\n  predicted = as.numeric(glm_predict_test)\n  )\n\n# Remove any rows with missing values\npa_df_test <- pa_df_test[complete.cases(pa_df_test), ]\n\n# Find threshold\n(opt_thresh <- PresenceAbsence::optimal.thresholds(\n                DATA= pa_df_test,\n                opt.methods = c('MaxSens+Spec', 'Sens=Spec', 'MinROCdist', 'MaxKappa')))\n\n# Use one threshold (e.g., MaxSens+Spec)\n    chosen_thresh <- opt_thresh$predicted[opt_thresh$Method == 'MaxSens+Spec']  \n\n# Classify predictions\npred_binary <- ifelse(as.numeric(glm_predict_test) >= chosen_thresh, 1, 0)    \n\n# Confusion matrix\ncm <- table(Observed = obs_test, Predicted = pred_binary)\nprint(cm)\n\n# Compute sensitivity, specificity, TSS\nsensitivity <- sum(pred_binary == 1 & obs_test == 1) / sum(obs_test == 1)\nspecificity <- sum(pred_binary == 0 & obs_test == 0) / sum(obs_test == 0)\ntss <- sensitivity + specificity - 1\n\nlist(ConfusionMatrix = cm,\n      Sensitivity = sensitivity,\n      Specificity = specificity,\n      TSS = tss)   \n      \n```\n\n```{r}\n#| label: threshold-safe\n#| echo: false\n\nlibrary(PresenceAbsence)\n\n# Construct PresenceAbsence dataframe\n# Ensure observed and predicted values are numeric\npa_df_test <- data.frame(\n  ID = seq_along(obs_test),\n  observed = as.numeric(as.character(obs_test)),\n  predicted = as.numeric(glm_predict_test)\n  )\n\n# Remove any rows with missing values\npa_df_test <- pa_df_test[complete.cases(pa_df_test), ]\n\n# Compute thresholds\nopt_thresh <- optimal.thresholds(DATA = pa_df_test,\n                                opt.methods = c('MaxSens+Spec', 'Sens=Spec', 'MinROCdist', 'MaxKappa'))\n\n# Safe selection of threshold\nif(!is.null(opt_thresh$predicted)) {\n  chosen_thresh <- opt_thresh$predicted[opt_thresh$Method == 'MaxSens+Spec']\n}\n\n# Fallback if threshold missing or unrealistic\nif(!exists(\"chosen_thresh\") || is.na(chosen_thresh) || chosen_thresh <= 0 || chosen_thresh >= 1) {\n  chosen_thresh <- 0.5\n  message(\"chosen_thresh missing or unrealistic; using default = 0.5\")\n}\n```\n\n::: {.callout-note collapse=\"true\" icon=\"info-circle\"}\n### Choosing thresholds for binary predictions\n\nSpecies distribution models typically produce **continuous predictions** (relative suitability or probability of occurrence).\\\nTo evaluate performance or produce presence/absence maps, these need to be **converted into binary predictions**.\\\nThere is no single \"best\" threshold. Instead, different methods emphasize different trade-offs.\n\nHere are four widely used options:\n\n-   **MaxSens+Spec**\\\n    Maximizes the sum of sensitivity (true positive rate) and specificity (true negative rate).\\\n    → Balanced choice when false positives and false negatives are equally costly.\n\n-   **Sens=Spec**\\\n    Selects the threshold where sensitivity and specificity are equal.\\\n    → Useful when both error types are equally undesirable.\n\n-   **MinROCdist**\\\n    Minimizes the distance between the ROC curve and the perfect classifier point (0,1).\\\n    → Robust option, often similar to MaxSens+Spec but less sensitive to skewed prevalence.\n\n-   **MaxKappa**\\\n    Maximizes Cohen’s Kappa statistic, which accounts for agreement expected by chance.\\\n    → Historically popular in ecology, still informative for comparing classifiers.\n\nIt is good practice to report **multiple thresholds**.\\\nThis ensures transparency, allows comparison across studies, and lets end-users (e.g., conservation managers) choose the threshold best suited to their objectives (e.g., maximizing detection vs. minimizing false alarms).\n:::\n\n**Interpretation**\n\n**AUC**: reflects overall discriminatory ability, independent of any threshold.\n\n**Threshold-dependent metrics**: provide information about model performance in classifying presences vs. absences at a chosen threshold.\n\n**TSS**: balances sensitivity and specificity, commonly used in SDM studies.\n\nIn practice, both perspectives are valuable. Threshold-independent measures provide a broad sense of predictive skill, while threshold-dependent measures allow for more concrete classification performance.\n\n### 2. Projection across space and time\n\nOnce a species distribution model has been calibrated and evaluated, the next step is projection: predicting habitat suitability across geographic space or future environmental scenarios. Projections enable us to identify potential ranges, assess climate change impacts, and inform conservation planning.\n\nThere are two main projection contexts:\n\n**Spatial projection**: applying the model to current environmental layers across the study area to produce a habitat suitability map.\n\n**Temporal projection**: applying the model to future climate scenarios (e.g., CMIP6 SSPs) to forecast potential range shifts.\n\n::: {.callout-tip collapse=\"true\" icon=\"globe\"}\\\n\\### Important considerations when projecting SDMs\n\nEnsure predictor variables in the projection layers match the model in units, resolution, and extent.\n\nBeware of extrapolation: predictions in environmental space outside the range of training data are uncertain.\n\nFor presence–pseudoabsence models, predictions remain relative suitability, not absolute probabilities.\n\nWhen projecting to future climates, using ensembles of GCMs helps capture uncertainty. :::\n\n<br>\n\n#### 2.1 Projecting the fitted GLM across current environmental layers\n\n```{r}\n#| label: projection_current\n#| echo: true\n#| message: false\n#| warning: false\n\nlibrary(terra)\n\n# Prepare environmental stack for projection\nproj_stack <- Env_UK_stack[[c(\"bio_1\", \"bio_12\", \"elevation\")]]\nnames(proj_stack)  # needs to match predictors in glm_step\n\n# Predict relative suitability across the raster stack\nglm_suitability_map <- terra::predict(proj_stack, glm_step, type = \"response\")\n\n# Plot the predicted suitability\nplot(glm_suitability_map,\n     main = \"Predicted habitat suitability for R. hipposideros (current)\",\n     col = viridis(50, option = \"D\"))\n```\n\nTo support conservation decisions or map projected ranges, we often need binary predictions — areas where the species is likely present (1) or absent (0) — instead of continuous suitability values. We’ll now apply a threshold to the predicted suitability map and visualize this binary prediction.\n\nWe’ll use the threshold identified earlier (e.g., *\"MaxSens+Spec\"*) to reclassify the continuous map.\n\n```{r}\n#| label: binary-map\n#| echo: true\n#| message: false\n#| warning: false\n\n# Create binary prediction map: 1 = presence, 0 = absence\nbinary_map <- glm_suitability_map >= chosen_thresh\n\n# Plot the binary prediction map\nplot(binary_map,\n     main = \"Predicted presence/absence of *Rhinolophus hipposideros* (current)\",\n     col = c(\"lightgrey\", \"darkgreen\"),\n     legend = FALSE)\nlegend(\"bottomleft\", legend = c(\"Absence\", \"Presence\"),\n       fill = c(\"lightgrey\", \"darkgreen\"), bty = \"n\")\n```\n\n<br>\n\n::: {.callout-note collapse=\"true\" icon=\"info-circle\"}\n### Why binary maps?\n\nBinary SDM outputs are often used in applied contexts — for example:\n\nEstimating species' range size\n\nIdentifying priority conservation areas\n\nCommunicating results to decision-makers\n\n*But*: Always interpret binary maps with caution. Thresholding reduces a continuous gradient to a yes/no decision, which can oversimplify ecological reality. That’s why it is recommend:\n\nReporting both continuous and binary maps\n\nExplaining how the threshold was chosen\n\nBeing transparent about limitations\n:::\n\n::: {.callout-tip collapse=\"true\" icon=\"light-bulb\"}\n### View interactive map of projected habitat suitability, presence/absence, and occurrence records\n\nWe can also explore an interactive version of the current habitat suitability map and binary map overlaid with occurrence records using **Leaflet**.\n\n```{r}\n#| label: leaflet_map\n#| echo: false\n#| message: false\n#| warning: false\n\nlibrary(leaflet)\nlibrary(terra)\nlibrary(sf)\n\n# Ensure raster layers are in WGS84 (leaflet requires EPSG:4326)\nsuitability_wgs84 <- project(glm_suitability_map, \"EPSG:4326\")\n\n# Create color palette for suitability (continuous)\npal_suit <- colorNumeric(\n  palette = alpha(inferno(50), 0.5),\n  domain = values(suitability_wgs84),\n  na.color = \"transparent\"\n)\n\n# Convert logical TRUE/FALSE raster to numeric 1/0\nbinary_map_numeric <- as.numeric(binary_map)\nbinary_map_factor <- as.factor(binary_map_numeric)\n\n# Project to WGS84 for leaflet\nbinary_wgs84 <- project(binary_map_factor, \"EPSG:4326\")\n\n# Define color palette\npal_bin <- colorFactor(\n  palette = c(\"lightgrey\", \"darkgreen\"),\n  domain = c(0, 1),\n  na.color = \"transparent\"\n)\n\n# Build the leaflet map\nleaflet() |>\n  # addTiles(group = \"Base Map\") |> #  default addTiles(): OpenStreetMap)\n  addProviderTiles(\"Esri.WorldImagery\", group = \"Satellite\") |>  #  Satellite base map\n    \n  # Add continuous suitability raster\n  addRasterImage(suitability_wgs84, colors = pal_suit, opacity = 0.4, group = \"Suitability (continuous)\") |>\n  addLegend(pal = pal_suit, values = values(suitability_wgs84),\n            title = \"Habitat Suitability\", position = \"topright\", group = \"Suitability (continuous)\") |>\n  \n  # Add binary presence/absence raster\n addRasterImage(binary_wgs84, colors = pal_bin, opacity = 0.4, group = \"Presence/Absence (binary)\") |>\n addLegend(\n  position = \"bottomright\",\n  colors = c(\"lightgrey\", \"darkgreen\"),\n  labels = c(\"Absence\", \"Presence\"),\n  title = \"Presence/Absence\",\n  opacity = 1\n) |>\n  \n  # Add occurrence points (red)\n  addCircleMarkers(\n    data = occ_rhinhipp_cleaned,\n    lng = ~decimalLongitude,\n    lat = ~decimalLatitude,\n    radius = 5,\n    color = \"black\",\n    fillColor = \"#e31a1c\",\n    fillOpacity = 0.9,\n    weight = 1,\n    group = \"Cleaned points\"\n  ) |>\n  \n  # Layer controls\n  addLayersControl(\n    baseGroups = c(\"Suitability (continuous)\", \"Presence/Absence (binary)\"),\n    overlayGroups = c(\"Cleaned points\"),\n    options = layersControlOptions(collapsed = FALSE)\n  ) |>\n  \n  addScaleBar(position = \"bottomleft\")\n\n\n```\n:::\n\n#### 2.2 Projecting under future climate scenarios\n\n```{r}\n\nlibrary(terra)\n\n# First, generate a raster stack for future climate and elevation data\n# Resample Elev_UK raster to match Clim_cmip6_2041_2060_UK raster\nElev_UK_aligned_future <- terra::resample(Elev_UK, Clim_cmip6_2041_2060_UK, method = \"bilinear\")\n# Crop to common extent\ncommon_extent_future <- terra::intersect(terra::ext(Clim_cmip6_2041_2060_UK), terra::ext(Elev_UK_aligned_future))\nClim_UK_future_crop2 <- terra::crop(Clim_cmip6_2041_2060_UK, common_extent_future)\nElev_UK_future_crop2 <- terra::crop(Elev_UK_aligned_future, common_extent_future)\n\n# Stack layers and name variables\nEnv_UK_stack_future <- c(Clim_UK_future_crop2 , Elev_UK_future_crop2)\nnames(Env_UK_stack_future) <- c(sub(\"^wc2\\\\.1_10m_\", \"\", names(Clim_UK_crop2)), \"elevation\")\n\n# Example: future CMIP6 scenario (SSP5-8.5, 2041-2060)\nfuture_stack <-  Env_UK_stack_future[[c(\"bio_1\", \"bio_12\", \"elevation\")]]\nnames(future_stack) <- c(\"bio_1\", \"bio_12\", \"elevation\")\n\n# Predict future relative suitability\nfuture_suitability <- terra::predict(future_stack, glm_step, type = \"response\")\n\n# Plot future suitability\nplot(future_suitability,\n     main = \"Projected habitat suitability under CMIP-6 (2041-2060)\",\n     col = viridis(50, option = \"E\"))\n```\n\n::: {.callout-note collapse=\"true\" icon=\"lightbulb\"}\n### Literature\n\nAraujo, M. B., Pearson, R. G., Thuiller, W., & Erhard, M. (2005). **Validation of species-climate impact models under climate change.** *Global Change Biology*, 11(9), 1504–1513. [doi:10.1111/j.1365-2486.2005.001000.x](https://doi.org/10.1111/j.1365-2486.2005.001000.x)\n\nHirzel, A. H., Le Lay, G., Helfer, V., Randin, C., & Guisan, A. (2006). **Evaluating the ability of habitat suitability models to predict species presences.** *Ecological Modelling*, 199(2), 142–152. [doi:10.1016/j.ecolmodel.2006.05.017](https://doi.org/10.1016/j.ecolmodel.2006.05.017)\n\nLiu, C., Berry, P. M., Dawson, T. P., & Pearson, R. G. (2005). **Selecting thresholds of occurrence in the prediction of species distributions.** *Ecography*, 28(3), 385–393. doi:  [doi:10.1111/j.0906-7590.2005.03957.x](https://doi.org/10.1111/j.0906-7590.2005.03957.x)\n\nSmith, A. B., & Santos, M. J. (2020). **Testing the ability of species distribution models to infer variable importance.** *Ecography*, 43(12), 1801–1813. [doi:10.1111/ecog.05317] (https://doi.org/10.1111/ecog.05317)\n\nWilson, K. A., Westphal, M. I., Possingham, H. P., & Elith, J. (2005). **Sensitivity of conservation planning to different approaches to using predicted species distribution data.** *Biological Conservation*, 122(1), 99–112.\n\nZurell, D., Franklin, J., König, C., Bouchet, P.J., Dormann, C.F., Elith, J., Fandos, G., Feng, X., Guillera-Arroita, G., Guisan, A., Lahoz-Monfort, J.J., Leitão, P.J., Park, D.S., Peterson, A.T., Rapacciuolo, G., Schmatz, D.R., Schröder, B., Serra-Diaz, J.M., Thuiller, W., Yates, Katherine L., Zimmermann, Niklaus E. & Merow, C. (2020). **A standard protocol for reporting species distribution models.** *Ecography*, 43(9), 1261–1277. [doi:10.1111/ecog.04960](https://doi.org/10.1111/ecog.04960)\n:::\n","srcMarkdownNoYaml":"\n\n```{r}\n#| label: setup-assessproject\n#| echo: false\n#| include: false\n#| warning: false\n#| message: false\n\n# Load required packages\nsource(\"scripts/load_packages.R\")\n\n# Load shared environmental and species data\nsource(\"scripts/shared_sdm_data.R\")\n\n# Load shared SDM models\nsource(\"scripts/shared_sdm_models.R\")\n```\n\n<br>\n\nAfter a species distribution model (SDM) has been fitted, the next critical steps are to **assess its performance** and **project it across space or time**. Model assessment ensures that the relationships identified between species occurrences and environmental predictors are robust, interpretable, and generalisable. Projection then allows us to translate these relationships into maps of habitat suitability, either under current conditions or for future scenarios such as climate change.\n\n**Model assessment** involves several interconnected steps:\\\n**Model fit**: Examining how well the model explains the training data.\\\n**Model validation**: Testing model predictions on independent data (data not used for fitting) to avoid overfitting and assess generalization.\\\n**Model evaluation**: Calculating quantitative performance metrics on the validation data to objectively judge model quality.\\\n\nA common and recommended approach is to **split the available data into training and testing sets** or use **cross-validation** methods. This ensures that evaluation statistics such as AUC or TSS reflect the model’s predictive ability rather than just its capacity to reproduce the training data.\n\nEvaluation metrics fall into two main types:\\\n**Threshold-independent metrics** like AUC and Continuous Boyce Index evaluate the model’s discrimination ability without requiring a binary classification.\\\n**Threshold-dependent metrics**\\*\\* like sensitivity, specificity, and True Skill Statistic (TSS) require converting continuous predictions into binary outcomes and assess classification accuracy at selected thresholds.\\\n\nTogether, these evaluation metrics, when applied on validation data, provide a comprehensive picture of model performance.\n\n**Projection** extends model application by using fitted relationships to predict species distributions across geographic space, or under different environmental conditions. This is particularly powerful for biodiversity monitoring, invasive species risk mapping, and forecasting impacts of global change. However, projections must always be interpreted in light of uncertainties stemming from data quality, model choice, and environmental scenarios.\n\n::: {.callout-note collapse=\"true\" icon=\"info\"}\n### Essential SDM Workflow: Fitting, Validating, Evaluation and Projection\n\nWhen building species distribution models (SDMs), it's essential to distinguish between **model fit**, **model validation**, **model evaluation**, and **final projection**. These are distinct but interconnected steps in a robust modelling workflow.\n\n------------------------------------------------------------------------\n\n#### **Model fit**\n\nRefers to how well a model explains the data it was **trained on**. This can be assessed using residuals, likelihood measures, or internal performance statistics. However, good fit to training data does **not guarantee predictive accuracy** — models can overfit, especially with small sample sizes or many predictors.\n\n------------------------------------------------------------------------\n\n#### **Model validation**\n\nEvaluates how well the model **generalizes to new data**. This requires testing predictions on **data not used for model fitting**, typically via: - **Data splitting** (e.g., 70% training, 30% testing), or\\\n- **Cross-validation** (e.g., k-fold, spatial block).\n\nValidation ensures the model's performance is **not just memorizing patterns in the training data**, but reflects true predictive power.\n\n------------------------------------------------------------------------\n\n#### **Model evaluation**\n\nInvolves calculating quantitative performance metrics (e.g., AUC, TSS, Boyce Index) on the **validation (test) data**. This step quantifies how well the model distinguishes presences from absences, both: - **Threshold-independently** (e.g., AUC, Boyce), and\\\n- **Threshold-dependently** (e.g., sensitivity, specificity, TSS).\n\nEvaluation provides **objective evidence** of model reliability.\n\n------------------------------------------------------------------------\n\n#### **Final projection**\n\nOnce the model has been validated and evaluated: - You can **refit the final model using all available data** (training + test) to maximize information. - Use this full model to **project species distributions** across space (e.g., habitat suitability maps) or time (e.g., future climate scenarios).\n\n**Why refit with all data?**\\\nTo improve accuracy of projections, especially for rare species or sparse datasets. But remember: this projection step should **only happen after validation** is complete.\n\n------------------------------------------------------------------------\n\n**Best practice workflow summary**:\n\n1.  **Split** data → for training and validation.\\\n2.  **Fit** model on training data.\\\n3.  **Validate & evaluate** on test data.\\\n4.  **Refit** model on all data.\\\n5.  **Project** using the refitted model.\n\nThis approach supports both **scientific rigor** and **reliable application** of SDMs in conservation, forecasting, and spatial planning.\n:::\n\n<br>\n\n### 1. Model evaluation\n\nModel evaluation is a key step in SDM analysis. Since SDMs typically predict a continuous probability of occurrence or relative suitability, while field data are often binary (presence vs. absence/pseudo-absence), we need appropriate metrics to judge model quality.\n\nA critical principle in SDM evaluation is to **assess model predictions on independent data** not used during model fitting. This can be achieved by:\\\n**Data splitting**: dividing the dataset into training and testing subsets, fitting the model only on training data, then evaluating on testing data.\\\n**Cross-validation**: repeatedly splitting the data into folds, training on some folds and testing on the others, to obtain robust performance estimates. Evaluating models on independent data helps to avoid overly optimistic assessments caused by overfitting and better reflects how the model will perform in real-world applications.\n\nAfter a model is fitted, and data have been partitioned appropriately, evaluation metrics are calculated on the validation (test) data to assess predictive performance based on either\n\n**Threshold-independent** or **Threshold-dependent** metrics.\n\nFollowing best-practice, we now split our data into training and testing subsets and refit the GLM model using only the training data before performing any validation.\n\n```{r}\n#| label: data_split\n#| echo: true\n#| warning: false\n#| message: false\n\nlibrary(caret)\n\nset.seed(123)\n# Generate an index that assign 70% of data to training data\ntrainIndex <- createDataPartition(sdm_df_clean$pa, p = .7, list = FALSE)\nsdm_df_train <- sdm_df_clean[trainIndex, ]\nsdm_df_test <- sdm_df_clean[-trainIndex, ]\n\n# Fit model on training data\nglm_multi_train <- glm(pa ~ poly(elevation, 2) + poly(bio_1, 2) + poly(bio_12, 2),\n                 data = sdm_df_train,\n                 family = binomial)\n\nglm_step_train <- step(glm_multi_train , direction = \"both\", trace = FALSE)\n\n```\n\n#### 1.1 Threshold-independent evaluation\n\nThe most widely used threshold-independent measure is the **area under the ROC curve (AUC)**. AUC evaluates how well the model distinguishes presences from absences across all possible thresholds. An AUC of 0.5 indicates random performance, while values above 0.7 are typically considered fair, and above 0.9 excellent (Araujo et al. 2005).\n\n```{r}\n#| label: auc-eval\n#| echo: true\n#| warning: false\n#| message: false\n\nlibrary(pROC)\n\n# Predict on test data\nglm_predict_test <- predict(glm_step_train , newdata = sdm_df_test, type = \"response\")\n\n# 'Observed' test data (presence/pseudoabsence)\nobs_test <- sdm_df_test$pa\n\n# Evaluate with ROC and AUC on test data\nroc_glm <- pROC::roc(obs_test, glm_predict_test)\nauc_glm <- pROC::auc(roc_glm)\n\n# Print AUC\nprint(auc_glm)\n\n# Plot ROC curve\nplot(roc_glm, col = \"#e31a1c\", lwd = 2, main = \"ROC Curve\")\nabline(a = 0, b = 1, lty = 2, col = \"grey\")\n```\n\n::: {.callout-note collapse=\"true\" icon=\"info-circle\"}\n### How to interpret the ROC curve\n\n-   The ROC curve plots **sensitivity (true positive rate)** against **1 – specificity (false positive rate)** across all thresholds.\\\n-   A model with **no predictive ability** follows the diagonal line (AUC ≈ 0.5).\\\n-   Curves that **bow towards the upper-left corner** indicate stronger discrimination between presences and absences.\\\n-   The **area under the curve (AUC)** provides a summary metric:\n    -   **AUC = 0.5** → random predictions\\\n    -   **0.7 ≤ AUC \\< 0.8** → fair\\\n    -   **0.8 ≤ AUC \\< 0.9** → good\\\n    -   **AUC ≥ 0.9** → excellent\n\nAlways interpret AUC together with ecological reasoning and other evaluation measures\\\n(e.g., TSS, Boyce index), since AUC alone can be misleading if prevalence is unbalanced\\\nor sampling design introduces bias.\n:::\n\n<br>\n\nThe **Continuous Boyce Index (CBI)** is an evaluation metric used for species distribution models (SDMs) and habitat suitability models that are built using presence-only data. The CBI is particularly useful for SDMs, as it directly evaluates how predictions for observed presences deviate from a random distribution of available habitat suitability values. It ranges from -1 (counter-predictive), to 0 (no better than random), to +1 (perfect agreement between model predictions and presence distribution).\n\n```{r}\n#| label: boyce-eval\n#| echo: true\n#| warning: false\n#| message: false\n\nlibrary(ecospat)\n\n# Compute Continuous Boyce Index (CBI)\nboyce_res <- ecospat.boyce(\n  fit = glm_predict_test,            # model predictions\n  obs = glm_predict_test[obs_test == 1],  # predictions at presence points\n  nclass = 0,                  # automatic binning\n  window.w = \"default\"         # default moving window\n)\n\nboyce_res$Spearman.cor\n```\n\n::: {.callout-note collapse=\"true\" icon=\"info-circle\"}\n### How to interprete the Boyce curve\n\n-   **X-axis (suitability):** predicted suitability scores from the SDM.\\\n-   **Y-axis (predicted/expected ratio):** how much more (or less) frequently presences are found in each suitability class than expected at random.\n\n**Guidelines:**\\\n- A flat line near **1** → model performs no better than random.\\\n- An increasing curve, with ratios **\\>1** in high suitability bins → good model performance.\\\n- Ratios **\\<1** in high suitability bins → the model may be misleading.\n\nWhile P/E ratios in the plot may exceed 1, the Boyce Index correlation ranges from -1 to +1..\n:::\n\n<br>\n\n#### 1.2 Threshold-dependent evaluation\n\nTo compute threshold-dependent metrics, we need to choose a threshold that converts continuous probabilities into binary predictions. Several rules exist (see Liu et al., 2005), such as:\n\nMaximizing sensitivity + specificity\n\nEqual sensitivity and specificity\n\nFixed probability cutoffs (e.g., 0.5)\n\nHere we use the `PresenceAbsence` package to determine an optimal threshold and then compute metrics like sensitivity, specificity, and the True Skill Statistic (TSS).\n\n```{r}\n#| label: threshold-eval\n#| echo: true\n#| warning: false\n#| message: false\n\nlibrary(PresenceAbsence)\n\n# Construct PresenceAbsence dataframe\n# Ensure observed and predicted values are numeric\npa_df_test <- data.frame(\n  ID = seq_along(obs_test),\n  observed = as.numeric(as.character(obs_test)),\n  predicted = as.numeric(glm_predict_test)\n  )\n\n# Remove any rows with missing values\npa_df_test <- pa_df_test[complete.cases(pa_df_test), ]\n\n# Find threshold\n(opt_thresh <- PresenceAbsence::optimal.thresholds(\n                DATA= pa_df_test,\n                opt.methods = c('MaxSens+Spec', 'Sens=Spec', 'MinROCdist', 'MaxKappa')))\n\n# Use one threshold (e.g., MaxSens+Spec)\n    chosen_thresh <- opt_thresh$predicted[opt_thresh$Method == 'MaxSens+Spec']  \n\n# Classify predictions\npred_binary <- ifelse(as.numeric(glm_predict_test) >= chosen_thresh, 1, 0)    \n\n# Confusion matrix\ncm <- table(Observed = obs_test, Predicted = pred_binary)\nprint(cm)\n\n# Compute sensitivity, specificity, TSS\nsensitivity <- sum(pred_binary == 1 & obs_test == 1) / sum(obs_test == 1)\nspecificity <- sum(pred_binary == 0 & obs_test == 0) / sum(obs_test == 0)\ntss <- sensitivity + specificity - 1\n\nlist(ConfusionMatrix = cm,\n      Sensitivity = sensitivity,\n      Specificity = specificity,\n      TSS = tss)   \n      \n```\n\n```{r}\n#| label: threshold-safe\n#| echo: false\n\nlibrary(PresenceAbsence)\n\n# Construct PresenceAbsence dataframe\n# Ensure observed and predicted values are numeric\npa_df_test <- data.frame(\n  ID = seq_along(obs_test),\n  observed = as.numeric(as.character(obs_test)),\n  predicted = as.numeric(glm_predict_test)\n  )\n\n# Remove any rows with missing values\npa_df_test <- pa_df_test[complete.cases(pa_df_test), ]\n\n# Compute thresholds\nopt_thresh <- optimal.thresholds(DATA = pa_df_test,\n                                opt.methods = c('MaxSens+Spec', 'Sens=Spec', 'MinROCdist', 'MaxKappa'))\n\n# Safe selection of threshold\nif(!is.null(opt_thresh$predicted)) {\n  chosen_thresh <- opt_thresh$predicted[opt_thresh$Method == 'MaxSens+Spec']\n}\n\n# Fallback if threshold missing or unrealistic\nif(!exists(\"chosen_thresh\") || is.na(chosen_thresh) || chosen_thresh <= 0 || chosen_thresh >= 1) {\n  chosen_thresh <- 0.5\n  message(\"chosen_thresh missing or unrealistic; using default = 0.5\")\n}\n```\n\n::: {.callout-note collapse=\"true\" icon=\"info-circle\"}\n### Choosing thresholds for binary predictions\n\nSpecies distribution models typically produce **continuous predictions** (relative suitability or probability of occurrence).\\\nTo evaluate performance or produce presence/absence maps, these need to be **converted into binary predictions**.\\\nThere is no single \"best\" threshold. Instead, different methods emphasize different trade-offs.\n\nHere are four widely used options:\n\n-   **MaxSens+Spec**\\\n    Maximizes the sum of sensitivity (true positive rate) and specificity (true negative rate).\\\n    → Balanced choice when false positives and false negatives are equally costly.\n\n-   **Sens=Spec**\\\n    Selects the threshold where sensitivity and specificity are equal.\\\n    → Useful when both error types are equally undesirable.\n\n-   **MinROCdist**\\\n    Minimizes the distance between the ROC curve and the perfect classifier point (0,1).\\\n    → Robust option, often similar to MaxSens+Spec but less sensitive to skewed prevalence.\n\n-   **MaxKappa**\\\n    Maximizes Cohen’s Kappa statistic, which accounts for agreement expected by chance.\\\n    → Historically popular in ecology, still informative for comparing classifiers.\n\nIt is good practice to report **multiple thresholds**.\\\nThis ensures transparency, allows comparison across studies, and lets end-users (e.g., conservation managers) choose the threshold best suited to their objectives (e.g., maximizing detection vs. minimizing false alarms).\n:::\n\n**Interpretation**\n\n**AUC**: reflects overall discriminatory ability, independent of any threshold.\n\n**Threshold-dependent metrics**: provide information about model performance in classifying presences vs. absences at a chosen threshold.\n\n**TSS**: balances sensitivity and specificity, commonly used in SDM studies.\n\nIn practice, both perspectives are valuable. Threshold-independent measures provide a broad sense of predictive skill, while threshold-dependent measures allow for more concrete classification performance.\n\n### 2. Projection across space and time\n\nOnce a species distribution model has been calibrated and evaluated, the next step is projection: predicting habitat suitability across geographic space or future environmental scenarios. Projections enable us to identify potential ranges, assess climate change impacts, and inform conservation planning.\n\nThere are two main projection contexts:\n\n**Spatial projection**: applying the model to current environmental layers across the study area to produce a habitat suitability map.\n\n**Temporal projection**: applying the model to future climate scenarios (e.g., CMIP6 SSPs) to forecast potential range shifts.\n\n::: {.callout-tip collapse=\"true\" icon=\"globe\"}\\\n\\### Important considerations when projecting SDMs\n\nEnsure predictor variables in the projection layers match the model in units, resolution, and extent.\n\nBeware of extrapolation: predictions in environmental space outside the range of training data are uncertain.\n\nFor presence–pseudoabsence models, predictions remain relative suitability, not absolute probabilities.\n\nWhen projecting to future climates, using ensembles of GCMs helps capture uncertainty. :::\n\n<br>\n\n#### 2.1 Projecting the fitted GLM across current environmental layers\n\n```{r}\n#| label: projection_current\n#| echo: true\n#| message: false\n#| warning: false\n\nlibrary(terra)\n\n# Prepare environmental stack for projection\nproj_stack <- Env_UK_stack[[c(\"bio_1\", \"bio_12\", \"elevation\")]]\nnames(proj_stack)  # needs to match predictors in glm_step\n\n# Predict relative suitability across the raster stack\nglm_suitability_map <- terra::predict(proj_stack, glm_step, type = \"response\")\n\n# Plot the predicted suitability\nplot(glm_suitability_map,\n     main = \"Predicted habitat suitability for R. hipposideros (current)\",\n     col = viridis(50, option = \"D\"))\n```\n\nTo support conservation decisions or map projected ranges, we often need binary predictions — areas where the species is likely present (1) or absent (0) — instead of continuous suitability values. We’ll now apply a threshold to the predicted suitability map and visualize this binary prediction.\n\nWe’ll use the threshold identified earlier (e.g., *\"MaxSens+Spec\"*) to reclassify the continuous map.\n\n```{r}\n#| label: binary-map\n#| echo: true\n#| message: false\n#| warning: false\n\n# Create binary prediction map: 1 = presence, 0 = absence\nbinary_map <- glm_suitability_map >= chosen_thresh\n\n# Plot the binary prediction map\nplot(binary_map,\n     main = \"Predicted presence/absence of *Rhinolophus hipposideros* (current)\",\n     col = c(\"lightgrey\", \"darkgreen\"),\n     legend = FALSE)\nlegend(\"bottomleft\", legend = c(\"Absence\", \"Presence\"),\n       fill = c(\"lightgrey\", \"darkgreen\"), bty = \"n\")\n```\n\n<br>\n\n::: {.callout-note collapse=\"true\" icon=\"info-circle\"}\n### Why binary maps?\n\nBinary SDM outputs are often used in applied contexts — for example:\n\nEstimating species' range size\n\nIdentifying priority conservation areas\n\nCommunicating results to decision-makers\n\n*But*: Always interpret binary maps with caution. Thresholding reduces a continuous gradient to a yes/no decision, which can oversimplify ecological reality. That’s why it is recommend:\n\nReporting both continuous and binary maps\n\nExplaining how the threshold was chosen\n\nBeing transparent about limitations\n:::\n\n::: {.callout-tip collapse=\"true\" icon=\"light-bulb\"}\n### View interactive map of projected habitat suitability, presence/absence, and occurrence records\n\nWe can also explore an interactive version of the current habitat suitability map and binary map overlaid with occurrence records using **Leaflet**.\n\n```{r}\n#| label: leaflet_map\n#| echo: false\n#| message: false\n#| warning: false\n\nlibrary(leaflet)\nlibrary(terra)\nlibrary(sf)\n\n# Ensure raster layers are in WGS84 (leaflet requires EPSG:4326)\nsuitability_wgs84 <- project(glm_suitability_map, \"EPSG:4326\")\n\n# Create color palette for suitability (continuous)\npal_suit <- colorNumeric(\n  palette = alpha(inferno(50), 0.5),\n  domain = values(suitability_wgs84),\n  na.color = \"transparent\"\n)\n\n# Convert logical TRUE/FALSE raster to numeric 1/0\nbinary_map_numeric <- as.numeric(binary_map)\nbinary_map_factor <- as.factor(binary_map_numeric)\n\n# Project to WGS84 for leaflet\nbinary_wgs84 <- project(binary_map_factor, \"EPSG:4326\")\n\n# Define color palette\npal_bin <- colorFactor(\n  palette = c(\"lightgrey\", \"darkgreen\"),\n  domain = c(0, 1),\n  na.color = \"transparent\"\n)\n\n# Build the leaflet map\nleaflet() |>\n  # addTiles(group = \"Base Map\") |> #  default addTiles(): OpenStreetMap)\n  addProviderTiles(\"Esri.WorldImagery\", group = \"Satellite\") |>  #  Satellite base map\n    \n  # Add continuous suitability raster\n  addRasterImage(suitability_wgs84, colors = pal_suit, opacity = 0.4, group = \"Suitability (continuous)\") |>\n  addLegend(pal = pal_suit, values = values(suitability_wgs84),\n            title = \"Habitat Suitability\", position = \"topright\", group = \"Suitability (continuous)\") |>\n  \n  # Add binary presence/absence raster\n addRasterImage(binary_wgs84, colors = pal_bin, opacity = 0.4, group = \"Presence/Absence (binary)\") |>\n addLegend(\n  position = \"bottomright\",\n  colors = c(\"lightgrey\", \"darkgreen\"),\n  labels = c(\"Absence\", \"Presence\"),\n  title = \"Presence/Absence\",\n  opacity = 1\n) |>\n  \n  # Add occurrence points (red)\n  addCircleMarkers(\n    data = occ_rhinhipp_cleaned,\n    lng = ~decimalLongitude,\n    lat = ~decimalLatitude,\n    radius = 5,\n    color = \"black\",\n    fillColor = \"#e31a1c\",\n    fillOpacity = 0.9,\n    weight = 1,\n    group = \"Cleaned points\"\n  ) |>\n  \n  # Layer controls\n  addLayersControl(\n    baseGroups = c(\"Suitability (continuous)\", \"Presence/Absence (binary)\"),\n    overlayGroups = c(\"Cleaned points\"),\n    options = layersControlOptions(collapsed = FALSE)\n  ) |>\n  \n  addScaleBar(position = \"bottomleft\")\n\n\n```\n:::\n\n#### 2.2 Projecting under future climate scenarios\n\n```{r}\n\nlibrary(terra)\n\n# First, generate a raster stack for future climate and elevation data\n# Resample Elev_UK raster to match Clim_cmip6_2041_2060_UK raster\nElev_UK_aligned_future <- terra::resample(Elev_UK, Clim_cmip6_2041_2060_UK, method = \"bilinear\")\n# Crop to common extent\ncommon_extent_future <- terra::intersect(terra::ext(Clim_cmip6_2041_2060_UK), terra::ext(Elev_UK_aligned_future))\nClim_UK_future_crop2 <- terra::crop(Clim_cmip6_2041_2060_UK, common_extent_future)\nElev_UK_future_crop2 <- terra::crop(Elev_UK_aligned_future, common_extent_future)\n\n# Stack layers and name variables\nEnv_UK_stack_future <- c(Clim_UK_future_crop2 , Elev_UK_future_crop2)\nnames(Env_UK_stack_future) <- c(sub(\"^wc2\\\\.1_10m_\", \"\", names(Clim_UK_crop2)), \"elevation\")\n\n# Example: future CMIP6 scenario (SSP5-8.5, 2041-2060)\nfuture_stack <-  Env_UK_stack_future[[c(\"bio_1\", \"bio_12\", \"elevation\")]]\nnames(future_stack) <- c(\"bio_1\", \"bio_12\", \"elevation\")\n\n# Predict future relative suitability\nfuture_suitability <- terra::predict(future_stack, glm_step, type = \"response\")\n\n# Plot future suitability\nplot(future_suitability,\n     main = \"Projected habitat suitability under CMIP-6 (2041-2060)\",\n     col = viridis(50, option = \"E\"))\n```\n\n::: {.callout-note collapse=\"true\" icon=\"lightbulb\"}\n### Literature\n\nAraujo, M. B., Pearson, R. G., Thuiller, W., & Erhard, M. (2005). **Validation of species-climate impact models under climate change.** *Global Change Biology*, 11(9), 1504–1513. [doi:10.1111/j.1365-2486.2005.001000.x](https://doi.org/10.1111/j.1365-2486.2005.001000.x)\n\nHirzel, A. H., Le Lay, G., Helfer, V., Randin, C., & Guisan, A. (2006). **Evaluating the ability of habitat suitability models to predict species presences.** *Ecological Modelling*, 199(2), 142–152. [doi:10.1016/j.ecolmodel.2006.05.017](https://doi.org/10.1016/j.ecolmodel.2006.05.017)\n\nLiu, C., Berry, P. M., Dawson, T. P., & Pearson, R. G. (2005). **Selecting thresholds of occurrence in the prediction of species distributions.** *Ecography*, 28(3), 385–393. doi:  [doi:10.1111/j.0906-7590.2005.03957.x](https://doi.org/10.1111/j.0906-7590.2005.03957.x)\n\nSmith, A. B., & Santos, M. J. (2020). **Testing the ability of species distribution models to infer variable importance.** *Ecography*, 43(12), 1801–1813. [doi:10.1111/ecog.05317] (https://doi.org/10.1111/ecog.05317)\n\nWilson, K. A., Westphal, M. I., Possingham, H. P., & Elith, J. (2005). **Sensitivity of conservation planning to different approaches to using predicted species distribution data.** *Biological Conservation*, 122(1), 99–112.\n\nZurell, D., Franklin, J., König, C., Bouchet, P.J., Dormann, C.F., Elith, J., Fandos, G., Feng, X., Guillera-Arroita, G., Guisan, A., Lahoz-Monfort, J.J., Leitão, P.J., Park, D.S., Peterson, A.T., Rapacciuolo, G., Schmatz, D.R., Schröder, B., Serra-Diaz, J.M., Thuiller, W., Yates, Katherine L., Zimmermann, Niklaus E. & Merow, C. (2020). **A standard protocol for reporting species distribution models.** *Ecography*, 43(9), 1261–1277. [doi:10.1111/ecog.04960](https://doi.org/10.1111/ecog.04960)\n:::\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":true,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"setup":"scripts/load_packages.R","message":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"css":["styles.css"],"output-file":"page_SDM.7_SDMAssessmentProjection.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.24","pages":["index.qmd","page_SDM.1_DataPreparation.qmd"],"theme":"cosmo","toc-location":"left","page-layout":"full","code-copy":true,"title":"Assessment & Projection","editor":"visual","draft":false},"extensions":{"book":{"multiFile":true}}}},"draft":false,"projectFormats":["html","pdf"]}
[
  {
    "objectID": "page_SDM.9_SDMEnsembleModelFitting.html",
    "href": "page_SDM.9_SDMEnsembleModelFitting.html",
    "title": "Ensemble Model Fitting",
    "section": "",
    "text": "Species distribution models (SDMs) are, at best, imperfect approximations of reality. They rely on human-defined mathematical structures to represent complex ecological processes, forcing observational data into simplified functional forms that may not fully capture the underlying biological mechanisms. Each model is a filtered view, shaped by assumptions, algorithmic constraints, and the quality of the available data.\nExploring different algorithms for SDM has shown that every method carries its own strengths and limitations—no single model consistently outperforms others across species, regions, or scenarios. This recognition has led to the growing adoption of ensemble modelling, which combines predictions from multiple algorithms to generate more robust, reliable, and generalisable estimates of species’ potential distributions.\nThe core idea behind ensemble modelling is that each individual model captures a mix of signal (i.e. captures some of the genuine ecological patterns) and noise (errors, biases, or uncertainties). By aggregating across models, ensembles aim to amplify the signal while dampening the noise—leading to improved predictive performance and more defensible ecological inference.\nBy combining multiple models, ensemble approaches aim not only to amplify the signal and suppress the noise, but also to provide a more reliable central tendency (e.g. mean or consensus prediction) across different modelling approaches. Crucially, ensembles do more than just improve average predictive performance, as they also allow us to quantify uncertainty more effectively. By comparing variation among model predictions, we can estimate the degree of confidence in our forecasts and highlight areas of high or low agreement. This makes ensembles particularly valuable for decision-making and risk assessment under uncertainty, such as conservation planning or climate impact assessments."
  },
  {
    "objectID": "page_SDM.9_SDMEnsembleModelFitting.html#ensemble-modelling-strategies",
    "href": "page_SDM.9_SDMEnsembleModelFitting.html#ensemble-modelling-strategies",
    "title": "Ensemble Model Fitting",
    "section": "1. Ensemble Modelling Strategies",
    "text": "1. Ensemble Modelling Strategies\nEnsemble modelling in species distribution modelling (SDM) is not a single method but a framework that can incorporate different strategies for combining predictions. Broadly, ensemble methods can be grouped into three categories, depending on how models are weighted and how uncertainty is handled:\n\n\n1.1 Simple averaging\nThe most basic ensemble method calculates the unweighted average of predictions from multiple models: • Every algorithm contributes equally to the final prediction. • Assumes all models are equally informative and reliable. • Easy to implement and interpret. When to use: When you have little prior reason to favour one model over another, or want a straightforward consensus.\n\n\n\n1.2 Weighted averaging\nIn this approach, models are weighted according to performance metrics such as AUC, TSS, or cross-validated deviance: • Better-performing models contribute more heavily to the final ensemble. • Weights can be calculated based on internal validation (e.g., cross-validation AUC), independent test data, or expert judgment. When to use: When you want to prioritize models with stronger support from the data, while still retaining a diverse model set.\n\n\n\n1.3 Consensus-based voting or thresholding\nHere, ensemble predictions are made by voting across binary outputs (e.g. habitat suitable/unsuitable): • A site is predicted suitable if a majority of models agree. • Can also use stricter rules (e.g. unanimity) or soft thresholds (e.g. 70% agreement). • Useful when decisions are binary (e.g. protect vs not protect) or when interpretation of continuous probabilities is unclear. When to use: In policy or management contexts that require clear thresholds for decision-making.\n\n\n\n\n\n\nNotePractical Considerations for Ensemble Modelling\n\n\n\n\n\n\nAligning model outputs\nBefore combining predictions from different SDM algorithms, it’s essential to ensure they are aligned:\n\nAll models should be projected onto the same spatial extent, resolution, and use the same environmental layers.\nPredictions must be on the same scale (e.g. probabilities between 0 and 1).\nModel names should be clearly matched to their corresponding prediction layers or files to ensure reproducibility.\n\n\n\nIncluding uncertainty in ensembles\nWhile most ensemble implementations focus on generating a central prediction (e.g. mean or weighted mean), it is equally important to assess and communicate uncertainty:\n\nInclude proxies for model disagreement, such as standard deviation (SD) across predictions.\nCalculate quantiles or prediction intervals, which are especially useful when communicating risk or making conservative decisions.\nIt is good practice to report both the ensemble mean and an uncertainty layer (e.g. SD or interquartile range) in any spatial output intended for planning or publication."
  },
  {
    "objectID": "page_SDM.9_SDMEnsembleModelFitting.html#manual-ensemble-modelling",
    "href": "page_SDM.9_SDMEnsembleModelFitting.html#manual-ensemble-modelling",
    "title": "Ensemble Model Fitting",
    "section": "2 Manual Ensemble Modelling",
    "text": "2 Manual Ensemble Modelling\nAfter fitting multiple SDM algorithms and generating predictions on the same test dataset or raster stack, we can manually construct ensemble predictions by combining these outputs.\n\n2.1 Prepare model predictions and evaluation metrics\nFor this example, suppose we have predicted probabilities from four based on GLM, GAM, BRT and randomForest models on the test data: glm_predict_test, gam_predict_test, brt_predict_test, rF_predict_test and their respective AUC values from validation: auc_glm, auc_gam, auc_brt, auc_rF\n\n\n2.2 Simple average ensemble\n\n# Combine predictions into a data frame\nmodels_predict_df &lt;- data.frame(GLM = as.numeric(glm_predict_test),\n                      GAM = as.numeric(gam_predict_test),\n                      BRT = as.numeric(brt_predict_test),\n                      RF  = as.numeric(rF_predict_test))\n\n# Simple average ensemble prediction\nensemble_predict_simple &lt;- rowMeans(models_predict_df)\n\n\n\n2.3 Weighted average ensemble\n\n# Create weight vector based on AUCs\nmodels_auc_values &lt;- c(auc_glm, auc_gam, auc_brt, auc_rF)\nweights_auc &lt;- models_auc_values / sum(models_auc_values)\n\n\n# Weighted average ensemble prediction\nensemble_predict_weighted &lt;- as.numeric(as.matrix(models_predict_df) %*% weights_auc)\n\n\n\n2.4 Quantify ensemble uncertainty\n\nensemble_predict_sd &lt;- apply(models_predict_df, 1, sd)\n\n\n\n2.4 Visualise ensemble prediction and uncertainty\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\n# Create data frame for plotting\nplot_df &lt;- tibble(\n  Observation = 1:length(ensemble_predict_simple),\n  SimpleMean = ensemble_predict_simple,\n  WeightedMean = ensemble_predict_weighted,\n  Uncertainty = ensemble_predict_sd\n) %&gt;%\n  pivot_longer(cols = c(SimpleMean, WeightedMean), \n               names_to = \"EnsembleType\", values_to = \"Prediction\")\n\n# Plot predictions with uncertainty ribbon\nggplot(plot_df, aes(x = Observation, y = Prediction, color = EnsembleType)) +\n  geom_line() +\n  geom_ribbon(aes(ymin = Prediction - Uncertainty, ymax = Prediction + Uncertainty),\n              fill = \"grey80\", alpha = 0.3, color = NA) +\n  labs(title = \"Ensemble Predictions with Uncertainty\",\n       y = \"Predicted Probability\",\n       x = \"Test Sample Index\") +\n  theme_minimal()"
  },
  {
    "objectID": "page_SDM.9_SDMEnsembleModelFitting.html#packages-for-constructing-ensemble-sdms-in-r",
    "href": "page_SDM.9_SDMEnsembleModelFitting.html#packages-for-constructing-ensemble-sdms-in-r",
    "title": "Ensemble Model Fitting",
    "section": "3. Packages for Constructing Ensemble SDMs in R",
    "text": "3. Packages for Constructing Ensemble SDMs in R\nR offers several packages for building and combining species distribution models, each with different strengths, model algorithms, and ways of handling ensemble predictions. Three commonly used tools are biomod2, sdm, and ENMeval.\n\n3.1 Using biomod2\nbiomod2 is a powerful and flexible package designed specifically for ensemble species distribution modelling. It supports a wide range of modelling algorithms (e.g., GLM, GAM, RF, Maxent), provides built-in tools for data preprocessing and evaluation, and includes robust ensemble functionalities.\n\nKey Features:\n\nSupports up to ten different SDM algorithms\nAutomates cross-validation and model tuning\nAllows weighted ensemble predictions\nCan output uncertainty layers (e.g. confidence intervals, variance)\n\n\n\nbiomod2 basic Workflow\nHere is an outline of how to run ensemble SDMs using biomod2:\n\nbiomod2 step 1: Format the input data\nTo use biomod2, we need to provide presence–absence data coded as 1s and 0s, corresponding geographic coordinates (longitude and latitude), a stack of environmental predictor layers, a species name, and optional also background or absence points (if not provided will be generated by the package). Here we use the data generated earlier in the worshop (and stored also as saves/shared data for this worshop) and reformat then into the required object.\n\nlibrary(biomod2)\nlibrary(sf)\nlibrary(terra)\nlibrary(dplyr)\n\n# Load and prepare presence data\npresence_df &lt;- presence_sf %&gt;%\n  mutate(resp = 1) %&gt;%                         # Add response variable (1 = presence)\n  st_coordinates() %&gt;%                         # Extract coordinates\n  as.data.frame() %&gt;%                          # Convert to data frame\n  bind_cols(resp = 1)                          # Append response column again for clarity\n\n# Load and prepare background (pseudo-absence) data\nbackground_df &lt;- background_sf %&gt;%\n  mutate(resp = 0) %&gt;%                         # Add response variable (0 = background)\n  st_coordinates() %&gt;%                         # Extract coordinates\n  as.data.frame() %&gt;%                          # Convert to data frame\n  bind_cols(resp = 0)                          # Append response column again for clarity\n\n# Combine coordinates of presence and background points\nsdm_xy &lt;- bind_rows(\n  st_coordinates(presence_sf) %&gt;% as.data.frame(),      # Presence coordinates\n  st_coordinates(background_sf) %&gt;% as.data.frame()     # Background coordinates\n)\n\n# Create corresponding response vector (1 = presence, 0 = background)\nsdm_resp &lt;- c(rep(1, nrow(presence_sf)), rep(0, nrow(background_sf)))\n\n# Rename coordinate columns to match biomod2 requirements\ncolnames(sdm_xy) &lt;- c(\"lon\", \"lat\")\n\n# Select focal environmental layers\nenv_select &lt;- Env_UK_stack[[c(\"bio_1\", \"bio_12\", \"elevation\")]]\n\n# Convert terra SpatRaster to RasterStack (required by biomod2)\nenv_ras &lt;- raster::stack(env_select)\n\n# Format data for biomod2 modelling\nbiomod_data &lt;- BIOMOD_FormatingData(\n  resp.var = sdm_resp,              # Vector of 1s and 0s\n  expl.var = env_ras,               # Environmental predictors as RasterStack\n  resp.xy = sdm_xy,                 # Coordinates of all points\n  resp.name = \"Rhinolophus_hipposideros\",  # Species name (used to label outputs)\n  PA.nb.rep = 0                     # No pseudo-absence generation needed (already supplied)\n)\n\n\n-=-=-=-=-=-=-=-=-=-= Rhinolophus_hipposideros Data Formating -=-=-=-=-=-=-=-=-=-=\n\n      ! Response variable name was converted into Rhinolophus.hipposideros\n !!! Some data are located in the same raster cell. \n          Please set `filter.raster = TRUE` if you want an automatic filtering.\n      ! No data has been set aside for modeling evaluation\n ! Some NAs have been automatically removed from your data\n-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-= Done -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n\n\n\n\nbiomod2 step 2: Define and fit individual models\n\n# Set default model options (can be customised per algorithm)\nmod_options &lt;- bm_ModelingOptions(data.type = 'binary',\n                            models = c(\"GLM\", \"RF\", \"GBM\", \"GAM\"),\n                            strategy = 'default')\n\n# Run SDMs using selected algorithms\n# Fit individual species distribution models using biomod2\nmyBiomodModelOut &lt;- BIOMOD_Modeling(\n  bm.format = biomod_data,               # Formatted input data\n  modeling.id = \"AllModels\",             # Unique ID for this modelling run\n  models = c(\"GLM\", \"RF\", \"GBM\", \"GAM\"),  # SDM algorithms to fit\n  \n  # Cross-validation settings\n  CV.strategy = \"random\",                # Randomly split data into train/test\n  CV.nb.rep = 2,                         # Number of cross-validation replicates\n  CV.perc = 0.7,                         # 70% data used for training, 30% for testing\n  \n  # Model optimization and evaluation\n  OPT.strategy = \"bigboss\",             # Optimisation strategy (biomod2 default)\n  metric.eval = c(\"TSS\", \"ROC\"),        # Evaluation metrics to use\n  var.import = 2,                       # Number of permutations for variable importance\n  seed.val = 42                         # Random seed for reproducibility\n)\n\n\n\nbiomod2 step 3: Build ensemble model\n\n# Inspect model evaluation metrics for each algorithm and CV replicate\nget_evaluations(myBiomodModelOut) \n\nThese evaluation results show that some models (e.g. RF) tend to have higher ROC/TSS etc., meaning better discrimination and more confidence; others may be lower, which suggests more caution or lower predictability under your environmental data.\n\n\nbiomod2 step 4: Ensemble prediction and uncertainty mapping\nHere we combine models, project them spatially, generate ensemble outputs (mean probability, weighted mean, and uncertainty), then forecast ensemble to new environmental rasters\n\n# Project individual models to environmental raster layers\nmodel_proj &lt;- BIOMOD_Projection(\n  bm.mod        = myBiomodModelOut,\n  new.env       = env_ras,           # raster stack of environmental predictors\n  proj.name     = \"current\",         # a label for this projection scenario\n  binary.meth   = \"TSS\",             # thresholding method for binary maps\n  compress      = FALSE\n)\n\n\n-=-=-=-=-=-=-=-=-=-=-=-=-= Do Single Models Projection -=-=-=-=-=-=-=-=-=-=-=-=-=\n\n    &gt; Building clamping mask\n\n    &gt; Projecting Rhinolophus.hipposideros_allData_RUN1_GLM ...\n    &gt; Projecting Rhinolophus.hipposideros_allData_RUN1_RF ...\n    &gt; Projecting Rhinolophus.hipposideros_allData_RUN1_GBM ...\n    &gt; Projecting Rhinolophus.hipposideros_allData_RUN2_GLM ...\n    &gt; Projecting Rhinolophus.hipposideros_allData_RUN2_RF ...\n    &gt; Projecting Rhinolophus.hipposideros_allData_RUN2_GBM ...\n    &gt; Projecting Rhinolophus.hipposideros_allData_allRun_GLM ...\n    &gt; Projecting Rhinolophus.hipposideros_allData_allRun_RF ...\n    &gt; Projecting Rhinolophus.hipposideros_allData_allRun_GBM ...\n-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-= Done -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n\n# Build the ensemble using all cross-validation replicates\nmyEnsembleOut &lt;- BIOMOD_EnsembleModeling(\n  bm.mod                = myBiomodModelOut,\n  em.by                 = \"all\",                     # combine all CV runs\n  em.algo               = c(\"EMmean\", \"EMcv\", \"EMwmean\"), # different ensemble methods: mean, coefficient of variation (CV=uncertainty), weighted mean\n  metric.select         = \"TSS\",                     # metric for selecting which models to include\n  metric.select.thresh  = 0.5                        # only include individual models with TSS ≥ 0.5\n)\n\n\n-=-=-=-=-=-=-=-=-=-=-=-=-=-= Build Ensemble Models -=-=-=-=-=-=-=-=-=-=-=-=-=-=\n\n   ! all models available will be included in ensemble.modeling\n  ! Ensemble Models will be filtered and/or weighted using validation dataset (if possible). Please use `metric.select.dataset` for alternative options.\n   &gt; Evaluation & Weighting methods summary :\n      TSS over 0.5\n\n!!! Removed models using the Full dataset as ensemble models cannot merge repetition dataset (RUN1, RUN2, ...) with Full dataset unless em.by = 'PA+run'.\n\n  &gt; mergedData_mergedRun_mergedAlgo ensemble modeling\n   &gt; Mean of probabilities by TSS ...\n   &gt; Coef of variation of probabilities by TSS ...\n          original models scores =  0.598 0.66 0.679 0.652 0.635 0.647\n          final models weights =  0.154 0.17 0.175 0.168 0.164 0.167\n   &gt; Probabilities weighting mean by TSS ...\n\n\nError in {: task 1 failed - \"task 1 failed - \"task 1 failed - \"cannot open the connection\"\"\"\n\n# Apply the ensemble to the projections\nmyEnsembleProj &lt;- BIOMOD_EnsembleForecasting(\n  bm.em   = myEnsembleOut,         # ensemble modeling object\n  bm.proj = model_proj             # projections from individual models\n)\n\n\n-=-=-=-=-=-=-=-=-=-=-=-= Do Ensemble Models Projection -=-=-=-=-=-=-=-=-=-=-=-=\n\n\nError in eval(expr, envir, enclos): object 'myEnsembleOut' not found\n\n# Visualise mean suitability, CV, and weigthed suitabilty onto map\n# Extract layers from biomod2 ensemble projection\nens_stack &lt;- get_predictions(myEnsembleProj)\n\nError in h(simpleError(msg, call)): error in evaluating the argument 'obj' in selecting a method for function 'get_predictions': object 'myEnsembleProj' not found\n\n# Rename for clarity (adjust names if needed)\nnames(ens_stack) &lt;- c(\"Suitability_mean\", \"CV\", \"Suitability_weighted\")\n\nError: object 'ens_stack' not found\n\n# Make 0-values in the weighted layer transparent\n# Set values of 0 to NA in the weighted layer\nens_stack$Suitability_weighted[ens_stack$Suitability_weighted == 0] &lt;- NA\n\nError: object 'ens_stack' not found\n\n# Plot all three side-by-side\nplot(ens_stack,\n     main = c(\"Mean Suitability\", \"Cross-Validation Variation\", \"Weighted Suitability\"),\n     colNA = \"transparent\",       # Show NA (0-values) as transparent\n     nc = 3,                      # Number of columns in layout\n     mar = c(2, 2, 2, 4))         # Margins\n\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'plot': object 'ens_stack' not found\n\n\n\n\n\n\n3.2 Using sdm\nThe sdm package in R is another powerful tool for species distribution modeling that supports many algorithms, cross-validation, and ensemble modeling. It is user-friendly and integrates well with spatial data.\n\nKey Features:\n\nSupports multiple algorithms (GLM, GAM, RF, Maxent, etc.)\nEasy setup of cross-validation and replication\nBuilt-in ensemble modeling and projection\nOutputs predictions and variable importance metrics\n\n\n\nsdm basic Workflow\nHere is an outline of how to run ensemble SDMs using sdm:\n\nsdm step 1: Format the input data\n\nlibrary(sdm)\nlibrary(terra)\nlibrary(sf)\nlibrary(dplyr)\n\n# Convert tibbles of pre-prepared sdm training and testing data subsets to base data.frames\nsdmData_df_train &lt;- as.data.frame(sdm_df_train)[, c(\"pa\", \"bio_1\", \"bio_12\", \"elevation\")]\nsdmData_df_test  &lt;- as.data.frame(sdm_df_test)[, c(\"pa\", \"bio_1\", \"bio_12\", \"elevation\")]\n\n# Make sure response variable is numeric\nsdmData_df_train$pa &lt;- as.numeric(as.character(sdmData_df_train$pa))\nsdmData_df_test$pa  &lt;- as.numeric(as.character(sdmData_df_test$pa))\n\n# Define formula using only selected predictors\nformula_selected &lt;- pa ~ bio_1 + bio_12 + elevation\n\n# Create sdmData object using the pre-prepared sdm training and testing data subsets\nsdmData_df &lt;- sdm::sdmData(\n  pa ~ bio_1+bio_12+elevation,\n  train = sdmData_df_train,\n  test = sdmData_df_test\n)\n\n\n\nsdm step 2: Fit models with cross-validation\n\n# Fit SDMs using multiple algorithms with replication (e.g., CV)\n# Available algorithms: 'glm', 'rf', 'brt', 'svm', 'mars', 'gam', etc.\nsdm_model &lt;- sdm::sdm(\n  formula = pa ~ bio_1 + bio_12 + elevation,\n  data = sdmData_df,\n  methods = c(\"glm\", \"gam\", \"rf\", \"brt\"),         # SDM algorithms\ntest.percent = 0   # use external test data supplied in sdmData_df\n  )\n\n\n\nsdm step 3: Evaluate model performance\n\n# View evaluation metrics: AUC, TSS, ...\neval_sdm &lt;- sdm::getEvaluation(sdm_model, stat = c(\"AUC\", \"TSS\", \"sensitivity\", \"specificity\"))\n\nprint(eval_sdm)\n\n  modelID   AUC    TSS sensitivity specificity\n1       1 0.817 0.5189      0.9114      0.6075\n2       2 0.871 0.6591      0.9114      0.7477\n3       3 0.896 0.6497      0.9114      0.7383\n4       4 0.860 0.6018      0.8354      0.7664\n\n\nAll four algorithms perform reasonably well at predicting species presence and absence, but there are differences in their strengths. The GLM (model 1) has very high sensitivity, meaning it rarely misses actual presences, but lower specificity, so it predicts some false presences. In contrast, the Random Forest (model 3) achieves the best overall balance (highest TSS) and good AUC, indicating it correctly predicts both presences and absences reliably. This illustrates why no single model is perfect, and why ensemble modelling — combining predictions from multiple algorithms — can improve robustness and account for different strengths and weaknesses of individual models.\n\n\nsdm step 4: Variable importance\n\n# Assess variable importance\nvarimp_sdm &lt;- getVarImp(sdm_model, id = 1:3)  # Adjust ID to match your models if needed\nplot(varimp_sdm)\n\n\n\n\n\n\n\n\n\n\nsdm step 5: Project model spatially\n\n# Select focal environmental layers\nenv_select &lt;- Env_UK_stack[[c(\"bio_1\", \"bio_12\", \"elevation\")]]\n\n# Project model onto environmental space\nsdm_proj &lt;- predict(sdm_model, newdata = env_select, type=\"ensemble\")\n\n# Plot predictions\nplot(sdm_proj)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteLiterature\n\n\n\n\n\nAraújo, M. B., & New, M. (2007). Ensemble forecasting of species distributions. Trends in Ecology and Evolution, 22(1), 42–47.\nDormann Carsten, F., Calabrese Justin, M., Guillera‐Arroita, G., Matechou, E., Bahn, V., Bartoń, K., . . . Hartig, F. (2018). Model averaging in ecology: a review of Bayesian, information‐theoretic and tactical approaches for predictive inference. Ecological Monographs, 88(4): 485–504. doi:10.1002/ecm.1309\nHao, T., Elith, J., Lahoz-Monfort, J. J., & Guillera-Arroita, G. (2020). Testing whether ensemble modelling is advantageous for maximising predictive performance of species distribution models. Ecography, 43(4), 549–558. doi:10.1111/ecog.04890\nNaimi, B., & Araújo, M. B. (2016). sdm: a reproducible and extensible R platform for species distribution modelling. Ecography, 39(4), 368–375. doi:10.1111/ecog.01881\nThuiller, W., Lafourcade, B., Engler, R., & Araújo, M. B. (2009). BIOMOD - A platform for ensemble forecasting of species distributions. Ecography, 32(3), 369–373. doi/10.1111/j.1600-0587.2008.05742.x\nZurell, D., Zimmermann, N. E., Gross, H., Baltensweiler, A., Sattler, T., & Wüest, R. O. (2020). Testing species assemblage predictions from stacked and joint species distribution models. Journal of Biogeography, 47(1), 101–113. doi:10.1111/jbi.13608"
  },
  {
    "objectID": "page_SDM.7_SDMAssessmentProjection.html",
    "href": "page_SDM.7_SDMAssessmentProjection.html",
    "title": "Assessment & Projection",
    "section": "",
    "text": "After a species distribution model (SDM) has been fitted, the next critical steps are to assess its performance and project it across space or time. Model assessment ensures that the relationships identified between species occurrences and environmental predictors are robust, interpretable, and generalisable. Projection then allows us to translate these relationships into maps of habitat suitability, either under current conditions or for future scenarios such as climate change.\nModel assessment involves several interconnected steps:\nModel fit: Examining how well the model explains the training data.\nModel validation: Testing model predictions on independent data (data not used for fitting) to avoid overfitting and assess generalization.\nModel evaluation: Calculating quantitative performance metrics on the validation data to objectively judge model quality.\n\nA common and recommended approach is to split the available data into training and testing sets or use cross-validation methods. This ensures that evaluation statistics such as AUC or TSS reflect the model’s predictive ability rather than just its capacity to reproduce the training data.\nEvaluation metrics fall into two main types:\nThreshold-independent metrics like AUC and Continuous Boyce Index evaluate the model’s discrimination ability without requiring a binary classification.\nThreshold-dependent metrics** like sensitivity, specificity, and True Skill Statistic (TSS) require converting continuous predictions into binary outcomes and assess classification accuracy at selected thresholds.\n\nTogether, these evaluation metrics, when applied on validation data, provide a comprehensive picture of model performance.\nProjection extends model application by using fitted relationships to predict species distributions across geographic space, or under different environmental conditions. This is particularly powerful for biodiversity monitoring, invasive species risk mapping, and forecasting impacts of global change. However, projections must always be interpreted in light of uncertainties stemming from data quality, model choice, and environmental scenarios.\n\n\n\n\n\n\nNoteEssential SDM Workflow: Fitting, Validating, Evaluation and Projection\n\n\n\n\n\nWhen building species distribution models (SDMs), it’s essential to distinguish between model fit, model validation, model evaluation, and final projection. These are distinct but interconnected steps in a robust modelling workflow.\n\n\nModel fit\nRefers to how well a model explains the data it was trained on. This can be assessed using residuals, likelihood measures, or internal performance statistics. However, good fit to training data does not guarantee predictive accuracy — models can overfit, especially with small sample sizes or many predictors.\n\n\n\nModel validation\nEvaluates how well the model generalizes to new data. This requires testing predictions on data not used for model fitting, typically via: - Data splitting (e.g., 70% training, 30% testing), or\n- Cross-validation (e.g., k-fold, spatial block).\nValidation ensures the model’s performance is not just memorizing patterns in the training data, but reflects true predictive power.\n\n\n\nModel evaluation\nInvolves calculating quantitative performance metrics (e.g., AUC, TSS, Boyce Index) on the validation (test) data. This step quantifies how well the model distinguishes presences from absences, both: - Threshold-independently (e.g., AUC, Boyce), and\n- Threshold-dependently (e.g., sensitivity, specificity, TSS).\nEvaluation provides objective evidence of model reliability.\n\n\n\nFinal projection\nOnce the model has been validated and evaluated: - You can refit the final model using all available data (training + test) to maximize information. - Use this full model to project species distributions across space (e.g., habitat suitability maps) or time (e.g., future climate scenarios).\nWhy refit with all data?\nTo improve accuracy of projections, especially for rare species or sparse datasets. But remember: this projection step should only happen after validation is complete.\n\nBest practice workflow summary:\n\nSplit data → for training and validation.\n\nFit model on training data.\n\nValidate & evaluate on test data.\n\nRefit model on all data.\n\nProject using the refitted model.\n\nThis approach supports both scientific rigor and reliable application of SDMs in conservation, forecasting, and spatial planning.\n\n\n\n\n\n\n1. Model evaluation\nModel evaluation is a key step in SDM analysis. Since SDMs typically predict a continuous probability of occurrence or relative suitability, while field data are often binary (presence vs. absence/pseudo-absence), we need appropriate metrics to judge model quality.\nA critical principle in SDM evaluation is to assess model predictions on independent data not used during model fitting. This can be achieved by:\nData splitting: dividing the dataset into training and testing subsets, fitting the model only on training data, then evaluating on testing data.\nCross-validation: repeatedly splitting the data into folds, training on some folds and testing on the others, to obtain robust performance estimates. Evaluating models on independent data helps to avoid overly optimistic assessments caused by overfitting and better reflects how the model will perform in real-world applications.\nAfter a model is fitted, and data have been partitioned appropriately, evaluation metrics are calculated on the validation (test) data to assess predictive performance based on either\nThreshold-independent or Threshold-dependent metrics.\nFollowing best-practice, we now split our data into training and testing subsets and refit the GLM model using only the training data before performing any validation.\n\nlibrary(caret)\n\nset.seed(123)\n# Generate an index that assign 70% of data to training data\ntrainIndex &lt;- createDataPartition(sdm_df_clean$pa, p = .7, list = FALSE)\nsdm_df_train &lt;- sdm_df_clean[trainIndex, ]\nsdm_df_test &lt;- sdm_df_clean[-trainIndex, ]\n\n# Fit model on training data\nglm_multi_train &lt;- glm(pa ~ poly(elevation, 2) + poly(bio_1, 2) + poly(bio_12, 2),\n                 data = sdm_df_train,\n                 family = binomial)\n\nglm_step_train &lt;- step(glm_multi_train , direction = \"both\", trace = FALSE)\n\n\n1.1 Threshold-independent evaluation\nThe most widely used threshold-independent measure is the area under the ROC curve (AUC). AUC evaluates how well the model distinguishes presences from absences across all possible thresholds. An AUC of 0.5 indicates random performance, while values above 0.7 are typically considered fair, and above 0.9 excellent (Araujo et al. 2005).\n\nlibrary(pROC)\n\n# Predict on test data\nglm_predict_test &lt;- predict(glm_step_train , newdata = sdm_df_test, type = \"response\")\n\n# 'Observed' test data (presence/pseudoabsence)\nobs_test &lt;- sdm_df_test$pa\n\n# Evaluate with ROC and AUC on test data\nroc_glm &lt;- pROC::roc(obs_test, glm_predict_test)\nauc_glm &lt;- pROC::auc(roc_glm)\n\n# Print AUC\nprint(auc_glm)\n\nArea under the curve: 0.8567\n\n# Plot ROC curve\nplot(roc_glm, col = \"#e31a1c\", lwd = 2, main = \"ROC Curve\")\nabline(a = 0, b = 1, lty = 2, col = \"grey\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteHow to interpret the ROC curve\n\n\n\n\n\n\nThe ROC curve plots sensitivity (true positive rate) against 1 – specificity (false positive rate) across all thresholds.\n\nA model with no predictive ability follows the diagonal line (AUC ≈ 0.5).\n\nCurves that bow towards the upper-left corner indicate stronger discrimination between presences and absences.\n\nThe area under the curve (AUC) provides a summary metric:\n\nAUC = 0.5 → random predictions\n\n0.7 ≤ AUC &lt; 0.8 → fair\n\n0.8 ≤ AUC &lt; 0.9 → good\n\nAUC ≥ 0.9 → excellent\n\n\nAlways interpret AUC together with ecological reasoning and other evaluation measures\n(e.g., TSS, Boyce index), since AUC alone can be misleading if prevalence is unbalanced\nor sampling design introduces bias.\n\n\n\n\nThe Continuous Boyce Index (CBI) is an evaluation metric used for species distribution models (SDMs) and habitat suitability models that are built using presence-only data. The CBI is particularly useful for SDMs, as it directly evaluates how predictions for observed presences deviate from a random distribution of available habitat suitability values. It ranges from -1 (counter-predictive), to 0 (no better than random), to +1 (perfect agreement between model predictions and presence distribution).\n\nlibrary(ecospat)\n\n# Compute Continuous Boyce Index (CBI)\nboyce_res &lt;- ecospat.boyce(\n  fit = glm_predict_test,            # model predictions\n  obs = glm_predict_test[obs_test == 1],  # predictions at presence points\n  nclass = 0,                  # automatic binning\n  window.w = \"default\"         # default moving window\n)\n\n\n\n\n\n\n\nboyce_res$Spearman.cor\n\nNULL\n\n\n\n\n\n\n\n\nNoteHow to interprete the Boyce curve\n\n\n\n\n\n\nX-axis (suitability): predicted suitability scores from the SDM.\n\nY-axis (predicted/expected ratio): how much more (or less) frequently presences are found in each suitability class than expected at random.\n\nGuidelines:\n- A flat line near 1 → model performs no better than random.\n- An increasing curve, with ratios &gt;1 in high suitability bins → good model performance.\n- Ratios &lt;1 in high suitability bins → the model may be misleading.\nWhile P/E ratios in the plot may exceed 1, the Boyce Index correlation ranges from -1 to +1..\n\n\n\n\n\n\n1.2 Threshold-dependent evaluation\nTo compute threshold-dependent metrics, we need to choose a threshold that converts continuous probabilities into binary predictions. Several rules exist (see Liu et al., 2005), such as:\nMaximizing sensitivity + specificity\nEqual sensitivity and specificity\nFixed probability cutoffs (e.g., 0.5)\nHere we use the PresenceAbsence package to determine an optimal threshold and then compute metrics like sensitivity, specificity, and the True Skill Statistic (TSS).\n\nlibrary(PresenceAbsence)\n\n# Construct PresenceAbsence dataframe\n# Ensure observed and predicted values are numeric\npa_df_test &lt;- data.frame(\n  ID = seq_along(obs_test),\n  observed = as.numeric(as.character(obs_test)),\n  predicted = as.numeric(glm_predict_test)\n  )\n\n# Remove any rows with missing values\npa_df_test &lt;- pa_df_test[complete.cases(pa_df_test), ]\n\n# Find threshold\n(opt_thresh &lt;- PresenceAbsence::optimal.thresholds(\n                DATA= pa_df_test,\n                opt.methods = c('MaxSens+Spec', 'Sens=Spec', 'MinROCdist', 'MaxKappa')))\n\n        Method predicted\n1 MaxSens+Spec      0.47\n2    Sens=Spec      0.51\n3   MinROCdist      0.50\n4     MaxKappa      0.47\n\n# Use one threshold (e.g., MaxSens+Spec)\n    chosen_thresh &lt;- opt_thresh$predicted[opt_thresh$Method == 'MaxSens+Spec']  \n\n# Classify predictions\npred_binary &lt;- ifelse(as.numeric(glm_predict_test) &gt;= chosen_thresh, 1, 0)    \n\n# Confusion matrix\ncm &lt;- table(Observed = obs_test, Predicted = pred_binary)\nprint(cm)\n\n        Predicted\nObserved  0  1\n       0 77 25\n       1 13 82\n\n# Compute sensitivity, specificity, TSS\nsensitivity &lt;- sum(pred_binary == 1 & obs_test == 1) / sum(obs_test == 1)\nspecificity &lt;- sum(pred_binary == 0 & obs_test == 0) / sum(obs_test == 0)\ntss &lt;- sensitivity + specificity - 1\n\nlist(ConfusionMatrix = cm,\n      Sensitivity = sensitivity,\n      Specificity = specificity,\n      TSS = tss)   \n\n$ConfusionMatrix\n        Predicted\nObserved  0  1\n       0 77 25\n       1 13 82\n\n$Sensitivity\n[1] 0.8631579\n\n$Specificity\n[1] 0.754902\n\n$TSS\n[1] 0.6180599\n\n\n\n\n\n\n\n\nNoteChoosing thresholds for binary predictions\n\n\n\n\n\nSpecies distribution models typically produce continuous predictions (relative suitability or probability of occurrence).\nTo evaluate performance or produce presence/absence maps, these need to be converted into binary predictions.\nThere is no single “best” threshold. Instead, different methods emphasize different trade-offs.\nHere are four widely used options:\n\nMaxSens+Spec\nMaximizes the sum of sensitivity (true positive rate) and specificity (true negative rate).\n→ Balanced choice when false positives and false negatives are equally costly.\nSens=Spec\nSelects the threshold where sensitivity and specificity are equal.\n→ Useful when both error types are equally undesirable.\nMinROCdist\nMinimizes the distance between the ROC curve and the perfect classifier point (0,1).\n→ Robust option, often similar to MaxSens+Spec but less sensitive to skewed prevalence.\nMaxKappa\nMaximizes Cohen’s Kappa statistic, which accounts for agreement expected by chance.\n→ Historically popular in ecology, still informative for comparing classifiers.\n\nIt is good practice to report multiple thresholds.\nThis ensures transparency, allows comparison across studies, and lets end-users (e.g., conservation managers) choose the threshold best suited to their objectives (e.g., maximizing detection vs. minimizing false alarms).\n\n\n\nInterpretation\nAUC: reflects overall discriminatory ability, independent of any threshold.\nThreshold-dependent metrics: provide information about model performance in classifying presences vs. absences at a chosen threshold.\nTSS: balances sensitivity and specificity, commonly used in SDM studies.\nIn practice, both perspectives are valuable. Threshold-independent measures provide a broad sense of predictive skill, while threshold-dependent measures allow for more concrete classification performance.\n\n\n\n2. Projection across space and time\nOnce a species distribution model has been calibrated and evaluated, the next step is projection: predicting habitat suitability across geographic space or future environmental scenarios. Projections enable us to identify potential ranges, assess climate change impacts, and inform conservation planning.\nThere are two main projection contexts:\nSpatial projection: applying the model to current environmental layers across the study area to produce a habitat suitability map.\nTemporal projection: applying the model to future climate scenarios (e.g., CMIP6 SSPs) to forecast potential range shifts.\n::: {.callout-tip collapse=“true” icon=“globe”}\n### Important considerations when projecting SDMs\nEnsure predictor variables in the projection layers match the model in units, resolution, and extent.\nBeware of extrapolation: predictions in environmental space outside the range of training data are uncertain.\nFor presence–pseudoabsence models, predictions remain relative suitability, not absolute probabilities.\nWhen projecting to future climates, using ensembles of GCMs helps capture uncertainty. :::\n\n\n2.1 Projecting the fitted GLM across current environmental layers\n\nlibrary(terra)\n\n# Prepare environmental stack for projection\nproj_stack &lt;- Env_UK_stack[[c(\"bio_1\", \"bio_12\", \"elevation\")]]\nnames(proj_stack)  # needs to match predictors in glm_step\n\n[1] \"bio_1\"     \"bio_12\"    \"elevation\"\n\n# Predict relative suitability across the raster stack\nglm_suitability_map &lt;- terra::predict(proj_stack, glm_step, type = \"response\")\n\n# Plot the predicted suitability\nplot(glm_suitability_map,\n     main = \"Predicted habitat suitability for R. hipposideros (current)\",\n     col = viridis(50, option = \"D\"))\n\n\n\n\n\n\n\n\nTo support conservation decisions or map projected ranges, we often need binary predictions — areas where the species is likely present (1) or absent (0) — instead of continuous suitability values. We’ll now apply a threshold to the predicted suitability map and visualize this binary prediction.\nWe’ll use the threshold identified earlier (e.g., “MaxSens+Spec”) to reclassify the continuous map.\n\n# Create binary prediction map: 1 = presence, 0 = absence\nbinary_map &lt;- glm_suitability_map &gt;= chosen_thresh\n\n# Plot the binary prediction map\nplot(binary_map,\n     main = \"Predicted presence/absence of *Rhinolophus hipposideros* (current)\",\n     col = c(\"lightgrey\", \"darkgreen\"),\n     legend = FALSE)\nlegend(\"bottomleft\", legend = c(\"Absence\", \"Presence\"),\n       fill = c(\"lightgrey\", \"darkgreen\"), bty = \"n\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteWhy binary maps?\n\n\n\n\n\nBinary SDM outputs are often used in applied contexts — for example:\nEstimating species’ range size\nIdentifying priority conservation areas\nCommunicating results to decision-makers\nBut: Always interpret binary maps with caution. Thresholding reduces a continuous gradient to a yes/no decision, which can oversimplify ecological reality. That’s why it is recommend:\nReporting both continuous and binary maps\nExplaining how the threshold was chosen\nBeing transparent about limitations\n\n\n\n\n\n\n\n\n\nTipView interactive map of projected habitat suitability, presence/absence, and occurrence records\n\n\n\n\n\nWe can also explore an interactive version of the current habitat suitability map and binary map overlaid with occurrence records using Leaflet.\n\n\n\n\n\n\n\n\n\n\n\n2.2 Projecting under future climate scenarios\n\nlibrary(terra)\n\n# First, generate a raster stack for future climate and elevation data\n# Resample Elev_UK raster to match Clim_cmip6_2041_2060_UK raster\nElev_UK_aligned_future &lt;- terra::resample(Elev_UK, Clim_cmip6_2041_2060_UK, method = \"bilinear\")\n# Crop to common extent\ncommon_extent_future &lt;- terra::intersect(terra::ext(Clim_cmip6_2041_2060_UK), terra::ext(Elev_UK_aligned_future))\nClim_UK_future_crop2 &lt;- terra::crop(Clim_cmip6_2041_2060_UK, common_extent_future)\nElev_UK_future_crop2 &lt;- terra::crop(Elev_UK_aligned_future, common_extent_future)\n\n# Stack layers and name variables\nEnv_UK_stack_future &lt;- c(Clim_UK_future_crop2 , Elev_UK_future_crop2)\nnames(Env_UK_stack_future) &lt;- c(sub(\"^wc2\\\\.1_10m_\", \"\", names(Clim_UK_crop2)), \"elevation\")\n\n# Example: future CMIP6 scenario (SSP5-8.5, 2041-2060)\nfuture_stack &lt;-  Env_UK_stack_future[[c(\"bio_1\", \"bio_12\", \"elevation\")]]\nnames(future_stack) &lt;- c(\"bio_1\", \"bio_12\", \"elevation\")\n\n# Predict future relative suitability\nfuture_suitability &lt;- terra::predict(future_stack, glm_step, type = \"response\")\n\n# Plot future suitability\nplot(future_suitability,\n     main = \"Projected habitat suitability under CMIP-6 (2041-2060)\",\n     col = viridis(50, option = \"E\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteLiterature\n\n\n\n\n\nAraujo, M. B., Pearson, R. G., Thuiller, W., & Erhard, M. (2005). Validation of species-climate impact models under climate change. Global Change Biology, 11(9), 1504–1513. doi:10.1111/j.1365-2486.2005.001000.x\nHirzel, A. H., Le Lay, G., Helfer, V., Randin, C., & Guisan, A. (2006). Evaluating the ability of habitat suitability models to predict species presences. Ecological Modelling, 199(2), 142–152. doi:10.1016/j.ecolmodel.2006.05.017\nLiu, C., Berry, P. M., Dawson, T. P., & Pearson, R. G. (2005). Selecting thresholds of occurrence in the prediction of species distributions. Ecography, 28(3), 385–393. doi: doi:10.1111/j.0906-7590.2005.03957.x\nSmith, A. B., & Santos, M. J. (2020). Testing the ability of species distribution models to infer variable importance. Ecography, 43(12), 1801–1813. [doi:10.1111/ecog.05317] (https://doi.org/10.1111/ecog.05317)\nWilson, K. A., Westphal, M. I., Possingham, H. P., & Elith, J. (2005). Sensitivity of conservation planning to different approaches to using predicted species distribution data. Biological Conservation, 122(1), 99–112.\nZurell, D., Franklin, J., König, C., Bouchet, P.J., Dormann, C.F., Elith, J., Fandos, G., Feng, X., Guillera-Arroita, G., Guisan, A., Lahoz-Monfort, J.J., Leitão, P.J., Park, D.S., Peterson, A.T., Rapacciuolo, G., Schmatz, D.R., Schröder, B., Serra-Diaz, J.M., Thuiller, W., Yates, Katherine L., Zimmermann, Niklaus E. & Merow, C. (2020). A standard protocol for reporting species distribution models. Ecography, 43(9), 1261–1277. doi:10.1111/ecog.04960"
  },
  {
    "objectID": "page_SDM.5_PseudoAbsenceData.html",
    "href": "page_SDM.5_PseudoAbsenceData.html",
    "title": "Background & pseudo-absence data",
    "section": "",
    "text": "Species distribution modelling (SDM) relies on comparing where a species occurs to where it does not, yet in practice true absence data are extremely difficult to obtain. In most biodiversity databases such GBIF, the records available are presence-only. Demonstrating a true absence requires very intensive sampling: for plants, complete and repeated inventories across large areas; for birds and other mobile or cryptic animals, repeated visits of the sampling plot. Even with rigorous protocols, imperfect detection means that non-detections can represent false negatives rather than genuine absences. Because systematic presence–absence surveys are rare, ecologists frequently turn to background or pseudo-absence data to make presence-only datasets usable in SDM. These data do not attempt to identify where a species is truly absent, but instead provide a contrast set of conditions against which the environments of presences can be evaluated. In this sense, background data characterise the environmental or geographic domain of the study, while presences identify where within that domain the species has been observed.\nOver the past decade, numerous approaches have been developed for generating background or pseudo-absence data. Early methods focused on simple random sampling of points across a study region, or on excluding presence cells. More recent advances emphasise accounting for spatial sampling bias (e.g., target-group selection, bias weighting) and ecological representativeness (e.g., sampling in environmental space). These innovations recognise that poorly chosen background data can bias response curves, inflate model accuracy, and limit transferability. State-of-the-art methods now include hybrid strategies that combine geographic, environmental, and effort-based information, as well as point process frameworks that embed background generation into a statistically principled likelihood. Despite this progress, there is still no universal solution: the most appropriate strategy depends on study goals, data sources, and the ecological processes under investigation.\nWe will look at two major ways of creating background/pseudo-absence data: • random selection of points within study area • accounting for spatial sampling bias using target-group selection"
  },
  {
    "objectID": "page_SDM.5_PseudoAbsenceData.html#random-selection-of-points-within-study-area",
    "href": "page_SDM.5_PseudoAbsenceData.html#random-selection-of-points-within-study-area",
    "title": "Background & pseudo-absence data",
    "section": "1. Random selection of points within study area",
    "text": "1. Random selection of points within study area\nBelow, we demonstrate how to generate random background points within the spatial extent of the UK using terra and sf. We will then visualise the presence points for Rhinolophus hipposideros together with these background points.\nDefine study area and presence points\n\n# UK boundary (sf object)\nstudy_area &lt;- UK_sf\n\n# Cleaned occurrence points for Rhinolophus hipposideros (convert to sf)\n# Requires to generate occ_rhinhipp_cleaned in page \"Species data\npresence_sf &lt;- occ_rhinhipp_cleaned %&gt;%\n  # Select only the columns key, decimalLatitude, decimalLongitude\n  dplyr::select(species, key, decimalLatitude, decimalLongitude) %&gt;%\n  # Remove records with missing coordinates\n  filter(!is.na(decimalLongitude), !is.na(decimalLatitude)) %&gt;%\n  # convert to sf object\n  sf::st_as_sf(coords = c(\"decimalLongitude\", \"decimalLatitude\"), crs = 4326)\n\n# Check the number of presence points\nn_presence &lt;- nrow(presence_sf)\nn_presence\n\n[1] 355\n\n\n\n1.1 Generate random background points\n\nset.seed(2222)  # for reproducibility\n\n# Number of background points to generate (commonly 1-10x presence points)\nn_background &lt;- n_presence\n\n# Generate random points within UK polygon using sf::st_sample\nbackground_points &lt;- sf::st_sample(study_area, size = n_background, type = \"random\")\n\n# Convert to sf object with POINT geometry\nbackground_sf &lt;- st_sf(geometry = background_points)\n\n# Assign an ID for plotting/analysis\nbackground_sf &lt;- background_sf %&gt;%\n  mutate(type = \"background\")\n\npresence_sf &lt;- presence_sf %&gt;%\n  mutate(type = \"presence\")\n\nVisualize presence and background points on the UK map\n\nggplot() +\n  geom_sf(data = study_area, fill = \"gray95\", color = \"black\") +\n  geom_sf(data = background_sf, aes(color = type, shape = type), alpha = 0.3, size = 1) +\n  geom_sf(data = presence_sf, aes(color = type, shape = type), size = 1.5) +\n  scale_color_manual(\n    name = \"Point type\",\n    values = c(\"background\" = \"#1f78b4\", \"presence\" = \"#6a3d9a\")\n  ) +\n  scale_shape_manual(\n    name = \"Point type\",\n    values = c(\"background\" = 16, \"presence\" = 17)\n  ) +\n  labs(\n    title = \"Presence and random background points\",\n    subtitle = \"Background points randomly sampled within UK boundary\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11),\n    legend.position = \"right\"  # show legend on the right\n  )\n\n\n\n\n\n\n\n\n\n\n1.2 Spatially constrained background points\nRandomly selecting background points across the entire study area can lead to biased models, especially when occurrence data are not evenly distributed. Presence records often cluster in areas with higher accessibility or sampling effort, which may not represent the species’ actual habitat preferences. By spatially constraining background points to be near presence records, we ensure that the background data reflect the same spatial biases as the presence data, leading to more accurate and ecologically meaningful models. This method has been highlighted as a robust approach to mitigate spatial sampling bias in SDMs. For instance, Barve et al. (2011) discussed how spatially constrained background sampling can improve model performance by aligning the background data distribution with that of the presence data.\n\nlibrary(sf)\nlibrary(ggplot2)\n\n# Define buffer distance (to be used around presence points)\nbuffer_dist = 5000\n\n# Define a function to generate background points near presence points\n# Define a function to generate background points near presence points\ngenerate_background_constr &lt;- function(presence_sf, study_area, buffer_distance, n_points) {\n  # Create a buffer around each presence point\n  presence_buffer &lt;- sf::st_buffer(presence_sf, dist = buffer_distance)\n  \n  # Combine all buffers into a single geometry\n  combined_buffer &lt;- sf::st_union(presence_buffer)\n  combined_buffer &lt;- sf::st_make_valid(combined_buffer)\n  \n  # Intersect buffer with study area to constrain background to that zone\n  constrained_area &lt;- sf::st_intersection(combined_buffer, study_area)\n  \n  # Generate random points within the constrained area\n  background_points &lt;- sf::st_sample(constrained_area, size = n_points, type = \"random\")\n  \n  # Convert to sf object with same CRS\n  background_sf &lt;- sf::st_sf(geometry = background_points, crs = sf::st_crs(presence_sf))\n  \n  return(background_sf)\n}\n\n# Generate spatially constrained background points\nbackground_constr_sf &lt;- generate_background_constr(presence_sf, study_area, buffer_dist, n_background)\n\n# Visualize the presence and spatially contrained background points\nggplot() +\n  geom_sf(data = study_area, fill = \"gray95\", color = \"black\") +\n  geom_sf(data = presence_sf, aes(color = \"Presence\"), size = 1.5, shape = 17) +\n  geom_sf(data = background_constr_sf, aes(color = \"Background\"), alpha = 0.3, size = 1) +\n  scale_color_manual(values = c(\"Presence\" = \"#6a3d9a\", \"Background\" = \"#1f78b4\")) +\n  labs(\n    title = \"Presence and Spatially Constrained Background Points for Rhinolophus hipposideros\",\n    subtitle = \"Background points generated within 5 km of presence records\",\n    color = \"Point Type\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11)\n  )"
  },
  {
    "objectID": "page_SDM.5_PseudoAbsenceData.html#accounting-for-spatial-sampling-bias-using-target-group-selection",
    "href": "page_SDM.5_PseudoAbsenceData.html#accounting-for-spatial-sampling-bias-using-target-group-selection",
    "title": "Background & pseudo-absence data",
    "section": "2. Accounting for spatial sampling bias using target-group selection",
    "text": "2. Accounting for spatial sampling bias using target-group selection\nThe spatial distribution of species observations is often driven more by where observers go than by where the species truly occurs. This leads to spatial sampling bias, which — if uncorrected — can distort species distribution models (SDMs). One effective strategy to mitigate this is target-group background selection. This method uses presence records of other species from the same group (e.g., all bats) that were collected using the same methods and sources (e.g., GBIF, citizen science) to approximate the same sampling bias.\nThe idea is simple: assume that the observer effort captured in the presence data of the target group reflects the same biases as for your focal species. Using this “effort surface” as background allows models to distinguish environmental preferences of the species from spatial artefacts of data collection.\nWe now generate background points for Rhinolophus hipposideros using other GBIF records for UK bats as the target group.\n\nDefine target group species\nWe define the target group as all bat species recorded in Great Britain. You can adapt this list depending on your study needs.\n\n\n\n\n\n\nNoteCode to generate the target group species background points (already done in data preparation)\n\n\n\n\n\nThese steps are included in the data preparation file:\n👉 View full data preparation script\n\n\n\n\n\n\nVisualise target group background points\n\n# Check how many background records we have\nnrow(target_group_sf)\n\n[1] 9715\n\n# Plot target-group background vs presence points\nggplot() +\n  geom_sf(data = study_area, fill = \"gray95\", color = \"black\") +\n  geom_sf(data = presence_sf, aes(color = \"Presence\"), size = 2, shape = 17) +\n  geom_sf(data = target_group_sf, aes(color = \"Target group background\"), size = 1, alpha = 0.6) +\n  scale_color_manual(values = c(\"Presence\" = \"#6a3d9a\", \"Target group background\" = \"#33a02c\")) +\n  labs(\n    title = \"Presence and Target Group Background Points\",\n    subtitle = \"Target-group: Other bat species recorded in the UK (GBIF)\",\n    color = \"Point Type\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteWhen to use target-group background?\n\n\n\n\n\nTarget-group background is especially useful when: - Presence data are from opportunistic, bias-prone sources like GBIF or citizen science. - The focal species is part of a well-studied taxonomic group with sufficient records. - You want to reduce the impact of spatial sampling effort on model results.\nIt works best when the target group was sampled using the same methods, in the same places, and during the same time as the focal species. Poorly matched target groups can worsen bias, so always inspect their spatial distribution.\n\n\n\n\n\n\n\n\n\nImportantThe choice to make…\n\n\n\n\n\nThere is no single “best” pseudo-absence or background approach that universally fits all objectives and applications. Good practice is to (1) understand your data and sampling process, (2) choose background/pseudo-absence strategies that emulate sampling or maximally inform the species-environment contrast relevant to your question, and (3) evaluate sensitivity across plausible alternatives while using spatially aware evaluation. The recent literature strongly favours bias-aware (target-group / weighted) and environmental-space approaches, and pushes toward methods that explicitly model sampling intensity or treat presences as a point process for principled inference."
  },
  {
    "objectID": "page_SDM.5_PseudoAbsenceData.html#join-data-species-presence-backgroundpseudo-absence-environmental-data",
    "href": "page_SDM.5_PseudoAbsenceData.html#join-data-species-presence-backgroundpseudo-absence-environmental-data",
    "title": "Background & pseudo-absence data",
    "section": "3. Join data (species presence, background/pseudo-absence, environmental data)",
    "text": "3. Join data (species presence, background/pseudo-absence, environmental data)\nTo model species-environment relationships, we need to combine the species data (presence and background/pseudo-absence points) with the environmental data (climate, elevation, etc.). This involves extracting raster values at point locations and joining these with species labels (1 for presence, 0 for background/pseudo-absence).\nWe’ll extract the environmental predictors from the following rasters: - Clim_UK: Bioclimatic variables from WorldClim - Elev_UK: Elevation at 30 arc-second resolution\nThese predictors will be joined to: - presence_sf: Points where Rhinolophus hipposideros was observed - target_group_sf: Target-group background points from other bat species\n\nlibrary(terra)\nlibrary(tidyverse)\nlibrary(sf)\n\n# Use the stacked climate and elevation data generated in the page \"Envionmental Data\"\nenv_stack &lt;- Env_UK_stack\n\n# Extract environmental values at presence points\nenv_pres &lt;- terra::extract(env_stack, vect(presence_sf)) %&gt;%\n  dplyr::select(-ID)  # Remove ID column returned by extract\n\n# Combine with presence points\npresence_data &lt;- presence_sf %&gt;%\n  mutate(pa = 1) %&gt;%  # 1 = presence\n  bind_cols(env_pres)\n\n# Extract environmental values at background points\nenv_back &lt;- terra::extract(env_stack, vect(background_sf)) %&gt;%\n  dplyr::select(-ID)\n\n# Combine with background points\nbackground_data &lt;- background_sf %&gt;%\n  mutate(pa = 0) %&gt;%  # 0 = pseudo-absence\n  bind_cols(env_back)\n\n# Combine presence + background into one dataset for modelling\nsdm_data &lt;- bind_rows(presence_data, background_data)\n\n# Drop geometry for model matrix if needed\nsdm_df &lt;- st_drop_geometry(sdm_data)\n\n# Drop rows wit NA in covariates\nsdm_df_clean &lt;- sdm_df %&gt;%\n  drop_na(starts_with(\"bio_\"), elevation)\n\n# Preview\nhead(sdm_df_clean)\n\n# A tibble: 6 × 24\n  species      key   type     pa bio_1 bio_2 bio_3 bio_4 bio_5 bio_6 bio_7 bio_8\n  &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Rhinolophus… 5105… pres…     1  8.85  7.55  38.3  442.  20.1 0.382  19.7  4.78\n2 Rhinolophus… 5105… pres…     1  8.77  6.22  34.4  441.  19.0 0.853  18.1  4.84\n3 Rhinolophus… 5105… pres…     1  8.65  6.41  35.3  436.  18.8 0.665  18.1  4.76\n4 Rhinolophus… 5069… pres…     1  9.84  6.29  37.4  393.  19.1 2.33   16.8  8.31\n5 Rhinolophus… 5105… pres…     1  9.09  6.56  35.4  441.  19.5 1.02   18.5  5.13\n6 Rhinolophus… 5152… pres…     1  8.57  6.20  34.7  433.  18.6 0.727  17.9  4.74\n# ℹ 12 more variables: bio_9 &lt;dbl&gt;, bio_10 &lt;dbl&gt;, bio_11 &lt;dbl&gt;, bio_12 &lt;dbl&gt;,\n#   bio_13 &lt;dbl&gt;, bio_14 &lt;dbl&gt;, bio_15 &lt;dbl&gt;, bio_16 &lt;dbl&gt;, bio_17 &lt;dbl&gt;,\n#   bio_18 &lt;dbl&gt;, bio_19 &lt;dbl&gt;, elevation &lt;dbl&gt;\n\n\n\n\n\n\n\n\nNoteExercises: Explore background and pseudo-absence strategies\n\n\n\n\n\n\nTry a different background method:\nReplace the random background points with a spatially constrained version using buffers of 1 km, 5 km, and 10 km. Plot and compare the results. How does the choice of buffer size affect spatial coverage?\nTarget-group background for another species:\nSelect a different focal species (e.g., Pipistrellus pipistrellus) and define an appropriate target group (e.g., other bat species). Generate target-group background points and compare their spatial distribution with your presence data.\nCompare environmental distributions:\nExtract environmental values (e.g., climate, elevation) at presence and background points. Plot histograms or density plots to compare environmental coverage across different background sampling strategies.\n\n\n\n\n\n\n\n\n\n\nNoteReadings\n\n\n\n\n\nBaker, D. J., Maclean, I. M. D., & Gaston, K. J. (2024). Effective strategies for correcting spatial sampling bias in species distribution models without independent test data. Diversity and Distributions, 30(3), e13802. doi:10.1111/ddi.13802\nBarber, R. A., Ball, S. G., Morris, R. K. A., & Gilbert, F. (2022). Target-group backgrounds prove effective at correcting sampling bias in Maxent models. Diversity and Distributions, 28(1), 128–141. doi:10.1111/ddi.13442\nBarbet-Massin, M., Jiguet, F., Albert, C. H., & Thuiller, W. (2012). Selecting pseudo-absences for species distribution models: how, where and how many? Methods in Ecology and Evolution, 3(2), 327–338. doi:10.1111/j.2041-210X.2011.00172.x\nBroussin, J., Mouchet, M., & Goberville, E. (2024). Generating pseudo-absences in the ecological space improves the biological relevance of response curves in species distribution models. Ecological Modelling, 498, 110865. doi:10.1016/j.ecolmodel.2024.110865\nDa Re, D., Tordoni, E., Lenoir, J., Lembrechts, J. J., Vanwambeke, S. O., Rocchini, D., & Bazzichetto, M. (2023). USE it: Uniformly sampling pseudo-absences within the environmental space for applications in habitat suitability models. Methods in Ecology and Evolution, 14(11), 2873–2887. doi:10.1111/2041-210X.14209\nKneib, T., Muller, J., & Hothorn, T. (2008). Spatial smoothing techniques for the assessment of habitat suitability. Environmental and Ecological Statistics, 15(3), 343–364. doi:10.1007/s10651-008-0092-x\nKramer-Schadt, S. et al. (2013). The importance of correcting for sampling bias in MaxEnt species distribution models. Diversity and Distributions, 19(11), 1366–1379. doi:10.1111/ddi.12096\nPhillips, S.J., Dudík, M., Elith, J., Graham, C.H., Lehmann, A., Leathwick, J. & Ferrier, S. (2009) Sample selection bias and presence-only distribution models: Implications for background and pseudo-absence data. Ecological Applications, 19(1), 181–197. doi:10.1890/07-2153.1"
  },
  {
    "objectID": "page_SDM.3_SpeciesData.html",
    "href": "page_SDM.3_SpeciesData.html",
    "title": "Species Data",
    "section": "",
    "text": "Species data form the foundation of ecological and biodiversity analyses. These data often include species occurrence records, abundance counts, and trait information, collected across both space and time.\nOne common type of species data is occurrence records — point data indicating where a species has been observed. In some cases, you may have a file containing presence-only records (locations where a species was detected). In other cases, you may work with presence-absence data, where each location is marked with whether the species was observed (present) or not (absent). These data can also apply to related domains, such as disease incidence records.\nIn the era of big data, many occurrence datasets are readily available through online biodiversity databases, often as georeferenced records.\nRegardless of the data source, the goal is to map the occurrence points and link them to relevant environmental conditions at each location. To do this, your dataset must include at least two columns with latitude and longitude coordinates indicating where each species record was collected or assumed absent."
  },
  {
    "objectID": "page_SDM.3_SpeciesData.html#species-occurrence-data-concepts-and-formats",
    "href": "page_SDM.3_SpeciesData.html#species-occurrence-data-concepts-and-formats",
    "title": "Species Data",
    "section": "1. Species occurrence data: concepts and formats",
    "text": "1. Species occurrence data: concepts and formats\nSpecies occurrence data typically include: - Presence-only records: Locations where a species has been observed - Presence-absence data: Locations where presence or absence was recorded - Abundance data: Counts or estimates of individuals observed\nCommon data fields include: - Species scientific name (species) - Geographic coordinates (longitude, latitude) - Observation date (date) - Observer or data source - Environmental metadata (optional)\nOccurrence data are accessible from a growing number of online databases. Notable examples of interest - GBIF (Global Biodiversity Information Facility): the largest open-access biodiversity database, aggregating millions of occurrence records from museums, citizen science, and research projects worldwide. - iNaturalist: a citizen science platform where users submit geo-tagged observations of plants, animals, and fungi, often with photos verified by a community of experts. - eBird: a global bird observation database maintained by the Cornell Lab of Ornithology, containing millions of bird sightings submitted by birdwatchers. - OBIS (Ocean Biogeographic Information System): a specialized database focusing on marine species distribution data from research cruises, museums, and observation networks. - Movebank: detailed location data of animal movement sequences (mostly from tagging data that recorded movement paths)\n\n\n1.1 Downloading species occurrence data from GBIF\nThe Global Biodiversity Information Facility (GBIF) is a key open-access resource for species occurrence data. Using the rgbif package, we can query GBIF and download occurrence records for a species of interest. Below is an example workflow to download occurrence records for Rhinolophus hipposideros (the lesser horseshoe bat), restricted to records from the United Kingdom:\n\nlibrary(rgbif)\n\n# Define species name\nspecies_name &lt;- \"Rhinolophus hipposideros\"\n\n# Check whether synonyms are recorded in GBIF\nname_suggest(q=species_name, rank='species')\n\n# Check number of records with coordinate information\nocc_search(scientificName = species_name , hasCoordinate=T, limit = 10)$meta$count\n\n# Get GBIF taxon key for the species\nkey &lt;- name_backbone(name = species_name)$speciesKey\n\n# Download occurrence data from the UK only (limited to 1000 records)\nocc_data_rhinhipp &lt;- occ_search(\n  taxonKey = key,\n  country = \"GB\",          # Filter to UK only\n  hasCoordinate = TRUE,\n  limit = 1000\n)\n\n# Extract the data frame\nocc_df_rhinhipp &lt;- occ_data_rhinhipp$data\n\n\n\n\n\n\n\nNoteOur focal species of the day!\n\n\n\n\n\nIn this tutorial, we use occurrence data of the Lesser Horseshoe Bat (Rhinolophus hipposideros). Distributed across most of Europe, it can be found in warmer regions with woodlands, riparian forests and pastures. Lesser Horseshoe Bats forage for arthropods, often flying not more than a few meters above the ground. They may take large prey back to a roost or perch. From May, mixed-sex maternity colonies are formed in maternity roosting sites, and the females have a single pup. Lesser Horseshoe Bats hibernate over the winter in caves, disused mines, tunnels and cellars. They tend to roost in old houses, churches, stables and barns. According to the IUCN Red List, their population is decreasing.\n\n\n\nRhinolophus hipposideros (source: faluke, via iNaturalist, photo 102259155)\n\n\nIt’s worth browsing literature databases using search strings like “species distribution model” AND “Rhinolophus hipposideros”. Reviewing recent titles and abstracts (from the past 5 to 10 years) can clarify how SDMs are applied to this species in both theoretical and applied contexts. At the same time, explore studies on the species’ life history, including diet, habitat use, climate sensitivity, and roosting behavior. A strong biological foundation is essential for creating models that reflect real-world ecology and produce meaningful results.\n\n\n\n\nPlotting the occurrence records\nWe can now plot the downloaded occurrence records on a map to explore their spatial distribution within the UK. Mapping the raw occurrence data is important step in exploring your dataset, helping you verify that points fall roughly where expected. At this stage however, the data are likely uncleaned, so avoid drawing strong conclusions until further validation and cleaning have been performed.\n\nlibrary(rnaturalearth)\nlibrary(ggplot2)\n\n# UK map\nUK_sf &lt;- ne_countries(scale = \"medium\", country = \"United Kingdom\", returnclass = \"sf\")\n\n# Plot the occurrence records over a UK basemap\nggplot() +\n  geom_sf(data = UK_sf, fill = \"grey90\", color = \"black\") +\n  geom_point(\n    data = occ_df_rhinhipp,\n    aes(x = decimalLongitude, y = decimalLatitude),\n    size = 1,\n    shape = 23,\n    fill = \"#6a3d9a\"\n  ) +\n  labs(\n    title = \"Records of Rhin. hipposideros in the UK\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nInteractive map of occurrence records\nTo further explore the spatial distribution of Rhinolophus hipposideros in the UK, we can use an interactive map. The map below is rendered using the leaflet package and allows zooming, panning, and clicking on individual points for more information.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipTip: Using your own field data\n\n\n\n\n\nYou can easily import species occurrence data from your own field surveys using tools like the readxl package (for Excel files) or read.csv() (for CSVs). Regardless of format, the key requirement is to prepare a data frame with at least two columns for coordinates — ideally longitude first and latitude second (think: X and Y axes).\nDifferent coordinate systems can be used, such as geographic coordinates (longitude/latitude) or projected systems like UTM (Easting/Northing). These can be converted between each other in R by specifying and transforming the coordinate reference system (CRS).\nAdditional useful columns may include: - A species name column (if working with multiple species) - A presence/absence indicator (1 for presence, 0 for absence)\nOnce formatted, your own data can be processed and analyzed in exactly the same way as GBIF or other downloaded datasets.\n\n\n\n\n\n\n\n\n\nTipUse filters wisely…\n\n\n\n\n\nWhen downloading large datasets, apply filters such as date range, coordinate presence, or country to reduce size and improve relevance."
  },
  {
    "objectID": "page_SDM.3_SpeciesData.html#data-cleaning-for-species-occurrence-data",
    "href": "page_SDM.3_SpeciesData.html#data-cleaning-for-species-occurrence-data",
    "title": "Species Data",
    "section": "2 Data cleaning for species occurrence data",
    "text": "2 Data cleaning for species occurrence data\nAs with many data-intensive workflows, cleaning your occurrence data is a crucial early step. Public database like GBIF are rich sources, but weren’t originally designed specifically for species distribution modelling (SDM) or systematic analyses. That means you will often need to filter, correct and validate the data so that your subsequent models are meaningful, reproducible and robust. Below are common issues, rationale, and reproducible workflows using R.\n\nWhat to check & why\n\nDuplicates: Records with identical coordinates (and metadata) can artificially inflate data density.\n\nOutliers: Points way outside expected range (e.g., in the ocean for terrestrial species, or extreme lat/lon).\n\nCoordinate validation: Missing values, lat/long reversed, zero coordinates, or values outside valid ranges.\n\nCountry/administrative mismatches: The “country” field may not match what the spatial location implies.\n\nUngeoreferenced localities: Text descriptions without coordinates; some may be georeferenced later, others discarded.\n\nSampling bias: Over‑sampling near roads, institutions; under‑sampling in remote areas.\n\n\n\nSampling bias in species occurrence data\nWhen species occurrence records are collected opportunistically (e.g. near roads, cities, institutions, or along accessible routes), sampling bias often arises. Some areas end up heavily sampled, others under-sampled. If not addressed, this bias can distort species distribution models in that overrepresented zones may dominate the environmental signal and lead to misleading predictions. On the other hand, high point density in some places may reflect genuine habitat suitability. Subsampling or thinning helps, but you must balance bias reduction against losing valid information.\nA related concept is scale: spatial grain (resolution) and extent affect what patterns you see. Organisms perceive environments differently at different scales — what seems clustered or patchy at one grain may be regular or homogeneous at another. So exploring how different grain sizes / sampling distances influence your data is useful.\n\n\n\n\n\n\n\nNoteExploring spatial resolution effects:\n\n\n\n\n\nHow does changing the spatial resolution (grain size) affect the number and distribution of retained species occurrences?\nWatch the animation below to see how coarser grids retain fewer points while finer grids retain more.\nThe raster grid is created with terra::rast() and its resolution adjusted using res().\nPoints are subsampled using stratified random sampling with terra::spatSample(), selecting one point per grid cell.\n\n\n\n\n\n\nCleaning species data using CoordinateCleaner\nNot all coordinates in species occurrence data are correct or reliable. We need to cross-check these carefully to ensure robust modelling.\nAn overview of coordinate cleaning methods is provided by Robert Hijmans in tutorials from the dismo package. A more recent and powerful tool is the CoordinateCleaner package (Zizka et al., 2019), which efficiently cleans geographic coordinates using multiple automated tests.\nThe key cleaning steps we will apply here include: - Removing duplicates (exact same coordinates and metadata). - Checking if coordinates match the reported country codes (test centroids). - Detecting spatial outliers far from expected ranges. - Testing proximity to known biodiversity institutions (e.g., botanical gardens) that may bias records. - Additional cleaning steps are available; rigorous exploration and testing are crucial for professional data cleaning.\n\n\n# Load libraries\nlibrary(CoordinateCleaner)\nlibrary(terra)\nlibrary(sf)\nlibrary(dplyr)\n\n# Convert to sf object for spatial operations\nocc_sf_rhinhipp &lt;- st_as_sf(occ_df_rhinhipp, coords = c(\"decimalLongitude\", \"decimalLatitude\"), crs = 4326)\n\n# Step 1: Remove exact duplicates (including metadata if needed)\nocc_unique_rhinhipp &lt;- occ_df_rhinhipp %&gt;% distinct()\n\n# Flag records with potentially problematic coordinates using various tests:\n# - 'centroids': points near country or province centroids (likely errors)\n# - 'duplicates': exact duplicate records\n# - 'equal': points with identical latitude and longitude values\n# - 'gbif': records flagged by GBIF as problematic\n# - 'institutions': points near biodiversity institutions (e.g., museums, botanical gardens)\n# - 'seas': points falling in the sea for terrestrial species\n# - 'zeros': points with zero coordinates (0,0)\n# The parameter inst_rad = 10000 sets the radius (in meters) around institutions to flag points\nocc_rhinhipp_flags &lt;- clean_coordinates(occ_unique_rhinhipp, lon=\"decimalLongitude\", lat=\"decimalLatitude\", countries=\"countryCode\", \n            tests=c(\"centroids\", \"duplicates\", \"equal\", \"gbif\", \"institutions\", \"zeros\"), inst_rad = 10000)\n\n# Subset the original data to keep only the records that passed all cleaning tests\nocc_rhinhipp_cleaned &lt;- occ_unique_rhinhipp[occ_rhinhipp_flags$.summary,]\n\n# Print number of records retained after cleaning\ncat(\"Number of records before cleaning:\", nrow(occ_unique_rhinhipp), \"\\n\")\n\nNumber of records before cleaning: 2000 \n\ncat(\"Number of records retained after cleaning:\", nrow(occ_rhinhipp_cleaned), \"\\n\")\n\nNumber of records retained after cleaning: 355 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteAccessing multiple occurrence databases with spocc\n\n\n\n\n\nThe spocc package is a versatile R tool that lets you query multiple biodiversity databases in a single workflow — including GBIF, eBird, iNaturalist, and OBIS.\nThis is especially helpful when working with taxa not well covered in a single database, or when aiming to compare sources.\nBelow is a simple example querying iNaturalist for the species Zosterops emiliae (Mountain Black-eye, a species endemic to Borneo).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExercises: Examing different species data\n\n\n\n\n\n\nDownload and map a different species:\nChoose a different species relevant to your region or interests and the use occ_search() to download up to 1000 records with coordinates, restricted to a country of your choice. Plot the occurrence points on a static map.\nExplore sampling bias and thinning:\nCreate a terra::rast() object covering the extent of your species data. Use stratified sampling (terra::spatSample()) to thin the data across raster cells of varying resolutions (e.g., 0.1, 0.25, 0.5 degrees). Plot the full vs. thinned datasets over a base map to compare spatial density.\n\n\n\n\n\n\n\n\n\n\nNoteLiterature\n\n\n\n\n\nAiello-Lammens, M. E., Boria, R. A., Radosavljevic, A., Vilela, B., & Anderson, R. P. (2015). spThin: an R package for spatial thinning of species occurrence records for use in ecological niche models. Ecography, 38(5), 541–545. doi:10.1111/ecog.01132\nBaker, D. J., Maclean, I. M. D., & Gaston, K. J. (2024). Effective strategies for correcting spatial sampling bias in species distribution models without independent test data. Diversity and Distributions, 30(3), e13802. doi:10.1111/ddi.13802\nInman, R., Franklin, J., Esque, T., & Nussear, K. (2021). Comparing sample bias correction methods for species distribution modeling using virtual species. Ecosphere, 12(3), e03422. doi:10.1002/ecs2.3422\nSteen, V. A., Tingley, M. W., Paton, P. W. C., & Elphick, C. S. (2021). Spatial thinning and class balancing: Key choices lead to variation in the performance of species distribution models with citizen science data. Methods in Ecology and Evolution, 12(2), 216–226. doi:10.1111/2041-210X.13525\nZizka, A., Silvestro, D., Andermann, T., Azevedo, J., Duarte Ritter, C., Edler, D., Farooq, H., Herdean, A., Ariza, M., Scharn, R., Svantesson, S., Wengström, N., Zizka, V. & Antonelli, A. (2019). CoordinateCleaner: Standardized cleaning of occurrence records from biological collection databases. Methods in Ecology and Evolution, 10(5), 744–751. doi:10.1111/2041-210X.13152"
  },
  {
    "objectID": "page_SDM.1_DataPreparation.html",
    "href": "page_SDM.1_DataPreparation.html",
    "title": "Data Preparation",
    "section": "",
    "text": "This page documents the steps used to download and save the datasets used throughout the workshop.\nAll workshop pages will simply reload the saved .RData file to ensure reproducibility and avoid repeatedly downloading data from online sources (this helps with speed and reduces server load when 30+ students are running the same code but for actual research, always use the most up-to-date data!)).\n\n\n\n\n\n\nNotePre-saved data\n\n\n\n\n\nFor teaching efficiency, the workshop pages use pre-saved datasets stored in /data/.\nThis ensures exercises run quickly and consistently without repeatedly downloading large files.\nIf you want to reproduce the full workflow, you can re-run the download and cropping steps.\nBe aware that downloading global rasters and GBIF data may take several minutes.\nFor full reproducibility, the data downloading steps are also documented below.\n\n\n\n\n\n\n\n\n\nNoteRequired R packages for the workshop\n\n\n\n\n\nMake sure the following packages are installed and loaded before proceeding. These will be used throughout the workshop:\n\nterra — for handling spatial raster and vector data\ngeodata — provides easy access to global climate, topography, land cover and other raster datasets\nsf — for vector data in simple features format\nrnaturalearth — for country and world maps\nrnaturalearthdata — provides Natural Earth vector data (used with rnaturalearth)\nspocc — for accessing species occurrence data from multiple databases\nrgbif — for GBIF data queries\nggplot2 — for custom plotting\nviridis — for color palettes\ntidyverse — for data manipulation/formatting\nleaflet — for plotting interactive maps\nCoordinateCleaner — for cleaning species data\neffects — visualising model effects\nggcorrplot — visualising correlation matrices\npROC — analysing and visualising ROC curves\nPresenceAbsence — evaluating presence-absence model performance\necospat — spatial ecology and species distribution modeling tools\n\nYou can install any missing packages by running:\n\n# Set CRAN mirror for non-interactive install\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\n\n# List of required packages\nrequired_packages &lt;- c(\n  \"terra\", \"geodata\", \"sf\", \"raster\",\n  \"rnaturalearth\", \"rnaturalearthdata\",\n  \"spocc\", \"rgbif\", \"ggplot2\", \"viridis\", \"tidyverse\", \"dplyr\",\n  \"leaflet\", \"CoordinateCleaner\", \"effects\",\n  \"ggcorrplot\", \"caret\", \"pROC\", \"PresenceAbsence\", \"ecospat\",\n  \"mgcv\", \"dismo\", \"gbm\", \"randomForest\", \"maxnet\",\n  \"biomod2\", \"sdm\", \"ENMeval\"\n)\n\n# Identify missing packages\nmissing_packages &lt;- required_packages[!(required_packages %in% rownames(installed.packages()))]\n\n# Install missing packages with dependencies\nif (length(missing_packages) &gt; 0) {\n  suppressWarnings(suppressMessages(\n    install.packages(missing_packages, dependencies = TRUE, quiet = TRUE)\n  ))\n}\n\n# Load all required packages\ninvisible(lapply(required_packages, function(pkg) {\n  suppressWarnings(suppressMessages(\n    library(pkg, character.only = TRUE)\n  ))\n}))\n\n\n\n\n\n\n\n\n# Load country boundaries for the entire world as an sf object\nWorld_sf &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\n# Load the United Kingdom boundary as an sf object\nUK_sf &lt;- ne_countries(scale = \"medium\", country = \"United Kingdom\", returnclass = \"sf\")\n\n# Load UK administrative regions (states/regions) as an sf object\nUK_admin_sf &lt;- ne_states(country = \"United Kingdom\", returnclass = \"sf\")\n\n# Subset the UK administrative regions to only East Wales\nWalesEast_admin_sf &lt;- UK_admin_sf[UK_admin_sf$region == \"East Wales\", ]\n\n\n\n\n\n\n# Download WorldClim data\nClim_extant &lt;- geodata::worldclim_global(var = \"bio\", \n                                         res = 5, path = \"data/\")\n\nnames(Clim_extant) &lt;- sub(\"^wc2.1_5m_\", \"\", names(Clim_extant))\n\n# Download elevation data\nElev_UK &lt;- geodata::elevation_30s(country = \"GB\", path = \"data/\")\n\n# Crop climate to UK\nClim_UK &lt;- terra::crop(Clim_extant, UK_sf)\n\n# Crop elevation to UK\nElev_UK &lt;- terra::crop(Elev_UK, UK_sf)\n\n# Download bioclimatic variables for a future scenario\n# Example: CMIP6 data for SSP5-8.5, year 2060, GCM = BCC-CSM2-MR, resolution = 5 arc-min\n# Shared Socioeconomic Pathways (SSPs) code 126: refers to the \"Paris Agreement compatible\" pathway with:\n# SSP1: Sustainability (world shifts toward green growth, equity, and environmental awareness)\n# Radiative forcing stabilizes at 2.6 W/m² by 2100\nClim_cmip6_2041_2060 &lt;- geodata::cmip6_world(var = \"bioc\", ssp = \"126\", model = \"BCC-CSM2-MR\", time = \"2041-2060\", \n                                             res = 5, path = \"data/\")\n\nnames(Clim_cmip6_2041_2060) &lt;- gsub(\"^bio\", \"bio_\", names(Clim_cmip6_2041_2060))\n\n# Crop future climate to UK\nClim_cmip6_2041_2060_UK  &lt;- terra::crop(Clim_extant, UK_sf)\n\n\n\n\n\n\n####\n# Focal species for the SDM workshop\nspecies_name &lt;- \"Rhinolophus hipposideros\"\nkey &lt;- name_backbone(name = species_name)$speciesKey\n\nocc_data_rhinhipp &lt;- occ_search(\n  taxonKey = key,\n  country = \"GB\",\n  hasCoordinate = TRUE,\n  limit = 2000\n)\n\nocc_df_rhinhipp &lt;- occ_data_rhinhipp$data\n\n# Note the date when GBIF data were downloaded\nattr(occ_df_rhinhipp, \"gbif_download_date\") &lt;- Sys.Date()\n\n\n####\n#  Target group species for species background data\n# Define target group (Chiroptera = bats)\nbat_taxon_key &lt;- name_backbone(name = \"Chiroptera\")$usageKey\n\n# Download bat occurrence records from GBIF\nbat_occ &lt;- rgbif::occ_search(\n  taxonKey = bat_taxon_key,\n  country = \"GB\",\n  hasCoordinate = TRUE,\n  limit = 10000  # increase if needed, but beware GBIF limits\n)\n\n# Extract data\nbat_df &lt;- bat_occ$data\n\n# Remove records for the focal species (Rhinolophus hipposideros)\ntarget_group_df &lt;- bat_df %&gt;%\n    # Select only the columns key, decimalLatitude, decimalLongitude\n  dplyr::select(species, key, decimalLatitude, decimalLongitude) %&gt;%\n  # Remove records with missing coordinates\n  filter(!is.na(decimalLongitude), !is.na(decimalLatitude)) %&gt;%\n  filter(!species %in% \"Rhinolophus hipposideros\") %&gt;%\n  dplyr::select(species, decimalLatitude, decimalLongitude)\n\n# Note the date when GBIF data were downloaded\nattr(target_group_df, \"gbif_download_date\") &lt;- Sys.Date()\n\n# Convert to sf object\ntarget_group_sf &lt;- st_as_sf(\n  target_group_df,\n  coords = c(\"decimalLongitude\", \"decimalLatitude\"),\n  crs = 4326\n)\n\n\n\n\n\n# Save vector and tabular data only\nsave(\n  World_sf, UK_sf, UK_admin_sf, WalesEast_admin_sf,\n  species_name, occ_df_rhinhipp, target_group_sf, \n  file = \"data/workshop_data.RData\"\n)\n\n# Save raster data separately as GeoTIFFs\nterra::writeRaster(Clim_UK, \"data/Clim_UK.tif\", overwrite = TRUE)\nterra::writeRaster(Elev_UK, \"data/Elev_UK.tif\", overwrite = TRUE)\nterra::writeRaster(Clim_cmip6_2041_2060_UK, \"data/Clim_cmip6_2041_2060_UK.tif\", overwrite = TRUE)"
  },
  {
    "objectID": "page_SDM.1_DataPreparation.html#save-datasets-for-reuse",
    "href": "page_SDM.1_DataPreparation.html#save-datasets-for-reuse",
    "title": "Data Preparation",
    "section": "",
    "text": "# Save vector and tabular data only\nsave(\n  World_sf, UK_sf, UK_admin_sf, WalesEast_admin_sf,\n  species_name, occ_df_rhinhipp, target_group_sf, \n  file = \"data/workshop_data.RData\"\n)\n\n# Save raster data separately as GeoTIFFs\nterra::writeRaster(Clim_UK, \"data/Clim_UK.tif\", overwrite = TRUE)\nterra::writeRaster(Elev_UK, \"data/Elev_UK.tif\", overwrite = TRUE)\nterra::writeRaster(Clim_cmip6_2041_2060_UK, \"data/Clim_cmip6_2041_2060_UK.tif\", overwrite = TRUE)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nSDM Workshop\n",
    "section": "",
    "text": "SDM Workshop\n\n\n\nThis workshop introduces Species Distribution Modeling using R and open datasets. Explore spatial data, species occurrences, environmental predictors, and learn to fit, assess, and compare species distribution models including GLMs, Random Forests, and MaxEnt. Build your skills to generate predictions and ensemble models confidently.\n\n\n\n\nKonstans Wells\n\n\nBiodiversity and Animal Health Ecology (BAHE) Group, Swansea University\n\n \n\n\n\n\n\nWorkshop Highlights\n\n\n\nUnderstand spatial data in R\n\n\nWork with occurrence and environmental data\n\n\nFit basic species distribution models: GLMs, Random Forests, MaxEnt\n\n\nAssess model performance and make projections\n\n\nBuild ensemble species distribution models\n\n\n\n\n\n\n\n\n\n\nNoteRequired R packages for the workshop\n\n\n\n\n\nMake sure the following packages are installed and loaded before proceeding. These will be used throughout the workshop:\n\nterra — for handling spatial raster and vector data\ngeodata — provides easy access to global climate, topography, land cover and other raster datasets\nsf — for vector data in simple features format\nraster — needed for fitting Bioclim models with dismo package\nrnaturalearth — for country and world maps\nrnaturalearthdata — provides Natural Earth vector data (used with rnaturalearth)\nspocc — for accessing species occurrence data from multiple databases\nrgbif — for GBIF data queries\nggplot2 — for custom plotting\ntidyverse — for data manipulation/formatting\nleaflet — for plotting interactive maps\nCoordinateCleaner — for cleaning species data\neffects — visualising model effects\nggcorrplot — visualising correlation matrices\ncaret — use for data partitioning\npROC — analysing and visualising ROC curves\nPresenceAbsence — evaluating presence-absence model performance\necospat — spatial ecology and species distribution modeling tools\nmgcv — fitting Generalised Additive Models (GAM)\ndismo, gbm — for Boosted Regression Trees and general SDM tools\nrandomForest — for fitting Random Forest models\nmaxnet — for fitting Maxent models\nbiomod2 — for ensemble modeling\nsdm — for ensemble modeling\nENMeval — for ensemble modeling\n\nYou can install any missing packages by running:\n\n# Set CRAN mirror for non-interactive install\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\n\n# List of required packages\nrequired_packages &lt;- c(\n  \"terra\", \"geodata\", \"sf\", \"raster\",\n  \"rnaturalearth\", \"rnaturalearthdata\",\n  \"spocc\", \"rgbif\", \"ggplot2\", \"tidyverse\",\n  \"leaflet\", \"CoordinateCleaner\", \"effects\",\n  \"ggcorrplot\", \"caret\", \"pROC\", \"PresenceAbsence\", \"ecospat\",\n  \"mgcv\", \"dismo\", \"gbm\", \"randomForest\", \"maxnet\",\n  \"biomod2\", \"sdm\", \"ENMeval\"\n)\n\n# Identify packages that are not installed\nmissing_packages &lt;- required_packages[!(required_packages %in% installed.packages()[, \"Package\"])]\n\n# Install missing packages with dependencies\nif (length(missing_packages) &gt; 0) {\n  suppressWarnings(suppressMessages(\n    install.packages(missing_packages, dependencies = TRUE, quiet = TRUE)\n  ))\n}\n\n# Load all required packages\ninvisible(lapply(required_packages, function(pkg) {\n  suppressWarnings(suppressMessages(\n    library(pkg, character.only = TRUE)\n  ))\n}))\n\n\n\n\n\n\n\n\n\n\nNoteOpen source data and other sources used in this workshop\n\n\n\n\n\nOpen-source data sources are detailed on the Data Preparation page.\nThis workshop was build with Quarto.\nThe entry image and favicon are composites incorporating a photograph of Rhinolophus hipposiderus by © d’Oriol Massana Valeriano and Adrià López-Baucells."
  },
  {
    "objectID": "page_SDM.2_SpatialDataR.html",
    "href": "page_SDM.2_SpatialDataR.html",
    "title": "Spatial Data in R",
    "section": "",
    "text": "Spatial data form the foundation of geographical analysis and ecological modeling. In R, spatial data are typically managed in two primary formats: vector and raster.\nBeyond data structure, it is increasingly recognized that ecological observations must be interpreted within an appropriate spatiotemporal context. Species responses depend not only on local environmental conditions, but also on spatial patterns, temporal dynamics, and scale-dependent processes. Ignoring these dimensions can obscure ecological signals or lead to misleading inferences about species–environment relationships.\nAdopting tidy data principles helps structure ecological data in ways that make relationships between observations, space, and time explicit. This not only supports more efficient and reproducible analysis, but also encourages conceptual clarity, enabling environmental scientists and practitioners to think carefully about what constitutes an observation, what variables describe it, and how they are interrelated. Such practices bridge ecological theory with modern data science workflows, enabling transparent model building, integration across datasets, and robust generalization of ecological insights. Ultimately, developing a general understanding of spatial data—and how it fits within broader data science concepts—is not a technical luxury, but a conceptual necessity. It strengthens one’s ability to formulate better ecological questions, design meaningful analyses, and translate data into insight about environmental change, biodiversity patterns, and ecosystem processes."
  },
  {
    "objectID": "page_SDM.2_SpatialDataR.html#vector-data-in-r---getting-started",
    "href": "page_SDM.2_SpatialDataR.html#vector-data-in-r---getting-started",
    "title": "Spatial Data in R",
    "section": "1. Vector data in R - getting started",
    "text": "1. Vector data in R - getting started\nVector data are at the core of spatial data analysis in ecology and environmental science. In R, vector data are typically managed using the terra or sf packages, both of which support reading, manipulating, and visualizing point, line, and polygon data.\nThe terra package (which we use in this workshop) handles spatial vector and raster data in an efficient, modern, and consistent way."
  },
  {
    "objectID": "page_SDM.2_SpatialDataR.html#points-in-space",
    "href": "page_SDM.2_SpatialDataR.html#points-in-space",
    "title": "Spatial Data in R",
    "section": "1.1 Points in space",
    "text": "1.1 Points in space\n\nWhat are spatial points?\nPoints are the simplest vector data geometry and are used to represent individual locations in space, such as:\n\nSpecies occurrence records\n\nWeather stations\n\nField survey sites\n\nGPS locations\n\nEach point is defined by a coordinate pair, typically in latitude/longitude (geographic) or projected coordinates (e.g., UTM).\n\n\n\n\nCreating points in R with terra\nYou can create spatial points manually using vect() from the terra package:\n\nlibrary(terra)\n\n# Define coordinates as a matrix for multiple cities (longitude, latitude)\ncoords &lt;- matrix(c(\n  -1.2577, 51.7520,   # Oxford\n  12.4964, 41.9028,   # Rome\n  -3.9440, 51.6214,   # Swansea\n   8.6821, 50.1109,   # Frankfurt\n   8.5417, 47.3769,   # Zurich\n   2.3522, 48.8566    # Paris\n), ncol = 2, byrow = TRUE)\n\ncity_name &lt;- c(\"Oxford\", \"Rome\", \"Swansea\", \"Frankfurt\", \"Zurich\", \"Paris\")\n\n# Create spatial vector of points (longitude, latitude)\npoints &lt;- terra::vect(coords, type = \"points\", crs = \"EPSG:4326\")\npoints$city &lt;- city_name\n\n# Get the extent of the points and expand it by 10% (for plotting)\next &lt;- ext(points)\nx_range &lt;- ext[1:2]\ny_range &lt;- ext[3:4]\n\nx_margin &lt;- diff(x_range) * 0.1\ny_margin &lt;- diff(y_range) * 0.1\n\nx_ext &lt;- c(x_range[1] - x_margin, x_range[2] + x_margin)\ny_ext &lt;- c(y_range[1] - y_margin, y_range[2] + y_margin)\n\n# Plot the points with expanded plot extent\nplot(points, col = \"blue\", pch = 16, xlim = x_ext, ylim = y_ext)\ntext(points, labels = points$city, pos = 4, cex = 0.8, col = \"darkblue\")\n\n\n\n\n\n\n\n# Check out the object\nclass(points)\n\n[1] \"SpatVector\"\nattr(,\"package\")\n[1] \"terra\"\n\n\nThis example creates spatial points for Oxford, Rome, Swansea, Frankfurt, Zurich, and Paris, using their geographic coordinates (longitude, latitude), and displays them as blue dots.\n\nCoordinate reference systems (CRS)\nEvery spatial object must have an associated coordinate reference system (CRS). This defines how the coordinates relate to real-world locations. A consistent CRS ensures that all spatial layers align correctly and can be meaningfully compared for accurate mapping, analysis, and ecological interpretation. Defined CRS are also necessary to project spatial entities onto the curved surface of the Earth, translating the three-dimensional globe into a two-dimensional map. Without a proper projection, distances, areas, and spatial relationships can become distorted, leading to misleading ecological inferences or inaccurate spatial modeling.\nYou can check and change the CRS of a vector object using:\n\n# Check the CRS of a spatial object\ncrs(points)         # Check the CRS\n\n# Assign a new CRS (e.g., UTM Zone 32N)\ncrs(points) &lt;- \"EPSG:32632\"\n\n\n\n\n\n\n\nNoteWhat is EPSG:4326?\n\n\n\n\n\nThe code ‘EPSG:4326’ refers to the ‘WGS84’ coordinate reference system — the global standard used by GPS. It is a geographic CRS that uses latitude and longitude in decimal degrees.\nYou may also see this specified using an older PROJ string:\n‘+proj=longlat +datum=WGS84’\nWhile this still works, it’s now recommended to use the more modern and readable EPSG code (“EPSG:4326”), which ensures better compatibility across spatial tools and platforms.\n\nOther commonly used CRS\nDepending on your analysis or region of interest, other major coordinate systems are often used:\nEPSG:3857 — Web Mercator, used by most online map services (Google, OpenStreetMap).\nEPSG:27700 — British National Grid, used for detailed mapping in the UK.\nEPSG:5070 — USA Contiguous Albers Equal Area projection, suitable for continental-scale ecological analysis.\nEach CRS may serve slightly different analytical needs — global navigation, web mapping, or high-accuracy regional modelling, so choosing the right one is necessary for spatial accuracy and comparability.\n\n\n\n\n\n\n\n\n\n\nTipTip: Always check CRS compatibility\n\n\n\n\n\nBefore combining or comparing spatial datasets, ensure they share the same CRS. Mixing different CRSs can lead to incorrect overlays, maps, or analyses."
  },
  {
    "objectID": "page_SDM.2_SpatialDataR.html#lines-and-polygons-in-space",
    "href": "page_SDM.2_SpatialDataR.html#lines-and-polygons-in-space",
    "title": "Spatial Data in R",
    "section": "1.2 Lines and polygons in space",
    "text": "1.2 Lines and polygons in space\n\nRepresenting Spatial Features Beyond Points\nIn spatial data presentation and analysis, lines and polygons are used to represent more complex spatial features:\n\nLines can represent:\n\nAnimal migration paths (see Movebank for real-world examples)\nRiver courses\nTransects used during field surveys\n\nPolygons are used for:\n\nProtected habitat areas (e.g., national parks)\nLand cover classes (e.g., forest, grassland)\nSurvey zones or administrative regions\n\n\n\n\n\nExample: Drawing a migration path (line) and a protected area (polygon)\nLet’s create a line connecting a sequence of waypoints representing a bird’s migration route.\n\n# Define coordinates along a hypothetical bird migration route for Greylag Goose (approximate, illustrative points)\nmigration_coords &lt;- matrix(c(\n  15.0, 60.0,   # Northern Sweden breeding region\n   9.0, 56.0,   # Denmark / southern Sweden stopover\n   5.0, 52.0,   # Northern Germany\n   2.0, 48.0,   # Northeastern France / Alsace\n  -1.0, 44.0,   # Southwestern France\n   0.0, 40.0    # Northern Spain / wintering area\n), ncol = 2, byrow = TRUE)\n\n# Create a spatial line object\nmigration_line &lt;- vect(migration_coords, type = \"lines\", crs = \"EPSG:4326\")\n\nNow let’s define a polygon representing a protected habitat area along the migration corridor (e.g., a wetland reserve in France).\n\n# Define polygon coordinates (rough bounding box for an example protected area)\nreserve_coords &lt;- matrix(c(\n  1.0, 48.4,   # NW corner\n  1.0, 47.2,   # SW corner\n  2.2, 47.2,   # Mid-south (new point to bend the shape)\n  3.0, 47.4,   # SE corner (further east)\n  3.0, 48.4,   # NE corner\n  1.0, 48.4    # Close the polygon\n), ncol = 2, byrow = TRUE)\n\n# Create spatial polygon\nreserve_poly &lt;- vect(reserve_coords, type = \"polygons\", crs = \"EPSG:4326\")\n\nPlotting both: Migration route + protected area\n\n# Get the individual extents\next_poly &lt;- ext(reserve_poly)\next_line &lt;- ext(migration_line)\n\n# Combine them using union\ncombined_ext &lt;- terra::union(ext_poly, ext_line)\n\n# Plot the polygon with combined extent\nplot(reserve_poly, col = \"lightblue\", border = \"blue\",\n     ext = combined_ext,\n     main = \"Migration path and protected area\")\n\n# Add the migration line\nlines(migration_line, col = \"darkgreen\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteInsight: polygon closure\n\n\n\n\n\nPolygons must have their first and last point identical to form a closed shape — otherwise they won’t render as expected.\nAlways verify your coordinate order and closure when constructing polygons manually."
  },
  {
    "objectID": "page_SDM.2_SpatialDataR.html#read-vector-data-from-files",
    "href": "page_SDM.2_SpatialDataR.html#read-vector-data-from-files",
    "title": "Spatial Data in R",
    "section": "1.3 Read Vector Data from Files",
    "text": "1.3 Read Vector Data from Files\nIn real-world ecological and environmental projects, vector data are rarely created manually. Instead, they are typically stored in spatial file formats like:\n\nShapefile (.shp) – the most widely used format for vector data; stores geometry and attributes across multiple sidecar files\nGeoJSON (.geojson) – web-friendly and human-readable\nGPKG (GeoPackage) – a modern, single-file spatial format\nKML – often used for visualisation in tools like Google Earth\n\n\n\n\n\n\n\nNoteInsight: Common sources of vector-based biodiversity data\n\n\n\n\n\nWe live in an era of big data, where biodiversity datasets are increasingly available in digital format and at global scale. These data can come from:\n\nStandardised monitoring schemes\n\nCitizen science platforms\n\nExpert-drawn range maps\n\nEach data source has its own strengths and limitations. In this workshop, we’ll explore different types of spatial biodiversity data and how to load and visualise them in R.\nIn this session, we focus on basic description of data types, more on the species and environmental data will be described in after sections of the workshop.\n\n\n\n\n\nBase maps as an example of polygons (using rnaturalearth)\nTo provide geographic context, we often need base maps of countries, continents, or regions. The rnaturalearth(https://github.com/ropensci/rnaturalearth) package provides an excellent interface to the Natural Earth datasets.\nBelow is the code used to create spatial objects for the UK and the East Wales region.\nThese have already been prepared in advance and saved to disk to speed up the workshop.\n\n\n\n\n\n\nNoteCode to generate the spatial layers (already done in data preparation)\n\n\n\n\n\n\nlibrary(rnaturalearth)\nlibrary(sf)\n\n# Load country and region maps\nWorld_sf &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\nUK_sf &lt;- ne_countries(scale = \"medium\", country = \"United Kingdom\", returnclass = \"sf\")\nUK_admin_sf &lt;- ne_states(country = \"United Kingdom\", returnclass = \"sf\")\n\n# Subset to East Wales\nWalesEast_admin_sf &lt;- UK_admin_sf[UK_admin_sf$region == \"East Wales\", ]\n\nThese steps are included in the data preparation file:\n👉 View full data preparation script\n\n\n\n\n\nVisualise East Wales\n\n# Load saved spatial objects\nlibrary(ggplot2)\nlibrary(sf)\n\n# Plot East Wales over the UK map\nggplot() +\n  geom_sf(data = UK_sf, fill = \"grey90\", color = \"black\") +\n  geom_sf(data = WalesEast_admin_sf, fill = \"steelblue\", color = \"darkblue\", size = 0.8) +\n  labs(\n    title = \"East Wales within the United Kingdom\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteUnderstanding spatial data structures in R\n\n\n\n\n\nFamiliarise yourself with the structure of sp and sf spatial data formats. In this workshop, we use modern packages like sf and terra, which follow the Simple Features standard for spatial vector data.\nUse the functions crs() (from terra) or st_crs() (from sf) to inspect and manipulate coordinate reference systems (CRS). These determine how geometries relate to real-world locations and are essential when combining spatial layers.\nTo learn more, check the Simple Features vignette or run:\n\nvignette(\"sf1\", package = \"sf\")\n\nThe websiterspatial.org, developed by a team of researchers led by Robert J. Hijmans, is an excellent resource for learning more about spatial data analysis and modeling in R.\n\n\n\n\n\n\n\n\n\n\nNoteKey spatial vector workflow skills\n\n\n\n\n\n\nUse terra::vect() to read shapefiles (e.g., IUCN range maps)\n\nUse rnaturalearth::ne_countries() to get base maps of countries or continents\n\nCombine both to map species ranges in a geographic context\n\nAlways check file format, coordinate reference system (CRS), and attribute content when working with real-world spatial data"
  },
  {
    "objectID": "page_SDM.2_SpatialDataR.html#raster-data-in-r",
    "href": "page_SDM.2_SpatialDataR.html#raster-data-in-r",
    "title": "Spatial Data in R",
    "section": "2. Raster Data in R",
    "text": "2. Raster Data in R\nRaster datasets often come in file formats such as:\n\n.tif (GeoTIFF) — the most widely used raster format\n.grd — often used in older R workflows\n.nc (NetCDF) — commonly used in climate science\n\nWe use the terra package to read and manipulate raster data in R. From this package, the function terra::rast() is used to create or read SpatRaster objects (the primary raster data class in terra).\n\n# Create a raster from scratch\nraster_1 &lt;- terra::rast(ncol = 50, nrow = 50, xmin = 0, xmax = 50,  ymin = 0, ymax = 50)\n\nWe can access the attributes of each raster cell by using the function terra::values(). Obviously, there are no values yet in the SpatRaster object and thus we assign some, generating random numbers with rnorm():\n\n# Check the summary stats of the values in raster_1:\nsummary(terra::values(raster_1))\n\nWarning: [readValues] raster has no values\n\n\n     lyr.1     \n Min.   : NA   \n 1st Qu.: NA   \n Median : NA   \n Mean   :NaN   \n 3rd Qu.: NA   \n Max.   : NA   \n NA's   :2500  \n\n# Assign values (randomly drawn from normal distribution) to the SpatRaster object:\nterra::values(raster_1) &lt;- rnorm(ncell(raster_1))\n\n# plot the raster\nplot(raster_1)\n\n\n\n\n\n\n\n\nRaster resolution determines the spatial grain of your data — the size of each pixel on the ground. Higher resolution (e.g., 10m) provides more detail, but is computationally heavier.\nThis concept of spatial scale is crucial in ecology. Analyses done at fine scales may yield different insights than those at coarse scales.\nTo demonstrate this, we aggregate the raster to a coarser resolution. With multiple raster objects at hand, combine layers into a multi-layer SpatRaster object using the terra::rast() function to concatenate the layers.\n\n# Aggregate the raster to a coarser resolution (2x2)\nraster_2 &lt;- aggregate(raster_1, fact = 2, fun = mean)\n\n# Match raster_2 (coarse) to raster_1 (fine resolution)\nraster_2_resamp &lt;- resample(raster_2, raster_1, method = \"bilinear\")\n\n# Create multi-layer raster object\nraster_multi &lt;- c(raster_1, raster_2_resamp)\n\n# Assign layer names\nnames(raster_multi) &lt;- c(\"Fine resolution\", \"Coarse resolution\")\n\n# Plot the raster layers\nplot(raster_multi)"
  },
  {
    "objectID": "page_SDM.2_SpatialDataR.html#read-raster-data-from-files",
    "href": "page_SDM.2_SpatialDataR.html#read-raster-data-from-files",
    "title": "Spatial Data in R",
    "section": "2.1 Read raster data from files",
    "text": "2.1 Read raster data from files\nIn ecological and environmental research, raster data are commonly used to represent continuous environmental variables like climate, elevation, or land cover.\nOne widely used dataset is the WorldClim v2.1 Bioclimatic Variables, which provides global climate data at multiple resolutions. You can explore the available variables here. To download and use these data in R, this can be efficiently done with the geodata package (part of the rspatial site developed by the rspatial.org group led by Robert J. Hijmans).\n\nCode to download WorldClim 2.1 bioclimatic variables (already done in data preparation)\n\nlibrary(geodata)\n\n# download WorldClim 2.1 bioclimatic variables (10-minute resolution, this returns a SpatRaster with 19 layers, bio1 to bio19)\nClim_extant &lt;- geodata::worldclim_global(var = \"bio\", res = 5, path = \"data/\")\n\n# Crop climate to UK\nClim_UK.GB &lt;- terra::crop(Clim_extant, UK_sf)\n\nThese steps are included in the data preparation file:\n👉 View full data preparation script\nWe can now extract the focal data of interest, here we crop the mean annual precipitation (bio12) data to the spatial extent of the UK.\n\n#| fig-width: 4\n#| fig-height: 5\n#| fig-align: left\n\n# Extract Bioclim variable 12: annual precipitation\nprecip_UK &lt;- Clim_UK [[12]]  # bio12: annual precipitation in mm\n\n# Plot the precipitation map\nterra::plot(precip_UK, main = \"Mean annual precipitation (bio12) - UK\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipTip: Coordinate systems must match\n\n\n\n\n\nWhen working with raster and vector data together (e.g., cropping or masking), ensure both objects use the same CRS. Use terra::project() to reproject if needed."
  },
  {
    "objectID": "page_SDM.2_SpatialDataR.html#manipulate-raster-data",
    "href": "page_SDM.2_SpatialDataR.html#manipulate-raster-data",
    "title": "Spatial Data in R",
    "section": "2.2 Manipulate raster data",
    "text": "2.2 Manipulate raster data\n\nReproject raster data\n\n# Reproject raster to a different CRS (e.g. British National Grid EPSG:27700)\nprecip_UK_proj &lt;- terra::project(precip_UK, \"EPSG:27700\")\n\nterra::plot(precip_UK_proj, main = \"Precipitation (Reprojected)\")\n\n\n\n\n\n\n\n\n\n\nRaster math / algebra\n\n#| fig-width: 4\n#| fig-height: 5\n#| fig-align: left\n\n# Math / algebra example: Convert mm to cm for precipitation\nprecip_cm &lt;- precip_UK / 10\n\n# Difference between original and converted\nterra::plot(precip_cm - precip_UK, main = \"Difference after unit conversion\")\n\n\n\n\n\n\n\n# Apply mean filter with 3x3 window\nfocal_mean &lt;- terra::focal(precip_UK, w = matrix(1, 3, 3), fun = mean, na.policy = \"omit\")\n\nterra::plot(focal_mean, main = \"Smoothed Precipitation (3x3 mean)\")\n\n\n\n\n\n\n\n\n\n\nZonal statistics\nSummarising raster data over polygonal spatial units is a common task in ecological and environmental analysis (and the broader field of applied data science). This is known as zonal statistics. For example, we can compute and visualise mean annual precipitation across UK administrative zones using the terra and ggplot2 packages:\n\n#| fig-width: 4\n#| fig-height: 5\n#| fig-align: left\n\n# Add an ID column to UK_admin_sf (to explicitly match IDs between zonal_means and UK_admin_sf below)\nUK_admin_sf$admin_id &lt;- 1:nrow(UK_admin_sf)\n\n# Convert UK_admin_sf to SpatVector\nzones_vect &lt;- terra::vect(UK_admin_sf)\n\n# Compute mean precipitation by zone\nzonal_means &lt;- terra::extract(precip_UK, zones_vect, fun = mean, na.rm = TRUE, ID = TRUE)\n\n# Preview output\nhead(zonal_means)\n\n  ID   bio_12\n1  1 1504.444\n2  2 1716.789\n3  3 1312.486\n4  4 1131.222\n5  5 1029.231\n6  6 1376.529\n\n# Replace NA values with 0 for demonstration purposes\n# !Note: This should be done cautiously — replacing NA may not be appropriate depending on your analysis.\nprecip_no_na &lt;- terra::classify(precip_UK, cbind(NA, 0))\n\n# Merge zonal means (bio12 = mean annual precipitation) back to the spatial data (ensure correct ID match)\nUK_admin_sf$mean_precip_mm &lt;- zonal_means$bio_12[match(UK_admin_sf$admin_id, zonal_means$ID)]\n\n# Plot the map of mean precipitation by zone\nggplot(UK_admin_sf) +\n  geom_sf(aes(fill = mean_precip_mm), color = \"white\") +\n  scale_fill_viridis_c(name = \"Annual Precipitation (mm)\", na.value = \"grey90\") +\n  labs(\n    title = \"Annual precipitation by UK administrative zone\",\n    caption = \"Data source: WorldClim 2.1 + Natural Earth\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "page_SDM.2_SpatialDataR.html#extract-raster-values",
    "href": "page_SDM.2_SpatialDataR.html#extract-raster-values",
    "title": "Spatial Data in R",
    "section": "2.3 Extract Raster Values",
    "text": "2.3 Extract Raster Values\nExtracting raster values at specific locations is a key step in many ecological and environmental workflows, particularly in species distribution modelling. This process allows us to link spatially continuous variables—such as climate or elevation—to discrete observation points, like species occurrence records. In this section, we demonstrate how to extract raster values using the terra package, both for point locations and across multiple raster layers.\n\n# Set seed for reproducibility\nset.seed(1)\n\n# Convert UK_sf object (an sf object) into a SpatVector object\nUK_vect &lt;- terra::vect(UK_sf)\n\n# Simulate some (arbitrary) species occurrence points within the UK\nocc_points &lt;- terra::spatSample(UK_vect[,1], size = 100, method = \"random\")\n\n# Extract raster values at these point locations\nocc_precip &lt;- terra::extract(precip_UK, occ_points)\n\n# Combine coordinates and extracted values\nocc_df &lt;- cbind(as.data.frame(occ_points), precip_mm = occ_precip[,2])\n\nhead(occ_df)\n\n       featurecla precip_mm\n1 Admin-0 country       813\n2 Admin-0 country       897\n3 Admin-0 country       599\n4 Admin-0 country       857\n5 Admin-0 country       577\n6 Admin-0 country      1472\n\n# Extract all 19 bioclim variables (multiple raster layers) for each point\nclim_data_vals &lt;- terra::extract(Clim_UK, occ_points)\n\n# Combine with species occurrence coordinates\nclim_data_df &lt;- cbind(as.data.frame(occ_points), clim_data_vals[,-1])  # Remove ID column\n\nhead(clim_data_df)\n\n       featurecla    bio_1    bio_2    bio_3    bio_4    bio_5     bio_6\n1 Admin-0 country 7.281042 7.019583 38.77366 413.2269 17.42100 -0.683000\n2 Admin-0 country 9.604292 7.539417 37.85608 463.8675 21.04300  1.127000\n3 Admin-0 country 9.550167 8.080500 38.05275 486.4427 21.86000  0.625000\n4 Admin-0 country 6.861417 7.221333 38.35219 432.1033 17.46600 -1.363000\n5 Admin-0 country 9.800042 6.891583 34.70257 477.9860 21.01500  1.156000\n6 Admin-0 country 8.965159 7.169326 39.33672 417.4897 19.32128  1.095745\n     bio_7     bio_8     bio_9   bio_10   bio_11 bio_12 bio_13 bio_14   bio_15\n1 18.10400  7.780500  8.499666 12.63950 2.659833    813     85     55 15.05392\n2 19.91600  5.297667 13.855500 15.54483 4.369333    897    100     52 24.57603\n3 21.23500 10.247167  5.786000 15.72250 3.987000    599     59     37 13.67015\n4 18.82900  4.599000  8.310667 12.46333 1.977000    857     94     57 18.63359\n5 19.85900  7.771833  5.868667 15.91117 4.354167    577     61     34 19.14082\n6 18.22553  4.776773 12.928192 14.33546 4.185993   1472    171     84 25.41553\n  bio_16 bio_17 bio_18 bio_19\n1    240    172    190    208\n2    298    168    175    267\n3    172    127    144    148\n4    260    174    186    233\n5    174    112    158    141\n6    483    259    285    463\n\n\n\n\n\n\n\n\n\nNoteUseful sources\n\n\n\n\n\nHijmans, R. J. (2023). rspatial.org: Spatial Data in R. https://rspatial.org/\nLovelace, R., Nowosad, J., & Muenchow, J. (2023). Geocomputation with R. https://geocompr.robinlovelace.net\nPebesma, E. (2023). Simple Features for R: Standardized Support for Spatial Vector Data. https://r-spatial.github.io/sf/index.html\n\n\n\n\n\n\n\n\n\nNoteExercises: Load and Inspect a Raster\n\n\n\n\n\n\nLoad and plot raster:\nLoad the raster file bioclim_01.tif using the terra package. Plot the raster using a unique and appropriate color scale (e.g., viridis).\nExtract climate values at city locations:\nUse coordinates of 10 cities in the UK and Italy (or any other countries you choose). Extract and compare the climate values (mean annual temperature, bio1) at these points.\nCalculate zonal means for countries:\nUsing polygon shapefiles of European countries or countries worldwide, calculate the zonal mean of bio1 (mean annual temperature) for each country. Visualize the result with a choropleth map."
  },
  {
    "objectID": "page_SDM.4_EnvironmentalData.html",
    "href": "page_SDM.4_EnvironmentalData.html",
    "title": "Environmental Data",
    "section": "",
    "text": "In species distribution modelling (SDM) and related ecological applications such as habitat suitability assessments, understanding the relationship between species occurrences and the underlying environmental conditions is essential. Environmental data form the backbone of SDMs, providing the contextual information needed to analyse where species are found and, importantly, where they could be found under different conditions (according to the predictions from the underlying model).\nSince SDMs aim not only to identify ecological relationships but also to project these relationships across broader spatial extents or under future environmental scenarios, access to landscape-scale environmental data is critical. Environmental variables of abiotic conditions such as climate, topography, soil, and land cover or commonly of key interest to describe the ecological and spatial constraints that influence species distributions. These variables are often derived from remote sensing products, interpolated climate surfaces, digital elevation models, and in-situ measurements.\nSelecting relevant and high-quality environmental predictors is a key step in building robust, interpretable models. Considerations such as spatial and temporal resolution, variable collinearity, and ecological relevance all play a role in shaping model outcomes.\nIn this part of the workshop, we will:\nExplore the main types of environmental datasets commonly used in SDM\nDiscuss spatial and temporal resolution — and why they matter\nLearn how to access, prepare, and manage environmental layers for modelling\nAddress common challenges such as collinearity, scaling, and data gaps\nUnderstanding your environmental data is just as important as understanding your species data.\nTogether, they define the basis for making informed, ecologically meaningful predictions about species distributions, be it in the present, the past, and under future scenarios."
  },
  {
    "objectID": "page_SDM.4_EnvironmentalData.html#climate-data",
    "href": "page_SDM.4_EnvironmentalData.html#climate-data",
    "title": "Environmental Data",
    "section": "1. Climate data",
    "text": "1. Climate data\nClimate variables are critical predictors in SDMs, as they directly influence species physiology, habitat suitability, and ecological interactions. Commonly used climate variables include temperature (e.g., mean annual, maximum of warmest month), precipitation (e.g., total annual, seasonality), and derived bioclimatic variables.\nIn R, climate data can be easily accessed, processed, and visualized using a combination of packages and global datasets. One of the most widely used sources is WorldClim, which provides high-resolution global climate layers. You can explore the available variables here. To download and use these data in R, this can be efficiently done with the package. Other sources of climate data include for example the CHELSA database\n\n1.1 Accessing climate data\nThe geodata package simplifies the process of downloading WorldClim and CHELSA data. As discussed in the section “3. Raster Data in R”, the tera package can be used to crop and mask climate data to the study region to reduce computational load and improve model relevance.\n\nCode to access climate data (already done in data preparation)\n\n# Load required libraries\nlibrary(rnaturalearth)\nlibrary(terra)\nlibrary(geodata)\n\n# Create a temporary directory to store raster data\ntmp_dir &lt;- tempdir()  # automatically cleaned up when R session ends\n\n# Download WorldClim bioclimatic variables (1970–2000 baseline)\nClim_extant &lt;- geodata::worldclim_global(var = \"bio\", \n                        res = 10,          # resolution of 10 arc-minutes (~18 km)\n                        path = tmp_dir)   \n\n# View variable names\nnames(Clim_extant)\n\n# UK map\nUK_sf &lt;- ne_countries(scale = \"medium\", country = \"United Kingdom\", returnclass = \"sf\")\nUK_vect &lt;- terra::vect(UK_sf)\n\nClim_UK_crop &lt;- terra::crop(Clim_extant, UK_vect)\nClim_UK &lt;- terra::mask(Clim_UK_crop, UK_vect)\n\n# Data is in memory and temporary folder; no permanent files created\n\n\n\nPlot climate data\n\n# Define color palette\ncol_palette &lt;- colorRampPalette(c(\"blue\", \"yellow\", \"orange\"))(20)\n\n# Plot the first bioclimatic variable for the UK\nplot(Clim_UK[[1]], \n     main = names(Clim_UK)[1], \n     col = col_palette)\n\n\n\n\n\n\n\n\n\n\n\n\n1.2 Future climate data\nProjecting future species distributions under climate change is essential for understanding potential range shifts, extinction risks, and guide future conservation planning. Future climate data provide model inputs that represent different greenhouse gas concentration trajectories over time.\nThese data are generated by global climate models (GCM) and made available through datasets like WorldClim and CHELSA, which provide downscaled, bias-corrected projections for various future scenarios. Downscaled future climate data from WorldClim v2.1, which includes projections from multiple GCMs can be also accessed with the help of the geodata package. In this workshop, we use CMIP-6 (Coupled Model Intercomparison Project Phase 6) climate projections, which provide simulations of future climate based on various greenhouse gas concentration pathways (known as SSPs). These models incorporate complex physical processes and interactions within the atmosphere, oceans, land surface, and ice.\n\n# Download future climate data (CMIP6 projections for 2041–2060)\n# We use bioclimatic variables (bioc) at 5 arc-min resolution (~9 km)\n# Scenario: SSP1-2.6 (“Paris Agreement compatible”), GCM = BCC-CSM2-MR\n# SSP1: Sustainability pathway – world shifts toward green growth and equity\n# Radiative forcing stabilizes at 2.6 W/m² by 2100\nClim_cmip6_2041_2060 &lt;- geodata::cmip6_world(\n  var   = \"bioc\",        # bioclimatic variables\n  ssp   = \"126\",         # SSP1-2.6 scenario\n  model = \"BCC-CSM2-MR\", # Global climate model\n  time  = \"2041-2060\",   # target period\n  res   = 5,             # resolution (arc-minutes)\n  path  = \"data/\"        # folder to save downloaded files\n)\n\n# View variable names\nnames(Clim_future_cmip6_2041_2060)\n\n# Plot a variable (e.g., future annual mean temperature)\nterra::plot(Clim_future_cmip6_2041_2060[[1]])\n\n\n\n\n\n\n\n\n\n\nThe figure below shows the projected change in annual mean temperature relative to a historical baseline (1970–2000). The animation cycles through three future periods (2021–2040, 2041–2060, 2061–2080). Each frame represents the spatial pattern of temperature change (in °C) predicted by CMIP-6 for that period, highlighting where and by how much temperatures are expected to increase or decrease in the UK.\n\n\n\nProjected temperature change over time according to the CMIP-6 model.\n\n\n\n\n\n\n\n\n\nNoteChoosing among GCMs and SSPs scenarios\n\n\n\n\n\nThere is no single “best” model or scenario for future climates. Best practice includes:\n\nEnsemble modelling: Use multiple GCMs to average predictions and estimate uncertainty.\nScenario comparison: Run projections under different Shared Socioeconomic Pathways (SSP) to capture a range of plausible futures.\nTemporal exploration: Compare mid- and end-century scenarios for temporal trends."
  },
  {
    "objectID": "page_SDM.4_EnvironmentalData.html#land-cover-data",
    "href": "page_SDM.4_EnvironmentalData.html#land-cover-data",
    "title": "Environmental Data",
    "section": "2. Land cover data",
    "text": "2. Land cover data\nLand cover is an essential environmental predictor in species distribution modelling (SDM), representing the physical surface of the Earth—such as forests, grasslands, wetlands, croplands, and urban areas. It provides crucial information about habitat availability, fragmentation, and human impacts on ecosystems.\nIn R, land cover data can be accessed and processed using remote sensing-derived datasets and spatial packages.\n\n\nAccessing Global Land Cover Data\nSeveral global land cover datasets are publicly available, including:\n\nCopernicus CGLS-LC100: Global land cover maps at 100m spatial resolution\nMODIS MCD12Q1: Global land cover maps at 500m resolution\nESA WorldCover: Global land cover at 10m resolution\nMODIS NDVI (Normalized Difference Vegetation Index): Satellite-derived vegetation greenness index available at 250m resolution, useful for tracking vegetation health and phenology.\nSentinel-2 Vegetation Indices: High-resolution (10-60m) vegetation indices including NDVI, EVI, and others, from the European Sentinel-2 mission.\n\nAmong these, the ESA WorldCover dataset can be accessed and downloaded using the geodata R package. However, this direct download option is not available for all land cover datasets.\n\n# EDS land cover data (variable selected: trees) for the UK\nlc_trees &lt;- geodata::landcover(var = \"trees\")"
  },
  {
    "objectID": "page_SDM.6_SDMSimpleModelFitting.html",
    "href": "page_SDM.6_SDMSimpleModelFitting.html",
    "title": "Simple model fitting",
    "section": "",
    "text": "We now arrive at the core of species distribution modelling (SDM): fitting the model. Having prepared occurrence, background, and environmental data, this section introduces a foundational yet flexible approach — the generalised linear model (GLM) to estimate habitat suitability for Rhinolophus hipposideros across the UK.\nSDMs quantify relationships between observed species distributions and environmental predictors, then project these relationships in space or time to identify potentially suitable areas under current or future conditions. While numerous algorithms exist, including MaxEnt, Random Forest, Boosted Regression Trees, and ensemble methods — the choice of model should reflect the modelling objective: inference, spatial prediction, or forecasting (Zurell et al. 2020).\nGLMs, despite being statistically “simpler” than many machine learning alternatives, remain widely used because they are interpretable, computationally efficient, and well-suited to presence–background data. However, building meaningful SDMs still requires rigorous conceptualisation:\nAre the data spatially biased or autocorrelated?\nAre predictor variables collinear or ecologically redundant?\nIs the model too simple or overly complex?\nAre we modelling for understanding, mapping, or future projection?\nIn this session, we will fit a logistic regression model using key bioclimatic and topographic predictors. You will explore how to interpret model coefficients, predict across environmental space, and visualise suitability maps. This exercise lays the foundation for more advanced techniques — including model validation, spatial transfer, and ensembling, which we will explore in subsequent sessions.\nFor further background, see foundational SDM texts such as Elith & Leathwick (2009), and Franklin (2010), and methodological overviews and tutorials (e.g., Zurell et al. 2020, Araújo et al. 2019)."
  },
  {
    "objectID": "page_SDM.6_SDMSimpleModelFitting.html#fit-a-basic-binomial-glm-logistic-regression",
    "href": "page_SDM.6_SDMSimpleModelFitting.html#fit-a-basic-binomial-glm-logistic-regression",
    "title": "Simple model fitting",
    "section": "1. Fit a basic binomial GLM (logistic regression)",
    "text": "1. Fit a basic binomial GLM (logistic regression)\n\n\n\n\n\n\n\n\n\nFigure 1: Illustration of presence–absence data for bats collected across forest stands differing in structure and rainfall. The biological questions can be framed verbally or represented in a generalised linear model (GLM) using relevant environmental predictors.\n\n\n\n\n\n\n\n\n\n\n\n\nNoteGLM refresher: Logistic regression basics\n\n\n\n\n\nBefore applying generalised linear models (GLMs) in species distribution modelling, let’s briefly revisit the fundamentals of logistic regression — the most common approach for modelling binary outcomes, such as species presence or absence.\nPresence–absence data are common in ecological and environmental research, but also in many other domains, such as infection status in epidemiology or success/failure in behavioural studies. Logistic regression models the probability of an event (e.g., species presence) as a function of one or more predictors, using a logit link function to map probabilities to the real line.\nTo illustrate, we begin with a simple example — random presence–absence data simulated as Bernoulli trials:\n\n# Draw random binary outcomes (e.g., 10% chance of presence)\ny &lt;- rbinom(50, 1, 0.1)\n\n# Observed mean presence probability\nmu_y &lt;- mean(y)\nmu_y\n\n[1] 0.08\n\n\nThis observed mean can be estimated with a logistic regression model. A model with no predictors is an intercept-only model:\n\n# Intercept-only GLM\nmod &lt;- glm(y ~ 1, family = \"binomial\")\n\n# Inspect design matrix (only intercept)\nmodel.matrix(mod)\n\n   (Intercept)\n1            1\n2            1\n3            1\n4            1\n5            1\n6            1\n7            1\n8            1\n9            1\n10           1\n11           1\n12           1\n13           1\n14           1\n15           1\n16           1\n17           1\n18           1\n19           1\n20           1\n21           1\n22           1\n23           1\n24           1\n25           1\n26           1\n27           1\n28           1\n29           1\n30           1\n31           1\n32           1\n33           1\n34           1\n35           1\n36           1\n37           1\n38           1\n39           1\n40           1\n41           1\n42           1\n43           1\n44           1\n45           1\n46           1\n47           1\n48           1\n49           1\n50           1\nattr(,\"assign\")\n[1] 0\n\n# Model summary\nsummary(mod)\n\n\nCall:\nglm(formula = y ~ 1, family = \"binomial\")\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -2.4423     0.5213  -4.685  2.8e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 27.877  on 49  degrees of freedom\nResidual deviance: 27.877  on 49  degrees of freedom\nAIC: 29.877\n\nNumber of Fisher Scoring iterations: 5\n\n\nBecause logistic regression uses the logit link, model coefficients are on the log-odds scale. To convert to probabilities, we apply the inverse-logit (or logistic) function:\n\n# Define inverse logit function\ninvlogit &lt;- function(x) { exp(x) / (1 + exp(x)) }\n\n# Estimate mean probability from model\nmu_mod &lt;- invlogit(coef(mod))\nmu_mod\n\n(Intercept) \n       0.08 \n\n\nThis simple model demonstrates how GLMs estimate the expected occurrence probability of a binary outcome. Understanding this structure is essential for interpreting more complex SDMs where environmental covariates explain spatial patterns in species occurrence.\nWe now move on to fitting such models using actual species and environmental data.\n\n\n\n\n1.1 Fitting the first GLM: a single environmental predictor\nWe now fit our first statistical species distribution model: a generalised linear model (GLM) with a logit link function and a binomial error distribution.\nGLMs extend ordinary least squares regression by allowing non-Gaussian responses and link functions that map probabilities to the real line. In our case:\n\nResponse variable: species occurrence (pa, 1 = presence, 0 = pseudo-absence)\n\nPredictor: one environmental covariate (here, elevation)\n\nModel family: binomial with logit link (logistic regression, applicable to a binary response variable)\n\nThis simple model quantifies how occurrence probability of Rhinolophus hipposideros varies along an environmental gradient.\n\n# Make sure sdm_df_clean$pa contains 0/1 numeric values, not factors\nsdm_df_clean$pa &lt;- as.numeric(sdm_df_clean$pa == 1)\n\n# Fit logistic regression with elevation as the only predictor\nglm_elev &lt;- glm(pa ~ elevation,\n                data = sdm_df_clean,\n                family = binomial(link = \"logit\"))\n\nsummary(glm_elev)\n\n\nCall:\nglm(formula = pa ~ elevation, family = binomial(link = \"logit\"), \n    data = sdm_df_clean)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  0.1347145  0.1338897   1.006   0.3143  \nelevation   -0.0011801  0.0006495  -1.817   0.0692 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 912.90  on 658  degrees of freedom\nResidual deviance: 909.55  on 657  degrees of freedom\nAIC: 913.55\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe coefficients from a logistic regression are expressed on the log-odds scale. The intercept can be transformed with the logistic (inverse-logit) function to give the baseline probability of occurrence (at predictor = 0). For slope terms, exponentiation (exp()) gives the odds ratio associated with a one-unit change in the predictor, rather than a direct probability\n\n# Define inverse logit\ninvlogit &lt;- function(x) exp(x) / (1 + exp(x))\n\n# Convert intercept to probability\ninvlogit(coef(glm_elev)[1])\n\n(Intercept) \n  0.5336278 \n\n# Odds ratio for 1-unit change in elevation\nexp(coef(glm_elev)[\"elevation\"])\n\nelevation \n0.9988206 \n\n\nVisualising the fitted response curve\nTo make the model more intuitive, we can plot the observed data and overlay the fitted probability curve.\n\n# Generate prediction grid\nelev_seq &lt;- data.frame(elevation = seq(min(sdm_df_clean$elevation, na.rm = TRUE),\n                                       max(sdm_df_clean$elevation, na.rm = TRUE),\n                                       length.out = 200))\n\n# Predict occurrence probability\nelev_seq$pred_prob &lt;- predict(glm_elev, newdata = elev_seq, type = \"response\")\n\n# Plot data + fitted curve\nggplot(sdm_df_clean, aes(x = elevation, y = pa)) +\n  geom_jitter(height = 0.05, alpha = 0.3, colour = \"darkblue\") +\n  geom_line(data = elev_seq, aes(x = elevation, y = pred_prob),\n            colour = \"firebrick\", linewidth = 1.2) +\n  labs(x = \"Elevation (m)\",\n       y = \"Habitat suitability (scaled 0–1)\",\n       title = \"GLM with elevation as predictor\") +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\nFigure 2: Relationship between elevation and probability of occurrence for R. hipposideros under a logistic regression model.\n\n\n\n\n\n\n\n\n\n\n\nNoteInterpreting the elevation-only GLM\n\n\n\n\n\nA positive slope means that the relative probability of occurrence increases with elevation.\nA negative slope means that the relative probability of occurrence decreases with elevation.\nThe intercept reflects the baseline log-odds, here at elevation = 0. Note that interpreting the intercept assumes a meaningful value at ‘elevation = 0’, which may be outside the observed range. It is often better to center predictors (e.g., subtract mean elevation or use the ‘scale()’ function for scaling all variables) for interpretability.\nWhile simple, this first GLM illustrates the core mechanics of SDMs: linking species occurrence data to environmental gradients. However, because we use presence–pseudoabsence data, the fitted values should not be interpreted as absolute occurrence probabilities — the number of background points (the “zeros”) is arbitrary. Instead, model outputs represent relative suitability across the environmental gradient.\nIn practice, species–environment relationships are often non-linear, motivating the use of transformations (e.g. quadratic terms) or more flexible algorithms.\n\n\n\n\n\n\n1.2 Extending the GLM: quadratic response curve\nSpecies–environment relationships are often non-linear: too cold or too hot, too wet or too dry. A quadratic term allows us to capture such unimodal responses.\nWe use the poly() function in R, which orthogonalises polynomial terms to reduce collinearity.\n\n# Fit logistic regression with 2nd order polynomial for elevation\nglm_elev_poly &lt;- glm(pa ~ poly(elevation, 2),\n                     data = sdm_df_clean,\n                     family = binomial(link = \"logit\"))\n\nsummary(glm_elev_poly)\n\n\nCall:\nglm(formula = pa ~ poly(elevation, 2), family = binomial(link = \"logit\"), \n    data = sdm_df_clean)\n\nCoefficients:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          -0.11654    0.08254  -1.412  0.15796    \npoly(elevation, 2)1  -8.33379    2.87411  -2.900  0.00374 ** \npoly(elevation, 2)2 -16.61301    3.40390  -4.881 1.06e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 912.90  on 658  degrees of freedom\nResidual deviance: 876.27  on 656  degrees of freedom\nAIC: 882.27\n\nNumber of Fisher Scoring iterations: 5\n\n\nVisualising the quadratic response\n\n# Predict occurrence probability for elevation range\nelev_seq$pred_prob_poly &lt;- predict(glm_elev_poly,\n                                   newdata = elev_seq,\n                                   type = \"response\")\n\n# Plot linear vs quadratic fits\nggplot(sdm_df_clean, aes(x = elevation, y = pa)) +\n  geom_jitter(height = 0.05, alpha = 0.3, colour = \"darkblue\") +\n  geom_line(data = elev_seq, aes(x = elevation, y = pred_prob),\n            colour = \"firebrick\", size = 1.1, linetype = \"dashed\") +\n  geom_line(data = elev_seq, aes(x = elevation, y = pred_prob_poly),\n            colour = \"forestgreen\", linewidth = 1.2) +\n  labs(x = \"Elevation (m)\",\n       y = \"Habitat suitability (scaled 0–1)\",\n       title = \"GLM with linear vs quadratic elevation effect\") +\n  theme_minimal(base_size = 14)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\nFigure 3: Quadratic logistic regression showing unimodal response of R. hipposideros to elevation.\n\n\n\n\n\n\n\n\n\n\n\nNoteWhy quadratic terms matter\n\n\n\n\n\nMany ecological responses are unimodal (species thrive at intermediate values).\nQuadratic GLMs provide a simple way to capture these shapes while retaining interpretability.\nLater, we will compare models formally using AIC or cross-validation to assess whether the quadratic term improves predictive performance.\n\n\n\n\n\n\n1.3 GLMs with multiple predictors\nWhile single-variable models are useful for learning, most species distributions are shaped by multiple interacting environmental drivers. Extending GLMs to include several predictors allows us to capture more realistic ecological responses.\nKey considerations for multivariable GLMs:\nEcological relevance: Select predictors based on natural history knowledge (e.g., bats depend on both climate and topography, not just elevation).\nMulticollinearity: strongly correlated predictors inflate variance and obscure interpretation. Tools like variance inflation factor (VIF), pairwise correlation checks, or principal component analysis (PCA) help diagnose and reduce collinearity.\nScaling and centering: standardising continuous predictors (e.g., scale()) improves interpretability and numerical stability.\nInteractions: consider biologically plausible interactions (e.g., the effect of temperature may depend on rainfall).\nInterpretation: each coefficient reflects the effect of a predictor while holding the others constant.\n\n\n\n\n\n\nTipChecking correlation among predictors and variable selection\n\n\n\n\n\nBefore fitting multivariable GLMs, it is crucial to examine correlations among predictors. Highly correlated variables can inflate standard errors, obscure interpretation, and lead to unstable models.\nStep 1: Compute correlations\n\nlibrary(corrr)\nlibrary(ggplot2)\nlibrary(ggcorrplot)\n\n# Select environmental predictors\nenv_vars &lt;- sdm_df_clean %&gt;%\n  dplyr::select(starts_with(\"bio_\"), elevation)\n\n# Compute correlation matrix\ncor_matrix &lt;- correlate(env_vars)\n\n# Stretch to long format\ncor_long &lt;- stretch(cor_matrix)  # columns: x, y, r\n\n# Visualise correlation matrix directly\nggcorrplot::ggcorrplot(cor(env_vars, use = \"pairwise.complete.obs\"),\n                       hc.order = TRUE,\n                       type = \"lower\",\n                       lab = TRUE,\n                       lab_size = 3,\n                       method = \"circle\",\n                       colors = c(\"blue\", \"white\", \"red\"),\n                       title = \"Pairwise correlations among predictors\")\n\n\n\n\n\n\n\n\nStep 2: Identify highly correlated pairs\n\n# Threshold for \"high correlation\"\nhigh_corr_thresh &lt;- 0.7\n\n# Find pairs above threshold\nhigh_corr_pairs &lt;- cor(env_vars, use = \"pairwise.complete.obs\") %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"var1\") %&gt;%\n  tidyr::pivot_longer(-var1, names_to = \"var2\", values_to = \"r\") %&gt;%\n  filter(abs(r) &gt; high_corr_thresh & var1 != var2)\n\nhigh_corr_pairs\n\n# A tibble: 146 × 3\n   var1  var2           r\n   &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 bio_1 bio_5      0.803\n 2 bio_1 bio_6      0.860\n 3 bio_1 bio_10     0.951\n 4 bio_1 bio_11     0.941\n 5 bio_1 bio_14    -0.701\n 6 bio_1 elevation -0.831\n 7 bio_2 bio_4      0.798\n 8 bio_2 bio_7      0.936\n 9 bio_2 bio_15    -0.725\n10 bio_4 bio_2      0.798\n# ℹ 136 more rows\n\n\nStep 3: Decide which variables to keep\nSeveral strategies exist:\nPrior knowledge: Retain ecologically relevant variables.\nUnivariate model importance: Fit GLMs individually and rank by AIC or explained deviance.\nAutomated selection: Dormann et al. (2013) suggest removing the variable with lower univariate importance from each highly correlated pair.\n\n# Example: compute AIC for univariate GLMs\nuniv_aic &lt;- sapply(names(env_vars), function(v) {\n  mod &lt;- glm(pa ~ ., data = cbind(pa = sdm_df_clean$pa, env_vars[v]), family = binomial)\n  AIC(mod)\n})\n\ntibble(variable = names(univ_aic), AIC = univ_aic) %&gt;%\n  arrange(AIC)\n\n# A tibble: 20 × 2\n   variable    AIC\n   &lt;chr&gt;     &lt;dbl&gt;\n 1 bio_11     795.\n 2 bio_9      811.\n 3 bio_1      815.\n 4 bio_6      830.\n 5 bio_10     847.\n 6 bio_5      876.\n 7 bio_8      895.\n 8 bio_18     901.\n 9 bio_12     903.\n10 bio_13     905.\n11 bio_19     906.\n12 bio_14     906.\n13 bio_16     906.\n14 bio_17     909.\n15 bio_3      912.\n16 elevation  914.\n17 bio_15     916.\n18 bio_7      916.\n19 bio_4      917.\n20 bio_2      917.\n\n\nStep 4: Reduce predictors for multivariable GLM\nAfter filtering for highly correlated variables and assessing univariate importance, we retain only the most relevant and weakly correlated predictors - here, bio_1 (annual mean temperature), bio_12 (annual precipitation), and elevation. This approach improves model interpretability, reduces the risk of overfitting, and ensures that the number of parameters aligns with recommended sample sizes per parameter (Guisan et al., 2017).\n\n\n\n\nExample: GLM with elevation and climate variables\n\n# Fit logistic regression with multiple predictors\nglm_multi_1 &lt;- glm(pa ~ elevation + bio_1 + bio_12,\n                 data = sdm_df_clean,\n                 family = binomial)\n\n# Alternatively, fit logistic regression with multiple predictors as polynomial terms\nglm_multi &lt;- glm(pa ~ poly(elevation, 2) + poly(bio_1, 2) + poly(bio_12, 2),\n                 data = sdm_df_clean,\n                 family = binomial)\n\nsummary(glm_multi)\n\n\nCall:\nglm(formula = pa ~ poly(elevation, 2) + poly(bio_1, 2) + poly(bio_12, \n    2), family = binomial, data = sdm_df_clean)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          -1.4916     0.2656  -5.617 1.95e-08 ***\npoly(elevation, 2)1  93.3200    12.7078   7.344 2.08e-13 ***\npoly(elevation, 2)2  24.7608     8.8285   2.805  0.00504 ** \npoly(bio_1, 2)1     125.1616    16.2360   7.709 1.27e-14 ***\npoly(bio_1, 2)2     -46.5758    10.7501  -4.333 1.47e-05 ***\npoly(bio_12, 2)1    -75.8114    15.8212  -4.792 1.65e-06 ***\npoly(bio_12, 2)2    -67.5172    11.8905  -5.678 1.36e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 912.90  on 658  degrees of freedom\nResidual deviance: 573.82  on 652  degrees of freedom\nAIC: 587.82\n\nNumber of Fisher Scoring iterations: 7\n\n\nHere we included:\nelevation (topographic constraint)\nbio_1: annual mean temperature,\nbio_12: annual precipitation.\nThis model allows us to examine how bats respond jointly to climate and topography.\n\n\nVisualising effects\nFor models with several predictors, it is often more useful to visualise marginal effects rather than raw coefficients:\n\n# Load package for partial effects\nlibrary(effects)\n\n# Plot marginal effect of each predictor\nplot(allEffects(glm_multi))\n\n\n\n\n\n\n\n\nAlternatively, response curves can be generated by varying one predictor while holding others at their mean or median.\nInterpretation caveats:\nWith presence–pseudoabsence data, predicted values remain relative suitability, not absolute probabilities.\nIncluding too many predictors risks overfitting. Use model selection (AIC, cross-validation) or regularisation methods to avoid this.\nAlways check whether fitted responses make ecological sense, not just statistical sense.\n\n\n\n1.4 Model diagnostics\nAfter fitting a GLM, it is essential to evaluate its performance and understand how well it captures species–environment relationships. Key diagnostics include deviance, pseudo-R², and effect plots.\n\n# Perform stepwise variable selection (forward/backward/both based on AIC)\nglm_step &lt;- step(glm_multi, direction = \"both\", trace = FALSE)\n\n# Inspect selected model\nsummary(glm_step)\n\n\nCall:\nglm(formula = pa ~ poly(elevation, 2) + poly(bio_1, 2) + poly(bio_12, \n    2), family = binomial, data = sdm_df_clean)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          -1.4916     0.2656  -5.617 1.95e-08 ***\npoly(elevation, 2)1  93.3200    12.7078   7.344 2.08e-13 ***\npoly(elevation, 2)2  24.7608     8.8285   2.805  0.00504 ** \npoly(bio_1, 2)1     125.1616    16.2360   7.709 1.27e-14 ***\npoly(bio_1, 2)2     -46.5758    10.7501  -4.333 1.47e-05 ***\npoly(bio_12, 2)1    -75.8114    15.8212  -4.792 1.65e-06 ***\npoly(bio_12, 2)2    -67.5172    11.8905  -5.678 1.36e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 912.90  on 658  degrees of freedom\nResidual deviance: 573.82  on 652  degrees of freedom\nAIC: 587.82\n\nNumber of Fisher Scoring iterations: 7\n\n# 3. Compute diagnostics on selected model\ncat(\"Residual deviance:\", glm_step$deviance, \"\\n\")\n\nResidual deviance: 573.8213 \n\ncat(\"Null deviance:\", glm_step$null.deviance, \"\\n\")\n\nNull deviance: 912.8987 \n\ncat(\"AIC:\", AIC(glm_step), \"\\n\")\n\nAIC: 587.8213 \n\n# McFadden's pseudo-R²\npseudo_r2 &lt;- 1 - glm_step$deviance / glm_step$null.deviance\ncat(\"McFadden's pseudo-R²:\", pseudo_r2, \"\\n\")\n\nMcFadden's pseudo-R²: 0.3714294 \n\n\n\n\n\n\n\n\nNoteInterpreting relative suitability\n\n\n\n\n\nWhen using presence–pseudoabsence or presence–background data, the predicted values from a logistic regression GLM do not represent true occurrence probabilities. Instead, they provide a relative measure of habitat suitability along environmental gradients.\nKey points to remember:\n\nHigher predicted values indicate more suitable conditions relative to other locations in the study area.\nThe absolute scale depends on the ratio of presences to pseudo-absences or background points.\nAvoid interpreting predictions as exact probabilities of occurrence in the field.\n\n\n\n\n\n\n\n\n\n\n\nTipOther SDM approaches: machine learning and advanced methods\n\n\n\n\n\nWhile GLMs offer interpretable models, many modern SDM studies use machine learning (ML) algorithms to capture complex, non-linear relationships and interactions among predictors. Common approaches include:\n\nMaxEnt – Uses presence-only data with background points; widely used for species distribution modelling.\n\nRandom Forests (RF) – Ensemble of decision trees; handles non-linearities and interactions automatically.\n\nBoosted Regression Trees (BRT / GBM) – Sequential ensemble of weak learners to improve predictive performance.\n\nThese methods are particularly useful when ecological responses are complex or highly non-linear. They complement GLMs and will be explored further in the “SDM Ensemble Models” section.\n\n\n\n\n\n\n\n\n\nNoteExercises: Building and evaluating species–environment models\n\n\n\n\n\n\nCompare linear vs quadratic responses:\nUsing the same species dataset, fit two GLMs: one with a single linear predictor (e.g., bio_1) and another with a second-order polynomial (poly(bio_1, 2)). Visualise both fits and compare their AIC values and response curves. Which model better captures the ecological response?\nFit multivariable GLMs and assess collinearity:\nSelect 4–5 bioclimatic and topographic predictors. Compute their pairwise correlation matrix, identify highly correlated pairs (e.g., |r| &gt; 0.7), and remove or justify the inclusion of collinear variables. Fit a multivariable GLM and interpret the model coefficients and pseudo-R².\nModel with and without scaled predictors:\nRefit a GLM using ‘scale()’ to standardise all continuous predictors. Compare the resulting coefficients and diagnostic metrics with the unscaled model. Discuss how scaling affects interpretability, model convergence, and numerical stability, especially in models with interaction or quadratic terms.\n\n\n\n\n\n\n\n\n\n\nNoteLiterature\n\n\n\n\n\nAraújo, M.B., Anderson, R.P., Márcia Barbosa, A., Beale, C.M., Dormann, C.F., Early, R., Garcia, R.A., Guisan, A., Maiorano, L., Naimi, B., O’Hara, R.B., Zimmermann, N.E. & Rahbek, C. (2019). Standards for distribution models in biodiversity assessments. Science Advances, 5(1), eaat4858. doi:10.1126/sciadv.aat4858\nElith, J., & Leathwick, J. R. (2009). Species distribution models: ecological explanation and prediction across space and time. Annual Review of Ecology Evolution and Systematics, 40, 677–697. doi:10.1146/annurev.ecolsys.110308.120159\nFranklin, J. (2010). Moving beyond static species distribution models in support of conservation biogeography. Diversity and Distributions, 16(3), 321–330. ]doi:10.1111/j.1472-4642.2010.00641.x](https://doi.org/10.1111/j.1472-4642.2010.00641.x)\nZurell, D., Franklin, J., König, C., Bouchet, P.J., Dormann, C.F., Elith, J., Fandos, G., Feng, X., Guillera-Arroita, G., Guisan, A., Lahoz-Monfort, J.J., Leitão, P.J., Park, D.S., Peterson, A.T., Rapacciuolo, G., Schmatz, D.R., Schröder, B., Serra-Diaz, J.M., Thuiller, W. Yates, Katherine L., Zimmermann, Niklaus E., Merow, C. (2020). A standard protocol for reporting species distribution models. Ecography, 43(9), 1261–1277. doi:10.1111/ecog.04960"
  },
  {
    "objectID": "page_SDM.8_SDMCompareAlgorithms.html",
    "href": "page_SDM.8_SDMCompareAlgorithms.html",
    "title": "Fitting & Comparing Algorithms",
    "section": "",
    "text": "Species Distribution Models (SDMs) can be built using a variety of algorithms, each with its own assumptions, strengths, and limitations. While earlier in this workshop we focused on fitting a simple Generalized Linear Model (GLM), this section broadens the scope to explore alternative approaches commonly used in ecological modelling and biogeography.\nWidely used SDM algorithms include:\nGeneralized Additive Models (GAM): Flexible regression models that capture non-linear relationships using smoothing functions.\nBoosted Regression Trees (BRT / GBM): Ensemble models that combine many decision trees for robust predictions.\nRandom Forests: A tree-based ensemble method that averages multiple models to improve accuracy and reduce overfitting.\nMaxEnt: A popular presence-only modelling method based on the principle of maximum entropy.\nBioclim: A simple climatic envelope model based on observed environmental ranges (not covered in this workshop).\nINLA (Integrated Nested Laplace Approximation): A Bayesian framework allowing spatially explicit modelling of species distributions.\nArtificial Neural Networks (ANN): Flexible machine learning models that can capture complex non-linear relationships, though often harder to interpret.\nIn this section we will fit some of these algorithms to the same species–environment dataset in order to compare their predictive performance and understand how different methods respond to the same data. This comparison highlights the variability that can arise depending on algorithm choice, which is an important consideration when using SDMs for decision-making in conservation, land-use planning, or climate change impact assessment.\nThroughout this section, we will assess model performance using metrics such as AUC (Area Under the Curve), sensitivity, specificity, and TSS (True Skill Statistic). We will also explore how each algorithm predicts habitat suitability across geographic space, and discuss differences in their outputs.\nThis foundation will prepare us for the next step: ensemble modelling, where we combine the strengths of multiple algorithms to generate more robust and reliable predictions."
  },
  {
    "objectID": "page_SDM.8_SDMCompareAlgorithms.html#generalised-additive-models-gams",
    "href": "page_SDM.8_SDMCompareAlgorithms.html#generalised-additive-models-gams",
    "title": "Fitting & Comparing Algorithms",
    "section": "1. Generalised Additive Models (GAMs)",
    "text": "1. Generalised Additive Models (GAMs)\nGeneralised Additive Models (GAMs) extend Generalised Linear Models (GLMs) by allowing for non-linear relationships between predictors and the response variable. In the context of species distribution modelling, GAMs can capture complex ecological responses to environmental gradients using smooth functions, rather than assuming a linear or quadratic form.\nGAMs are particularly useful when species responses to predictors (e.g., temperature, elevation, precipitation) are expected to be curved or irregular. This flexibility often improves model fit and interpretability without overfitting — especially when using automatic smoothing penalties.\nIn R, GAMs are commonly fitted using the mgcv package, which supports efficient estimation of smooth terms and model diagnostics.\n\nFitting a GAM with mgcv\n\nlibrary(mgcv)\n\n# Fit GAM with smooth terms for selected predictors\ngam_model &lt;- gam(pa ~ s(bio_1) + s(bio_12) + s(elevation), \n                 data = sdm_df_clean, \n                 family = binomial(link = \"logit\"))\n\nsummary(gam_model)\n\n\nFamily: binomial \nLink function: logit \n\nFormula:\npa ~ s(bio_1) + s(bio_12) + s(elevation)\n\nParametric coefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -1.1067     0.4739  -2.335   0.0195 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n               edf Ref.df Chi.sq p-value    \ns(bio_1)     3.965  4.879  83.35  &lt;2e-16 ***\ns(bio_12)    5.801  6.501  43.22  &lt;2e-16 ***\ns(elevation) 2.628  3.385  67.44  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =   0.47   Deviance explained = 42.6%\nUBRE = -0.16461  Scale est. = 1         n = 660\n\n\nPlotting smooth effects\nWe can plot the estimated smooth functions to explore how each predictor influences the probability of species presence:\n\nplot(gam_model, pages = 1, shade = TRUE, seWithMean = TRUE)\n\n\n\n\nSmoothed relationships estimated by GAM for each predictor.\n\n\n\n\nThis visualisation helps interpret non-linear ecological responses. For example, a unimodal response to temperature might suggest an optimal thermal range for the species.\n\n\nPredicting habitat suitability with GAMs\nOnce a GAM is fitted, we can project it onto environmental raster layers to generate spatial predictions of habitat suitability.\n\nlibrary(terra)\nlibrary(viridis)\n\n# Make sure the raster stack contains the same predictor names\nproj_stack &lt;- Env_UK_stack[[c(\"bio_1\", \"bio_12\", \"elevation\")]]\nnames(proj_stack)\n\n[1] \"bio_1\"     \"bio_12\"    \"elevation\"\n\n# Predict using the GAM model\ngam_suitability &lt;- terra::predict(proj_stack, gam_model, type = \"response\")\n\n# Plot the prediction\nplot(gam_suitability,\n     main = \"GAM-predicted habitat suitability\",\n     col = plasma(50))\n\n\n\n\n\n\n\n\n\n\nEvaluating GAM performance\nWe can evaluate the model using a hold-out test set or cross-validation. Here’s an example using a simple AUC-based assessment.\n\nlibrary(pROC)\n\n# Fit GAM to training data only\ngam_model_train &lt;- gam(pa ~ s(bio_1) + s(bio_12) + s(elevation), \n                 data = sdm_df_train, \n                 family = binomial(link = \"logit\"))\n\n# Predict on test data\ngam_predict_test &lt;- predict(gam_model_train, newdata = sdm_df_test, type = \"response\")\n\n# Evaluate on test data\nroc_gam &lt;- pROC::roc(sdm_df_test$pa, gam_predict_test)\nauc_gam &lt;- pROC::auc(roc_gam)\n\n# Plot ROC\nplot(roc_gam, col = \"#1f78b4\", main = paste(\"GAM ROC Curve – AUC:\", round(auc_gam, 3)))"
  },
  {
    "objectID": "page_SDM.8_SDMCompareAlgorithms.html#machine-learningbased-sdm-methods",
    "href": "page_SDM.8_SDMCompareAlgorithms.html#machine-learningbased-sdm-methods",
    "title": "Fitting & Comparing Algorithms",
    "section": "2. Machine Learning–Based SDM Methods",
    "text": "2. Machine Learning–Based SDM Methods\nMachine learning (ML) methods have become increasingly popular in species distribution modelling due to their ability to model complex, non-linear relationships between species occurrence and environmental variables. Unlike traditional regression-based approaches (such as GLMs or GAMs), which require the modeller to explicitly define the form of each predictor (e.g., linear terms, polynomial terms, or smooth functions), machine learning models can automatically learn patterns, thresholds, and interactions from the data.\nAt their core, machine learning algorithms aim to find the statistical relationships or decision rules that best predict an outcome based on input features, typically by minimizing some form of prediction error. They do this by iteratively adjusting model parameters or structures in response to data, often without assuming any fixed functional form in advance.\nThis flexibility makes ML methods particularly valuable in ecological applications where relationships between species and environment may be unknown, non-linear, or influenced by multiple interacting factors. They are also well suited for large datasets with many predictors or complex structures, such as spatial or temporal autocorrelation.\n\n\n\n\n\n\n\nTipWhat does a machine learning model ‘learn’?\n\n\n\n\n\nThe plot below shows a simplified example of how a machine learning algorithm (e.g. decision tree, random forest, or boosted tree) separates presence and absence records based on two environmental predictors. The algorithm automatically finds patterns and draws boundaries in the data space to predict species occurrence without the modeller needing to specify a particular equation. The model ‘learns’ by testing different patterns and combinations in the data, adjusting itself to improve predictions on new, unseen observations. This iterative process helps it find rules that generalize beyond the training examples.\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.1 Boosted Regression Trees (BRG/GBM)\nBoosted Regression Trees (BRTs), also known as Gradient Boosting Machines (GBMs), are powerful machine learning models that combine the strengths of two methods: regression trees and boosting.\nA regression tree is a model that recursively splits the data based on predictor variables to reduce variance in the response. While a single tree may be simple and prone to overfitting, boosting builds an ensemble of many small trees in sequence. Each new tree focuses on the prediction errors made by the previous ones, gradually improving overall accuracy.\nThis iterative process makes BRTs highly flexible and effective at modelling complex ecological responses, including interactions and non-linear effects. They are particularly well-suited to species distribution modelling, where ecological relationships can be intricate and noisy.\nWe will use the gbm.step() function from the dismo package, which automates parameter tuning and cross-validation for BRTs.\n\nlibrary(dismo)\nlibrary(gbm)\n\n# Data needs to in dataframe format for dismo::gbm.step()\nsdm_df_clean &lt;- as.data.frame(sdm_df_clean)\n# Ensure response is numeric 1-0\nsdm_df_clean$pa &lt;- as.numeric(as.character(sdm_df_clean$pa))\n\nbrt_model &lt;- gbm.step(data = sdm_df_clean,\n                      gbm.x = c(\"bio_1\", \"bio_12\", \"elevation\"),\n                      gbm.y = \"pa\",\n                      family = \"bernoulli\",\n                      tree.complexity = 3,\n                      learning.rate = 0.01,\n                      bag.fraction = 0.5,\n                      verbose = FALSE)\n\n\n \n GBM STEP - version 2.9 \n \nPerforming cross-validation optimisation of a boosted regression tree model \nfor NA and using a family of bernoulli \nUsing 660 observations and 3 predictors \ncreating 10 initial models of 50 trees \n\n folds are stratified by prevalence \ntotal mean deviance =  1.3852 \ntolerance is fixed at  0.0014 \nnow adding trees... \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteUnderstanding the holdout deviance plot\n\n\n\n\n\n\nThe plot shows holdout deviance (a measure of model error) on cross-validated data versus the number of trees in the Boosted Regression Tree (BRT) model.\nLower deviance indicates better model fit.\nEach point o the line represents the deviance after adding that many trees.\nThe red horizontal line marks the mean deviance at the optimal number of trees determined via cross-validation.\nThe curve usually declines initially, then levels off; if it rises, overfitting may occur.\nChoosing the number of trees at the minimum deviance balances predictive performance and model complexity.\n\n\nEven if the deviance curve looks smooth, cross-validation helps select a robust number of trees rather than relying solely on training data.\n\n\n\n\nOnce fitted, we can explore variable influence and partial dependence plots:\n\nsummary(brt_model)\n\n\n\n\nPartial dependence plots from the BRT model showing species response to each predictor.\n\n\n\n\n                var  rel.inf\nbio_1         bio_1 42.16197\nelevation elevation 29.16432\nbio_12       bio_12 28.67371\n\ngbm.plot(brt_model, n.plots = 3)\n\n\n\n\nPartial dependence plots from the BRT model showing species response to each predictor.\n\n\n\n\nPredicting habitat suitability with the BRT\nWe can now project the BRT model to the environmental raster layers:\n\nlibrary(terra)\nlibrary(viridis)\n\nproj_stack_sub &lt;- Env_UK_stack[[c(\"bio_1\", \"bio_12\", \"elevation\")]]\nnames(proj_stack_sub) &lt;- c(\"bio_1\", \"bio_12\", \"elevation\")  # ensure matching names\n\n# Predict habitat suitability\nbrt_prediction &lt;- terra::predict(proj_stack_sub, brt_model, type = \"response\")\n\n# Mask to land area (optional: using the first raster as template)\n# This sets ocean/NA areas back to NA\nbrt_prediction &lt;- mask(brt_prediction, proj_stack_sub[[1]])\n\n# Plot with color scale emphasizing land values\nplot(brt_prediction,\n     main = \"BRT-predicted habitat suitability\",\n     col = magma(50),\n     na.col = \"lightblue\")\n\n\n\n\n\n\n\n\nModel evaluation\nEvaluate the model on training data using AUC:\n\nlibrary(pROC)\n\n# Data needs to in dataframe format for dismo::gbm.step()\nsdm_df_train &lt;- as.data.frame(sdm_df_train)\n# Ensure response is numeric 1-0\nsdm_df_train$pa &lt;- as.numeric(as.character(sdm_df_train$pa))\n\n# Fit BRT to training data only\nbrt_model_train &lt;- gbm.step(data = sdm_df_train,\n                      gbm.x = c(\"bio_1\", \"bio_12\", \"elevation\"),\n                      gbm.y = \"pa\",\n                      family = \"bernoulli\",\n                      tree.complexity = 3,\n                      learning.rate = 0.01,\n                      bag.fraction = 0.5,\n                      verbose = FALSE)\n\n\n \n GBM STEP - version 2.9 \n \nPerforming cross-validation optimisation of a boosted regression tree model \nfor NA and using a family of bernoulli \nUsing 462 observations and 3 predictors \ncreating 10 initial models of 50 trees \n\n folds are stratified by prevalence \ntotal mean deviance =  1.386 \ntolerance is fixed at  0.0014 \nnow adding trees... \n\n\n\n\n\n\n\n\n# Predict on test data\nbrt_predict_test &lt;- predict(brt_model_train, \n                           newdata= sdm_df_test, n.trees = brt_model_train$gbm.call$best.trees, type = \"response\")\n\nroc_brt &lt;- pROC::roc(sdm_df_test$pa, brt_predict_test)\nauc_brt &lt;- pROC::auc(roc_brt )\nplot(roc_brt, col = \"#ff7f00\", main = paste(\"BRT ROC Curve – AUC:\", round(auc_brt, 3)))\n\n\n\n\n\n\n\n\n\n\n2.2 Random Forests\nRandom Forests are a powerful and widely used machine learning method in species distribution modelling. Like Boosted Regression Trees, they are based on ensembles of decision trees — but instead of boosting, Random Forests rely on bagging (bootstrap aggregation).\nEach tree in the forest is trained on a different random subset of the data. At each split, only a random subset of predictor variables is considered. This process reduces overfitting and makes the model more robust, particularly when dealing with correlated predictors or noisy data.\nIn SDM, Random Forests are valued for their high predictive accuracy, ability to handle non-linear and interactive effects, and their built-in estimates of variable importance.\nBelow we fit a Random Forest SDM using the randomForest package.\n\nlibrary(randomForest)\n\n# Make sure 'pa' is a factor for classification\nsdm_df_clean$pa &lt;- as.factor(sdm_df_clean$pa)\n\n# Fit the model\nrF_model &lt;- randomForest(pa ~ bio_1 + bio_12 + elevation,\n                         data = sdm_df_clean,\n                         ntree = 500,\n                         importance = TRUE)\n\n# View variable importance\nprint(rF_model)\n\n\nCall:\n randomForest(formula = pa ~ bio_1 + bio_12 + elevation, data = sdm_df_clean,      ntree = 500, importance = TRUE) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n        OOB estimate of  error rate: 16.97%\nConfusion matrix:\n    0   1 class.error\n0 274  67   0.1964809\n1  45 274   0.1410658\n\nvarImpPlot(rF_model, main = \"Random Forest Variable Importance\")\n\n\n\n\n\n\n\n\nPredict habitat suitability using Random Forest\nWe now project the trained model onto the environmental raster layers:\n\nlibrary(terra)\nlibrary(viridis)\n\n# Prepare stack\nproj_stack &lt;- Env_UK_stack[[c(\"bio_1\", \"bio_12\", \"elevation\")]]\nnames(proj_stack) &lt;- c(\"bio_1\", \"bio_12\", \"elevation\")\n\n# Predict probability of presence\nrF_prediction &lt;- terra::predict(proj_stack, rF_model, type = \"prob\", index = 2)\n\n# Plot prediction\nplot(rF_prediction,\n     main = \"Random Forest–predicted habitat suitability\",\n     col = inferno(50))\n\n\n\n\n\n\n\n\nEvaluate model performance\nWe use AUC and the ROC curve to evaluate predictive performance:\n\nlibrary(pROC)\n\n# Make sure 'pa' is a factor for classification\nsdm_df_train$pa &lt;- as.factor(sdm_df_train$pa)\nsdm_df_test$pa &lt;- as.factor(sdm_df_test$pa)\n\n# # Fit randomForest to training data only\nrF_model_train &lt;- randomForest(pa ~ bio_1 + bio_12 + elevation,\n                         data = sdm_df_train,\n                         ntree = 500,\n                         importance = TRUE)\n\nrF_predict_test &lt;- predict(rF_model_train, newdata= sdm_df_test, type = \"prob\")[, \"1\"]\n\nroc_rF &lt;- pROC::roc(sdm_df_test$pa, rF_predict_test)\nauc_rF &lt;- pROC::auc(roc_rF)\nplot(roc_rF, col= \"#33a02c\", main = paste(\"Random Forest ROC Curve – AUC:\", round(auc_rF, 3)))\n\n\n\n\n\n\n\n\nRandom Forests are often among the top-performing SDM algorithms, especially when the dataset is not too small. However, like other black-box models, they may lack transparency, so careful evaluation and interpretation of variable importance and response curves are recommended.\n\n\n2.3 Maxent\nMaxent (Maximum Entropy) is a widely used algorithms in species distribution modelling when only presence data are available. Unlike regression- or tree-based methods, which typically require both presence and absence points, Maxent estimates the species’ distribution by finding the probability distribution of maximum entropy (i.e., closest to uniform) constrained by the environmental conditions at known occurrence locations.\nIn practice, Maxent contrasts environmental conditions at occurrence points against conditions across a wider background area, identifying the combination of conditions that best explains the observed presences while avoiding overfitting. This makes it particularly useful for records where absences are unreliable or unknown such citizen museum datasets or science data.\nMaxent can be implemented in R via the maxnet package, which provides a native R re-implementation of the Maxent algorithm. Unlike the dismo package’s interface, which requires the standalone Java-based Maxent software, maxnet runs entirely in R and avoids Java dependencies. It fits presence–background models using the same maximum entropy framework, but with a formula interface that allows greater flexibility and integration into R workflows.\n\nlibrary(maxnet)\nlibrary(terra)\n\n# Extract environmental predictors at presence and background locations\npresence_vals &lt;- terra::extract(proj_stack, vect(presence_sf)) %&gt;%\n  dplyr::select(-ID)\nbackground_vals &lt;- terra::extract(proj_stack, vect(target_group_sf)) %&gt;%\n  dplyr::select(-ID)\n\n# Combine into one dataset\npa &lt;- c(rep(1, nrow(presence_vals)), rep(0, nrow(background_vals)))\nenv_vals &lt;- rbind(presence_vals, background_vals)\n\n# Remove rows with NA in predictors\ncomplete_idx &lt;- complete.cases(env_vals)\nenv_vals_clean &lt;- env_vals[complete_idx, ]\npa_clean &lt;- pa[complete_idx]\n\n# Fit Maxent with maxnet\nmaxnet_model &lt;- invisible(\n  suppressWarnings(\n    suppressMessages(\n      maxnet(\n        p = pa_clean,\n        data = env_vals_clean,\n        f = maxnet.formula(pa_clean, env_vals_clean)\n      )\n  )))\n\nThe output includes model coefficients, variable contributions, and training performance metrics.\nVisualising variable contributions\nMaxent provides estimates of how much each predictor contributes to the model:\n\nplot(maxnet_model)\n\n\n\n\nVariable importance estimates from the Maxent model.\n\n\n\n\nProjecting habitat suitability with Maxent\nOnce trained, the Maxent model can be projected across geographic space to estimate habitat suitability:\n\nlibrary(terra)\nlibrary(maxnet)\n\n# 1. Extract raster values as data.frame\nenv_df &lt;- as.data.frame(proj_stack, xy = TRUE, na.rm = FALSE)  # keep xy for plotting\n\n# 2. Identify rows without NA (complete cases)\ncomplete_idx &lt;- complete.cases(env_df[, names(env_vals)])  # only predictor columns\n\n# 3. Predict with maxnet\npred_vals &lt;- rep(NA, nrow(env_df))  # initialize output\npred_vals[complete_idx] &lt;- predict(maxnet_model, env_df[complete_idx, names(env_vals)], type = \"cloglog\")\n\n# 4. Convert back to raster\nmaxent_prediction &lt;- proj_stack[[1]]  # take first layer as template\nvalues(maxent_prediction) &lt;- pred_vals\n\n# 5. Plot\nplot(maxent_prediction,\n     main = \"Maxent (maxnet) predicted habitat suitability\",\n     col = cividis(50))\n\n\n\n\n\n\n\n\nEvaluating model performance\nWe can evaluate the Maxent model using AUC and the ROC curve, similar to other algorithms:\n\nlibrary(pROC)\n\n# Extract environmental values at presence and background points\npresence_vals &lt;- terra::extract(proj_stack, vect(presence_sf)) %&gt;% dplyr::select(-ID)\nbackground_vals &lt;- terra::extract(proj_stack, vect(target_group_sf)) %&gt;% dplyr::select(-ID)\n\n# Remove rows with NA\npresence_vals &lt;- presence_vals[complete.cases(presence_vals), ]\nbackground_vals &lt;- background_vals[complete.cases(background_vals), ]\n\n# {redict with maxnet model\npresence_probs &lt;- predict(maxnet_model, presence_vals, type = \"cloglog\")\nbackground_probs &lt;- predict(maxnet_model, background_vals, type = \"cloglog\")\n\n# Combine into evaluation vectors\neval_labels &lt;- c(rep(1, nrow(presence_probs)), rep(0, nrow(background_probs)))\neval_probs &lt;- c(presence_probs, background_probs)\n\n# ROC and AUC\nroc_maxnet &lt;- pROC::roc(eval_labels, eval_probs)\nauc_maxnet &lt;- roc_maxnet$auc\n\n# Plot ROC curve\nplot(roc_maxnet, col = \"#6a3d9a\", lwd = 2, main = paste(\"Maxent (maxnet) ROC Curve – AUC:\", round(auc_maxnet, 3)))\nabline(a = 0, b = 1, lty = 2, col = \"grey\")\n\n\n\n\n\n\n\n\nMaxent is often praised for its strong performance with limited or biased presence-only data, but users should remain mindful of its assumptions. In particular, results are sensitive to the choice of background points and regularisation settings, and caution is required when extrapolating into novel environmental space (e.g., under future climate scenarios)."
  },
  {
    "objectID": "page_SDM.8_SDMCompareAlgorithms.html#envelope-based-models",
    "href": "page_SDM.8_SDMCompareAlgorithms.html#envelope-based-models",
    "title": "Fitting & Comparing Algorithms",
    "section": "3. Envelope-Based Models",
    "text": "3. Envelope-Based Models\n\n3.1 Bioclim\nBioclim is one of the earliest and simplest species distribution modelling approaches.\nIt defines the species’ environmental envelope by summarizing the range of conditions (e.g., temperature, precipitation, elevation) at the observed occurrence locations. Predictions are then made by identifying areas in environmental space that fall within (or close to) this envelope.\nConceptually, Bioclim assumes that a species is equally likely to occur anywhere within its observed environmental limits. This makes it very intuitive and fast to compute, but also limited in its ability to model more complex, non-linear responses. Despite its simplicity, Bioclim is still useful as a baseline SDM and for teaching or comparison purposes.\nIn R, Bioclim can be fitted using the bioclim() function from the dismo package.\nFitting a Bioclim model\n\nlibrary(terra)\nlibrary(sf)\nlibrary(dplyr)\n\n# 1. Extract predictor values at presence points\npresence_vals &lt;- terra::extract(proj_stack, vect(presence_sf)) %&gt;% \n  dplyr::select(-ID)\n\n# 2. Remove any rows with NA\npresence_vals &lt;- presence_vals[complete.cases(presence_vals), ]\n\n# 3. Compute climatic envelope for each predictor\nbioclim_min &lt;- apply(presence_vals, 2, min)\nbioclim_max &lt;- apply(presence_vals, 2, max)\n\nbioclim_model &lt;- list(\n  min = bioclim_min,\n  max = bioclim_max\n)\n\nbioclim_model\n\n$min\n     bio_1     bio_12  elevation \n  7.412333 638.000000   8.870933 \n\n$max\n     bio_1     bio_12  elevation \n  10.82761 1647.00000  509.73306 \n\n\nVisualising Bioclim response curves\nWe can plot the response curves to inspect the species’ environmental envelope:\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\n# Convert min/max ranges to long format for plotting\nenv_df &lt;- data.frame(\n  predictor = names(bioclim_model$min),\n  min = bioclim_model$min,\n  max = bioclim_model$max\n) %&gt;%\n  pivot_longer(cols = c(min, max), names_to = \"limit\", values_to = \"value\")\n\n# Plot\nggplot(env_df, aes(x = predictor, y = value)) +\n  # Lines connecting min-max in gray\n  geom_line(aes(group = predictor), color = \"gray\", size = 1.2) +\n  \n  # Points colored by limit\n  geom_point(aes(color = limit), size = 3) +\n  \n  # Horizontal ticks at min and max\n  geom_segment(aes(x = as.numeric(factor(predictor)) - 0.1, \n                   xend = as.numeric(factor(predictor)) + 0.1,\n                   y = value, yend = value,\n                   color = limit),\n               size = 1.2) +\n  \n  labs(x = \"Environmental predictor\",\n       y = \"Value\",\n       color = \"Envelope limit\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\nBioclim response curves showing the species’ environmental envelope.\n\n\n\n\nProjecting habitat suitability with Bioclim\nOnce fitted, the Bioclim model can be projected across the study area:\n\nlibrary(terra)\n\n# Extract raster values as a data.frame\nenv_df &lt;- as.data.frame(proj_stack, xy = TRUE, na.rm = FALSE)\n\n# Compute suitability: 1 if all predictors within min/max, 0 otherwise\nsuitability &lt;- apply(env_df[, names(bioclim_model$min)], 1, function(x) {\n  all(x &gt;= bioclim_model$min & x &lt;= bioclim_model$max)\n})\n\n# Initialize raster for predicted suitability\nbioclim_prediction &lt;- proj_stack[[1]]  # template\nvalues(bioclim_prediction) &lt;- NA       # reset\nvalues(bioclim_prediction)[!is.na(env_df[[1]])] &lt;- suitability\n\n# Plot predicted suitability\nplot(bioclim_prediction,\n     main = \"Bioclim-predicted habitat suitability\",\n     col = c(\"lightgrey\", \"#FFC0C0\"),\n     legend = FALSE)\nlegend(\"topright\", legend = c(\"Unsuitable\", \"Suitable\"), fill = c(\"lightgrey\", \"#FFC0C0\"))\n\n\n\n\n\n\n\n\nBioclim’s simplicity makes it easy to implement and interpret, but it lacks the flexibility of regression- or machine-learning–based approaches. It assumes that all environmental conditions within the observed range are equally suitable, which is often unrealistic. Nevertheless, Bioclim provides a useful baseline comparator in SDM studies and highlights the importance of model choice in shaping predicted distributions."
  },
  {
    "objectID": "page_SDM.8_SDMCompareAlgorithms.html#advanced-and-emerging-methods",
    "href": "page_SDM.8_SDMCompareAlgorithms.html#advanced-and-emerging-methods",
    "title": "Fitting & Comparing Algorithms",
    "section": "4. Advanced and Emerging Methods",
    "text": "4. Advanced and Emerging Methods\nThe field of species distribution modelling is constantly evolving. Beyond classical statistical and machine learning approaches, a number of advanced or emerging methods are now being used in ecological research.\nThese methods offer new opportunities for handling spatial complexity, uncertainty, biotic interactions and ecological dynamics such as species dispersal and colonisation dynamics and very large datasets, but they may also require greater expertise, computational resources, or specialized software.\n\n4.1 Bayesian SDMs (e.g., INLA)\nBayesian approaches to SDMs provide a probabilistic framework that can incorporate prior knowledge, quantify uncertainty, and explicitly model spatial dependence. One of the most widely used implementations is INLA (Integrated Nested Laplace Approximation), which allows efficient Bayesian inference for spatial models.\nKey features of Bayesian SDMs: - Explicitly account for spatial autocorrelation in species occurrences.\n- Allow incorporation of prior knowledge about ecological processes.\n- Provide posterior distributions for parameters and predictions, offering richer uncertainty estimates than single-value statistics like AUC.\n- Flexible framework: can include random effects, hierarchical structures, and complex spatial/temporal dependencies.\nLimitations: - Steeper learning curve compared to GLMs or Random Forests - Can be computationally demanding for large datasets\nINLA is implemented in the R package R-INLA, though it requires some familiarity with Bayesian modelling concepts.\n\n\n\n4.2 Joint species distribution modelling to account for biotic interactions\nMost classical SDMs assume that species distributions are shaped primarily by the abiotic environment (e.g., climate, topography, land cover). However, species do not occur in isolation. Their distributions are often influenced by biotic interactions such as competition, predation, facilitation, or mutualism.\nJoint Species Distribution Models (jSDMs) extend traditional SDMs by modelling the occurrence of multiple species simultaneously. This allows the estimation of both species–environment relationships and species–species associations, offering a more realistic ecological picture.\n\nKey features of jSDMs:\n\nShared responses to the environment: capture similarities among species with similar niches.\n\nResidual correlations: identify co-occurrence patterns not explained by environment (potential biotic interactions).\n\nHierarchical framework: often Bayesian, with species treated as partially exchangeable units.\n\nCan improve predictions, especially for rare species with few records, by “borrowing strength” from related species.\n\n\n\nLimitations:\n\nMore complex to fit and interpret than single-species SDMs.\n\nComputationally demanding, especially with large species × site matrices.\n\nCorrelations may reflect unmeasured environmental covariates rather than true biotic interactions — so ecological interpretation must be cautious.\n\nIn R, jSDMs can be fitted using packages such as:\n- Hmsc: Hierarchical modelling of species communities (supports spatial/temporal structure).\n- boral: Bayesian ordination and regression analysis of multivariate abundance data.\n- gjam: Generalized Joint Attribute Modelling, flexible for presence–absence, counts, or continuous traits.\n\n\n\n4.3 Dynamic and spatio-temporal SDMs\nWhile spatial SDMs are common, explicitly incorporating time (seasonality, interannual variability, long-term change, movement/dispersal and colonisation) is essential for capturing ecological dynamics and climate change impacts.\nKey features:\nCapture temporal dynamics in species-environment relationships.\nCan model shifts in species ranges over time.\nAllow exploration of lagged effects, seasonal cycles, or temporal autocorrelation.\nApproaches:\nTime-varying covariates in standard SDMs.\nSpatio-temporal models using INLA (R-INLA), mgcv::bam() for GAMs with time components.\nDynamic occupancy models (in unmarked, spOccupancy).\nTemporal models are especially relevant for climate change projections, phenology, or seasonal migrations.\n\n\n4.4 Neural networks and deep learning approaches to SDM\nNeural networks (NNs), including deep learning approaches, are increasingly being applied to SDMs, especially when working with large, complex datasets such as high-resolution remote sensing imagery or citizen science data.\nKey features: - Highly flexible models that can capture complex non-linearities and interactions.\n- Convolutional Neural Networks (CNNs) are particularly useful for incorporating spatially structured data such as satellite images or climate surfaces.\n- Capable of end-to-end learning (directly predicting species presence/absence from raw data).\nLimitations: - Require large datasets for training\n- Computationally intensive; often need GPUs or cloud computing\n- Models can be difficult to interpret (“black-box” nature)\nDespite these challenges, deep learning holds promise for applications such as species recognition from images, integration of multiple data sources, and fine-scale spatial predictions.\n\n\n4.5 Mechanistic / process-based models\nUnlike correlative models, which are based on statistical relationships between species occurrences and environmental variables, mechanistic SDMs explicitly model species distributions using physiological, behavioural, or demographic processes that are assumed to drive species occurrence, abundance, and distribution. These processes are represented through biomathematical functional relationships.\nExamples include: - Niche mapping (thermodynamic constraints on heat, water and nutritional balance) - Dynamic range models that simulate dispersal and colonization - Individual-based models (IBMs) t\nMechanistic SDMs offer important advantages over correlative models, including the ability to model novel climates more reliably and explicitly incorporate species traits and physiological limits. However, they require detailed species-specific data and are often complex and computationally intensive, making them less accessible for many applications. Linking mechanistic models to real-world patterns remains a challenge, and model validation approaches are still the subject of ongoing debate. To balance these trade-offs, so-called hybrid approaches that combine correlative and mechanistic elements are emerging as a powerful compromise."
  },
  {
    "objectID": "page_SDM.8_SDMCompareAlgorithms.html#comparing-sdm-algorithms",
    "href": "page_SDM.8_SDMCompareAlgorithms.html#comparing-sdm-algorithms",
    "title": "Fitting & Comparing Algorithms",
    "section": "5. Comparing SDM algorithms",
    "text": "5. Comparing SDM algorithms\nOnce multiple species distribution models are fitted, it is informative to compare their predictions and performance metrics. This helps learners and practitioners understand how algorithm choice affects model outputs and ecological interpretation.\n\n5.1 Performance metrics comparison\nKey metrics include:\nAUC (Area Under the ROC Curve): overall discriminatory ability.\nTSS (True Skill Statistic): balances sensitivity and specificity.\nSensitivity / Specificity: model’s ability to predict presences vs absences.\nDeviance or log-likelihood: for statistical models like GAMs.\nYou can compile a simple table of metrics for each model:\n\nmodel_metrics &lt;- data.frame(\n  Model = c(\"GAM\", \"BRT\", \"Random Forest\", \"Maxent\", \"Bioclim\"),\n  AUC = c(\n    auc_gam,    # GAM\n    auc_brt,    # BRT\n    auc_rF,     # RF\n    auc_maxnet, # Maxnet\n    NA          # Bioclim\n  )\n)\n\nknitr::kable(model_metrics, caption = \"Comparison of model performance (AUC) across SDM algorithms\")\n\n\nComparison of model performance (AUC) across SDM algorithms\n\n\nModel\nAUC\n\n\n\n\nGAM\n0.8625923\n\n\nBRT\n0.8903815\n\n\nRandom Forest\n0.8989951\n\n\nMaxent\n0.8446836\n\n\nBioclim\nNA\n\n\n\n\n\n\n\n5.2 Performance metrics comparison\nVisual comparison of habitat suitability maps can highlight differences in predicted distributions, smoothness, and sensitivity to rare occurrences or extreme values.\n\nlibrary(viridis)\n\npar(mfrow = c(2,3))  # grid layout\n\nplot(glm_suitability_map, main = \"GLM\", col = viridis(50, option = \"D\"))\nplot(gam_suitability, main = \"GAM\", col = plasma(50))\nplot(brt_prediction, main = \"BRT\", col = magma(50))\nplot(rF_prediction, main = \"Random Forest\", col = inferno(50))\nplot(maxent_prediction, main = \"Maxent\", col = cividis(50))\nplot(bioclim_prediction, main = \"Bioclim\", col = c(\"lightgrey\", \"#FFC0C0\"))\n\n\n\n\nSpatial predictions from different SDM algorithms for visual comparison.\n\n\n\npar(mfrow = c(1,1))  # reset layout\n\n\n\n\n\n\n\nNoteLiterature\n\n\n\n\n\nBakka, H., Rue, H., Fuglstad, G.-.A., Riebler, A., Bolin, D., Illian, J., Krainski, E., Simpson, D. & Lindgren, F. (2018). Spatial modeling with R-INLA: A review. WIREs Computational Statistics, 10(6), e1443. doi:10.1002/wics.1443\nDeneu, B., Servajean, M., Bonnet, P., Botella, C., Munoz, F., & Joly, A. (2021). Convolutional neural networks improve species distribution modelling by capturing the spatial structure of the environment. PLOS Computational Biology, 17(4), e1008856. doi:10.1371/journal.pcbi.1008856\nElith, J., Leathwick, J. R., & Hastie, T. (2008). A working guide to boosted regression trees. Journal of Animal Ecology, 77(4), 802–813. doi:10.1111/j.1365-2656.2008.01390.x\nElith, J., & Leathwick, J. R. (2009). Species distribution models: ecological explanation and prediction across space and time. Annual Review of Ecology Evolution and Systematics, 40, 677–697. doi:10.1146/annurev.ecolsys.110308.120159\nKearney, M. R., & Porter, W. P. (2020). NicheMapR – an R package for biophysical modelling: the ectotherm and Dynamic Energy Budget models. Ecography, 43(1), 85–96. doi:https://doi.org/10.1111/ecog.04680\nWood, S. N. (2017). Generalized additive models. An introduction with R ( 2nd ed.). CRC Press.\nPhillips, S.J., Dudík, M., Elith, J., Graham, C.H., Lehmann, A., Leathwick, J. & Ferrier, S. (2009) Sample selection bias and presence-only distribution models: Implications for background and pseudo-absence data. Ecological Applications, 19(1), 181–197. doi:10.1890/07-2153.1"
  }
]